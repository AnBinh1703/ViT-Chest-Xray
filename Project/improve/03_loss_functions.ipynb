{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "066a4fb8",
   "metadata": {},
   "source": [
    "## 1.2 Class Imbalance Handling\n",
    "\n",
    "### â“ Táº¡i Sao ÄÃ¢y LÃ  Váº¥n Äá» NghiÃªm Trá»ng?\n",
    "\n",
    "**PhÃ¢n tÃ­ch class distribution:**\n",
    "```\n",
    "No Finding:    60,361 samples (53.84%) ðŸ˜±\n",
    "Infiltration:  19,894 samples (17.74%)\n",
    "Atelectasis:   11,559 samples (10.31%)\n",
    "...\n",
    "Hernia:           227 samples (0.20%)  ðŸ˜±\n",
    "```\n",
    "\n",
    "**Háº­u quáº£:**\n",
    "- Model bias vá» \"No Finding\" â†’ Predict \"No Finding\" cho má»i case\n",
    "- Rare diseases (Hernia, Pneumonia) bá»‹ ignore â†’ Nguy hiá»ƒm trong medical application!\n",
    "- AUC tá»•ng thá»ƒ cÃ³ thá»ƒ cao nhÆ°ng per-class performance kÃ©m\n",
    "\n",
    "### ðŸ’¡ Giáº£i PhÃ¡p: Multi-Strategy Approach\n",
    "\n",
    "#### Strategy 1: Focal Loss\n",
    "**Táº¡i sao:** Tá»± Ä‘á»™ng focus vÃ o hard/rare examples\n",
    "\n",
    "$$FL(p_t) = -\\alpha_t(1-p_t)^\\gamma \\log(p_t)$$\n",
    "\n",
    "- $\\gamma = 2$: Down-weight easy examples\n",
    "- $\\alpha = 0.25$: Balance positive/negative\n",
    "\n",
    "#### Strategy 2: Class Weights\n",
    "**Táº¡i sao:** Penalty cao hÆ¡n khi predict sai rare classes\n",
    "\n",
    "$$w_i = \\frac{N_{total} - N_positive_i}{N_positive_i}$$\n",
    "\n",
    "#### Strategy 3: Weighted Sampling\n",
    "**Táº¡i sao:** Äáº£m báº£o má»—i batch cÃ³ representation cá»§a rare classes\n",
    "\n",
    "### ðŸ“ˆ Expected Impact\n",
    "- **+3-5% AUC** on rare classes (Hernia, Pneumonia, Fibrosis)\n",
    "- More balanced predictions across all diseases\n",
    "- Clinically safer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b974efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Loss for multi-label classification\n",
    "    \n",
    "    Paper: \"Focal Loss for Dense Object Detection\" (Lin et al., 2017)\n",
    "    Adapted for multi-label medical imaging\n",
    "    \n",
    "    Args:\n",
    "        alpha (float): Weighting factor [0, 1]\n",
    "        gamma (float): Focusing parameter >= 0\n",
    "        pos_weight (Tensor): Positive class weights for each class\n",
    "    \n",
    "    Intuition:\n",
    "    - gamma=0: Standard BCE loss\n",
    "    - gammaâ†‘: More focus on hard examples\n",
    "    - Easy examples (pt â†’ 1) get down-weighted by (1-pt)^gamma\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, pos_weight=None):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.pos_weight = pos_weight\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: (N, C) logits (before sigmoid)\n",
    "            targets: (N, C) binary labels\n",
    "        \"\"\"\n",
    "        # BCE loss with logits\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(\n",
    "            inputs, targets, \n",
    "            pos_weight=self.pos_weight,\n",
    "            reduction='none'\n",
    "        )\n",
    "        \n",
    "        # Probability of correct class\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        \n",
    "        # Focal term: (1-pt)^gamma\n",
    "        # Khi pt â†’ 1 (easy): focal_term â†’ 0 (down-weight)\n",
    "        # Khi pt â†’ 0 (hard): focal_term â†’ 1 (keep weight)\n",
    "        focal_term = (1 - pt) ** self.gamma\n",
    "        \n",
    "        # Final loss\n",
    "        focal_loss = self.alpha * focal_term * BCE_loss\n",
    "        \n",
    "        return focal_loss.mean()\n",
    "\n",
    "\n",
    "class WeightedBCELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    BCE Loss vá»›i class weights\n",
    "    \n",
    "    TÃ­nh pos_weight dá»±a trÃªn class frequency:\n",
    "    pos_weight[i] = (N_total - N_positive[i]) / N_positive[i]\n",
    "    \n",
    "    Classes hiáº¿m â†’ pos_weight cao â†’ penalty cao khi predict sai\n",
    "    \"\"\"\n",
    "    def __init__(self, pos_weight=None):\n",
    "        super(WeightedBCELoss, self).__init__()\n",
    "        self.pos_weight = pos_weight\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        return F.binary_cross_entropy_with_logits(\n",
    "            inputs, targets,\n",
    "            pos_weight=self.pos_weight\n",
    "        )\n",
    "\n",
    "\n",
    "class LabelSmoothingBCE(nn.Module):\n",
    "    \"\"\"\n",
    "    Label Smoothing for BCE Loss\n",
    "    \n",
    "    Regularization technique:\n",
    "    - Original: target âˆˆ {0, 1}\n",
    "    - Smoothed: target âˆˆ [Îµ, 1-Îµ]\n",
    "    \n",
    "    Benefits:\n",
    "    - Prevent overconfident predictions\n",
    "    - Handle label noise (~10% in NIH dataset)\n",
    "    - Better calibration\n",
    "    \"\"\"\n",
    "    def __init__(self, smoothing=0.1, pos_weight=None):\n",
    "        super(LabelSmoothingBCE, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.pos_weight = pos_weight\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        # Smooth labels: 1 â†’ 1-Îµ, 0 â†’ Îµ\n",
    "        targets_smooth = targets * (1 - self.smoothing) + 0.5 * self.smoothing\n",
    "        \n",
    "        return F.binary_cross_entropy_with_logits(\n",
    "            inputs, targets_smooth,\n",
    "            pos_weight=self.pos_weight\n",
    "        )\n",
    "\n",
    "\n",
    "def compute_class_weights(df, disease_columns):\n",
    "    \"\"\"\n",
    "    TÃ­nh class weights dá»±a trÃªn frequency\n",
    "    \n",
    "    Formula: w_i = (N_total - N_positive_i) / N_positive_i\n",
    "    \n",
    "    Example:\n",
    "    - Hernia: 227 samples â†’ weight = (100000 - 227) / 227 â‰ˆ 440\n",
    "    - No Finding: 60361 samples â†’ weight = (100000 - 60361) / 60361 â‰ˆ 0.66\n",
    "    \"\"\"\n",
    "    class_counts = df[disease_columns].sum().values\n",
    "    total_samples = len(df)\n",
    "    \n",
    "    # Inverse frequency weighting\n",
    "    pos_weights = (total_samples - class_counts) / np.maximum(class_counts, 1)\n",
    "    \n",
    "    # Normalize to prevent extreme values\n",
    "    pos_weights = np.clip(pos_weights, 0.5, 100)  # Clip to reasonable range\n",
    "    \n",
    "    return torch.FloatTensor(pos_weights)\n",
    "\n",
    "\n",
    "def compute_sample_weights(df, disease_columns):\n",
    "    \"\"\"\n",
    "    TÃ­nh sampling weights cho WeightedRandomSampler\n",
    "    \n",
    "    Strategy: Sample cÃ³ rare disease â†’ weight cao â†’ probability sampling cao\n",
    "    \n",
    "    Returns:\n",
    "        weights: (N,) array of sampling weights\n",
    "    \"\"\"\n",
    "    # Inverse class frequency\n",
    "    class_counts = df[disease_columns].sum().values\n",
    "    class_weights = 1.0 / np.maximum(class_counts, 1)\n",
    "    \n",
    "    # Sample weight = max class weight cá»§a cÃ¡c diseases cÃ³ trong sample\n",
    "    sample_weights = []\n",
    "    for _, row in df.iterrows():\n",
    "        labels = row[disease_columns].values\n",
    "        # Weight = sum of weights for positive classes\n",
    "        weight = np.sum(class_weights * labels)\n",
    "        if weight == 0:  # No positive labels\n",
    "            weight = class_weights.min()\n",
    "        sample_weights.append(weight)\n",
    "    \n",
    "    return np.array(sample_weights)\n",
    "\n",
    "\n",
    "print(\"âœ… Loss functions implemented:\")\n",
    "print(\"   1. FocalLoss (Î±=0.25, Î³=2.0)\")\n",
    "print(\"   2. WeightedBCELoss\")\n",
    "print(\"   3. LabelSmoothingBCE (Îµ=0.1)\")\n",
    "print(\"\\nðŸ“Š Class weighting strategies:\")\n",
    "print(\"   - compute_class_weights(): For loss functions\")\n",
    "print(\"   - compute_sample_weights(): For WeightedRandomSampler\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
