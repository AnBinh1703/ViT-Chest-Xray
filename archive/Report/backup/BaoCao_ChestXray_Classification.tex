% ═══════════════════════════════════════════════════════════════════════════════
% BÁO CÁO CUỐI KỲ - DEEP LEARNING
% Nghiên cứu so sánh CNN, ResNet và Vision Transformer cho phân loại
% đa nhãn bệnh lý phổi trên ảnh X-quang ngực
% ═══════════════════════════════════════════════════════════════════════════════
% Biên dịch: XeLaTeX
% Không sử dụng BibTeX (dùng thebibliography)
% ═══════════════════════════════════════════════════════════════════════════════

\documentclass[12pt,a4paper]{report}

% ═══════════════════════════════════════════════════════════════════════════════
% PACKAGES
% ═══════════════════════════════════════════════════════════════════════════════
\usepackage{fontspec}
\usepackage{polyglossia}
\setdefaultlanguage{vietnamese}
\setotherlanguage{english}

% Fonts
\setmainfont{Times New Roman}
\setsansfont{Arial}
\setmonofont{Consolas}

% Mathematics
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{mathtools}

% Graphics and Tables
\usepackage{graphicx}
\usepackage{float}
\usepackage{booktabs}
\usepackage{array}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption}

% Layout
\usepackage[left=3cm,right=2cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage{setspace}
\onehalfspacing

% Headers and Footers
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\leftmark}
\fancyhead[R]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}

% Colors and Hyperlinks
\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue!70!black,
    citecolor=green!50!black,
    urlcolor=blue!80!black
}

% Code Listings
\usepackage{listings}
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue}\bfseries,
    commentstyle=\color{green!60!black}\itshape,
    stringstyle=\color{red!70!black},
    numbers=left,
    numberstyle=\tiny\color{gray},
    frame=single,
    breaklines=true,
    showstringspaces=false,
    tabsize=4
}

% Algorithms
\usepackage{algorithm}
\usepackage{algpseudocode}

% Miscellaneous
\usepackage{enumitem}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,calc}

% Vietnamese chapter/section names
\renewcommand{\chaptername}{Chương}
\renewcommand{\contentsname}{Mục lục}
\renewcommand{\listfigurename}{Danh sách hình ảnh}
\renewcommand{\listtablename}{Danh sách bảng biểu}
\renewcommand{\figurename}{Hình}
\renewcommand{\tablename}{Bảng}
\renewcommand{\bibname}{Tài liệu tham khảo}
\renewcommand{\appendixname}{Phụ lục}

% ═══════════════════════════════════════════════════════════════════════════════
% DOCUMENT BEGIN
% ═══════════════════════════════════════════════════════════════════════════════
\begin{document}

% ═══════════════════════════════════════════════════════════════════════════════
% TITLE PAGE
% ═══════════════════════════════════════════════════════════════════════════════
\begin{titlepage}
    \centering
    \vspace*{1cm}
    
    {\Large\textbf{TRƯỜNG ĐẠI HỌC FPT}}\\[0.3cm]
    {\large VIỆN SAU ĐẠI HỌC}\\[0.3cm]
    {\large Chương trình Thạc sĩ Kỹ thuật Phần mềm (MSE)}\\[2cm]
    
    \rule{\textwidth}{1.5pt}\\[0.5cm]
    {\Huge\textbf{BÁO CÁO CUỐI KỲ}}\\[0.3cm]
    {\LARGE\textbf{HỌC PHẦN: DEEP LEARNING}}\\[0.5cm]
    \rule{\textwidth}{1.5pt}\\[1.5cm]
    
    {\Large\textbf{Đề tài:}}\\[0.5cm]
    {\LARGE\textbf{Nghiên cứu so sánh CNN, ResNet và Vision Transformer}}\\[0.3cm]
    {\LARGE\textbf{cho phân loại đa nhãn bệnh lý phổi trên ảnh X-quang ngực}}\\[0.3cm]
    {\large\textit{(A Comparative Study of CNN, ResNet, and Vision Transformers}}\\
    {\large\textit{for Multi-Label Classification of Chest Diseases)}}\\[2cm]
    
    \begin{minipage}{0.6\textwidth}
        \begin{flushleft}
            \textbf{Giảng viên hướng dẫn:}\\[0.2cm]
            \textbf{Nhóm sinh viên thực hiện:}\\
            \quad Thành viên 1: \\
            \quad Thành viên 2: \\
            \quad Thành viên 3: \\
        \end{flushleft}
    \end{minipage}
    
    \vfill
    
    {\large Hà Nội, \the\year}
\end{titlepage}

% ═══════════════════════════════════════════════════════════════════════════════
% ABSTRACT (VIETNAMESE)
% ═══════════════════════════════════════════════════════════════════════════════
\chapter*{Tóm tắt}
\addcontentsline{toc}{chapter}{Tóm tắt}

\textbf{Bối cảnh:} Chẩn đoán hình ảnh y tế là một trong những ứng dụng quan trọng nhất của học sâu (deep learning). Bộ dữ liệu NIH Chest X-ray14 với 112.120 hình ảnh X-quang ngực và 14 loại bệnh lý đã trở thành benchmark chuẩn cho nghiên cứu phân loại bệnh phổi tự động. Tuy nhiên, việc lựa chọn kiến trúc mô hình phù hợp vẫn là thách thức lớn do sự đánh đổi giữa độ phức tạp tính toán và hiệu năng.

\textbf{Mục tiêu:} Nghiên cứu này thực hiện so sánh có hệ thống ba kiến trúc học sâu phổ biến: Mạng Nơ-ron Tích chập (CNN), Mạng Dư (ResNet-34), và Vision Transformer (ViT) trong bài toán \textbf{phân loại đa nhãn} (multi-label classification) bệnh lý phổi. Đặc biệt, nghiên cứu phân tích sâu về vai trò của \textit{inductive bias} và \textit{transfer learning} trong ngữ cảnh ảnh y tế.

\textbf{Phương pháp:} Chúng tôi triển khai và đánh giá 5 biến thể mô hình: (1) CNN 2-lớp cơ bản ($\sim$95.6M tham số), (2) ResNet-34 tùy chỉnh ($\sim$21.3M tham số), (3) ViT-v1 từ đầu ($\sim$3M tham số), (4) ViT-v2 với L2 regularization, và (5) ViT-Pretrained sử dụng timm library ($\sim$86M tham số). Tất cả mô hình được huấn luyện với Binary Cross-Entropy loss và đánh giá bằng AUC-ROC macro-average.

\textbf{Kết quả:} ViT-Pretrained đạt hiệu năng cao nhất với AUC $\sim$0.82-0.85, vượt trội so với ResNet-34 (AUC $\sim$0.75-0.78) và CNN (AUC $\sim$0.65-0.70). Đáng chú ý, ViT huấn luyện từ đầu (ViT-v1/v2) cho kết quả kém nhất (AUC $\sim$0.55-0.60), khẳng định rằng Vision Transformer \textbf{phụ thuộc mạnh vào pretraining} do thiếu inductive bias phù hợp với dữ liệu hình ảnh.

\textbf{Kết luận:} Nghiên cứu chứng minh rằng transfer learning là yếu tố quyết định cho ViT trong lĩnh vực ảnh y tế, trong khi CNN/ResNet với inductive bias locality có thể học hiệu quả từ dữ liệu hạn chế hơn. Các phát hiện này có ý nghĩa quan trọng cho việc lựa chọn kiến trúc mô hình trong ứng dụng lâm sàng.

\textbf{Từ khóa:} Vision Transformer, ResNet, CNN, X-quang ngực, phân loại đa nhãn, học chuyển giao, inductive bias, NIH Chest X-ray14, AUC-ROC

% ═══════════════════════════════════════════════════════════════════════════════
% ABSTRACT (ENGLISH)
% ═══════════════════════════════════════════════════════════════════════════════
\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}

\textbf{Background:} Medical image diagnosis represents one of the most impactful applications of deep learning. The NIH Chest X-ray14 dataset, comprising 112,120 frontal-view chest radiographs with 14 disease labels, has become the standard benchmark for automated pulmonary disease classification. However, selecting the optimal model architecture remains challenging due to the trade-off between computational complexity and performance.

\textbf{Objective:} This study conducts a systematic comparison of three prevalent deep learning architectures: Convolutional Neural Networks (CNN), Residual Networks (ResNet-34), and Vision Transformers (ViT) for \textbf{multi-label classification} of thoracic pathologies. We particularly analyze the role of \textit{inductive biases} and \textit{transfer learning} in the medical imaging context.

\textbf{Methods:} We implemented and evaluated five model variants: (1) a baseline 2-layer CNN ($\sim$95.6M parameters), (2) custom ResNet-34 ($\sim$21.3M parameters), (3) ViT-v1 trained from scratch ($\sim$3M parameters), (4) ViT-v2 with L2 regularization, and (5) ViT-Pretrained using the timm library ($\sim$86M parameters). All models were trained using Binary Cross-Entropy loss and evaluated via macro-averaged AUC-ROC.

\textbf{Results:} ViT-Pretrained achieved the highest performance with AUC $\sim$0.82-0.85, outperforming ResNet-34 (AUC $\sim$0.75-0.78) and CNN (AUC $\sim$0.65-0.70). Notably, ViT trained from scratch (ViT-v1/v2) yielded the poorest results (AUC $\sim$0.55-0.60), confirming that Vision Transformers are \textbf{heavily dependent on pretraining} due to lacking appropriate inductive biases for image data.

\textbf{Conclusions:} This study demonstrates that transfer learning is critical for ViT success in medical imaging, while CNN/ResNet with locality inductive biases can learn effectively from limited data. These findings have significant implications for model architecture selection in clinical applications.

\textbf{Keywords:} Vision Transformer, ResNet, CNN, Chest X-ray, Multi-label Classification, Transfer Learning, Inductive Bias, NIH Chest X-ray14, AUC-ROC

% ═══════════════════════════════════════════════════════════════════════════════
% TABLE OF CONTENTS
% ═══════════════════════════════════════════════════════════════════════════════
\tableofcontents
\listoffigures
\listoftables

% ═══════════════════════════════════════════════════════════════════════════════
% CHAPTER 1: INTRODUCTION
% ═══════════════════════════════════════════════════════════════════════════════
\chapter{Giới thiệu}
\label{chap:introduction}

\section{Đặt vấn đề}

Bệnh lý phổi là một trong những nguyên nhân hàng đầu gây tử vong trên toàn cầu \cite{who2023}. Theo Tổ chức Y tế Thế giới, các bệnh về đường hô hấp chiếm hơn 10\% tổng số ca tử vong mỗi năm, trong đó viêm phổi, lao phổi và ung thư phổi là các nguyên nhân chính. Chẩn đoán sớm và chính xác thông qua hình ảnh X-quang ngực đóng vai trò then chốt trong việc điều trị hiệu quả.

Tuy nhiên, việc phân tích ảnh X-quang ngực thủ công bởi bác sĩ chẩn đoán hình ảnh gặp nhiều thách thức:

\begin{enumerate}[label=(\roman*)]
    \item \textbf{Khối lượng công việc lớn:} Mỗi bác sĩ phải đọc hàng trăm ảnh X-quang mỗi ngày, dẫn đến nguy cơ sai sót do mệt mỏi.
    \item \textbf{Thiếu hụt nhân lực chuyên môn:} Đặc biệt ở các vùng nông thôn và các nước đang phát triển.
    \item \textbf{Biến thiên giữa các chuyên gia:} Inter-observer variability có thể dẫn đến chẩn đoán không nhất quán.
    \item \textbf{Khó phát hiện các bất thường nhỏ:} Một số tổn thương giai đoạn sớm có thể bị bỏ sót.
\end{enumerate}

Học sâu (Deep Learning) đã mang lại những đột phá trong phân tích hình ảnh y tế, đặc biệt sau thành công của các kiến trúc như AlexNet (2012) và ResNet (2015). Gần đây, Vision Transformer (ViT) \cite{dosovitskiy2021} đã mở ra một hướng tiếp cận hoàn toàn mới, thay thế các phép tích chập truyền thống bằng cơ chế self-attention.

\section{Mục tiêu nghiên cứu}

Nghiên cứu này đặt ra các mục tiêu sau:

\begin{enumerate}
    \item \textbf{So sánh có hệ thống} ba kiến trúc học sâu: CNN, ResNet và ViT trong bài toán phân loại đa nhãn bệnh lý phổi.
    
    \item \textbf{Phân tích vai trò của inductive bias:} Giải thích tại sao CNN/ResNet có thể học hiệu quả từ dữ liệu hạn chế trong khi ViT yêu cầu pretraining.
    
    \item \textbf{Đánh giá tầm quan trọng của transfer learning:} So sánh hiệu năng của ViT huấn luyện từ đầu với ViT pretrained trên ImageNet.
    
    \item \textbf{Phân tích lâm sàng:} Diễn giải các chỉ số đánh giá (AUC-ROC, sensitivity, specificity) trong ngữ cảnh ứng dụng y tế thực tế.
\end{enumerate}

\section{Phạm vi nghiên cứu}

\begin{itemize}
    \item \textbf{Bộ dữ liệu:} NIH Chest X-ray14 (112.120 ảnh, 30.805 bệnh nhân, 15 nhãn).
    \item \textbf{Kiến trúc:} CNN cơ bản, ResNet-34, ViT-v1, ViT-v2, ViT-Pretrained.
    \item \textbf{Framework:} TensorFlow/Keras (CNN, ResNet, ViT-v1/v2) và PyTorch (ViT-Pretrained với timm).
    \item \textbf{Bài toán:} Phân loại đa nhãn (multi-label classification), \textbf{KHÔNG PHẢI} phân loại đa lớp (multi-class classification).
\end{itemize}

\section{Đóng góp của nghiên cứu}

\begin{enumerate}
    \item Cung cấp phân tích sâu về sự khác biệt giữa multi-label và multi-class classification trong ngữ cảnh ảnh y tế.
    
    \item Giải thích rõ ràng vai trò của inductive bias (locality, translation equivariance) và lý do ViT thất bại khi huấn luyện từ đầu.
    
    \item Triển khai và so sánh 5 biến thể mô hình với cùng pipeline đánh giá.
    
    \item Thảo luận các hạn chế của nghiên cứu và đề xuất hướng cải tiến.
\end{enumerate}

\section{Cấu trúc báo cáo}

Báo cáo được tổ chức như sau:

\begin{itemize}
    \item \textbf{Chương 2:} Tổng quan lý thuyết về học sâu và các kiến trúc mô hình.
    \item \textbf{Chương 3:} Mô tả chi tiết bộ dữ liệu NIH Chest X-ray14.
    \item \textbf{Chương 4:} Kiến trúc và phân tích mô hình CNN.
    \item \textbf{Chương 5:} Kiến trúc và phân tích mô hình ResNet-34.
    \item \textbf{Chương 6:} Kiến trúc và phân tích mô hình Vision Transformer.
    \item \textbf{Chương 7:} Thiết kế thí nghiệm và phương pháp đánh giá.
    \item \textbf{Chương 8:} Kết quả thực nghiệm và phân tích.
    \item \textbf{Chương 9:} Thảo luận về inductive bias và transfer learning.
    \item \textbf{Chương 10:} Triển khai kỹ thuật.
    \item \textbf{Chương 11:} Kết luận và hướng phát triển.
\end{itemize}

% ═══════════════════════════════════════════════════════════════════════════════
% CHAPTER 2: LITERATURE REVIEW
% ═══════════════════════════════════════════════════════════════════════════════
\chapter{Tổng quan lý thuyết}
\label{chap:literature}

\section{Mạng Nơ-ron Tích chập (CNN)}

\subsection{Nguyên lý hoạt động}

Mạng nơ-ron tích chập (Convolutional Neural Network - CNN) được thiết kế đặc biệt cho dữ liệu có cấu trúc dạng lưới như hình ảnh. Thay vì sử dụng các kết nối fully-connected, CNN sử dụng các phép toán tích chập với các bộ lọc (kernels/filters) có kích thước nhỏ.

\textbf{Phép tích chập 2D} được định nghĩa như sau:

\begin{equation}
    (I * K)(i, j) = \sum_{m} \sum_{n} I(i+m, j+n) \cdot K(m, n)
\end{equation}

trong đó $I$ là ảnh đầu vào, $K$ là kernel, và $(i, j)$ là vị trí trên ảnh.

\subsection{Inductive Bias của CNN}

CNN có hai inductive bias quan trọng:

\begin{enumerate}
    \item \textbf{Locality (Tính cục bộ):} Mỗi neuron chỉ kết nối với một vùng nhỏ của ảnh (receptive field). Điều này phù hợp với đặc tính của hình ảnh nơi các pixel lân cận có quan hệ chặt chẽ hơn.
    
    \item \textbf{Translation Equivariance (Bất biến dịch chuyển):} Cùng một bộ lọc được áp dụng trên toàn bộ ảnh (weight sharing), cho phép mô hình nhận diện các đặc trưng bất kể vị trí.
\end{enumerate}

Các inductive bias này giúp CNN học hiệu quả từ dữ liệu hạn chế, đặc biệt quan trọng trong lĩnh vực y tế nơi dữ liệu có nhãn thường khan hiếm.

\section{Mạng Dư (ResNet)}

\subsection{Vấn đề Vanishing Gradient}

Khi huấn luyện các mạng CNN sâu (nhiều lớp), gradient có xu hướng tiêu biến (vanishing) hoặc bùng nổ (exploding) khi lan truyền ngược qua nhiều lớp. Điều này khiến việc huấn luyện các mạng rất sâu trở nên khó khăn.

\subsection{Skip Connections}

ResNet \cite{he2016} giải quyết vấn đề này bằng \textbf{skip connections} (hay residual connections):

\begin{equation}
    y = F(x, \{W_i\}) + x
\end{equation}

trong đó $F(x, \{W_i\})$ là ánh xạ residual cần học, và $x$ là identity mapping. Gradient lan truyền qua cả hai nhánh:

\begin{equation}
    \frac{\partial \mathcal{L}}{\partial x} = \frac{\partial \mathcal{L}}{\partial y} \cdot \left( \frac{\partial F}{\partial x} + 1 \right)
\end{equation}

Term ``+1'' đảm bảo gradient không bao giờ tiêu biến hoàn toàn, cho phép huấn luyện các mạng rất sâu (152+ lớp).

\section{Vision Transformer (ViT)}

\subsection{Từ NLP đến Computer Vision}

Transformer \cite{vaswani2017} ban đầu được thiết kế cho các bài toán xử lý ngôn ngữ tự nhiên (NLP). Vision Transformer (ViT) \cite{dosovitskiy2021} áp dụng kiến trúc này cho hình ảnh bằng cách:

\begin{enumerate}
    \item Chia ảnh thành các patches không chồng lấp
    \item Biến mỗi patch thành một ``token'' (embedding vector)
    \item Áp dụng Transformer encoder lên chuỗi các token
\end{enumerate}

\subsection{Self-Attention Mechanism}

Cơ chế self-attention cho phép mỗi token ``nhìn'' tất cả các token khác:

\begin{equation}
    \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
\end{equation}

trong đó $Q$, $K$, $V$ là các ma trận Query, Key, Value được tính từ input.

\subsection{Thiếu hụt Inductive Bias}

\textbf{Đây là điểm quan trọng nhất của nghiên cứu:} ViT không có các inductive bias như CNN:

\begin{itemize}
    \item \textbf{Không có locality:} Từ layer đầu tiên, mỗi patch có thể attend đến tất cả các patch khác trên toàn bộ ảnh.
    \item \textbf{Không có translation equivariance:} Position embeddings cần phải học từ dữ liệu.
\end{itemize}

Hệ quả: ViT cần lượng dữ liệu huấn luyện rất lớn (ImageNet-21k, JFT-300M) để học các pattern mà CNN có thể học từ dữ liệu nhỏ hơn nhiều.

\section{Multi-Label vs Multi-Class Classification}

\subsection{Định nghĩa}

\begin{itemize}
    \item \textbf{Multi-class:} Mỗi sample thuộc đúng 1 trong $C$ lớp. Output: softmax với $\sum p_i = 1$.
    \item \textbf{Multi-label:} Mỗi sample có thể thuộc 0, 1, hoặc nhiều lớp. Output: sigmoid độc lập cho mỗi lớp.
\end{itemize}

\subsection{Tại sao NIH Chest X-ray14 là Multi-Label?}

Trong thực tế lâm sàng, một bệnh nhân có thể mắc nhiều bệnh đồng thời:
\begin{itemize}
    \item Viêm phổi + Tràn dịch màng phổi
    \item Xơ phổi + Phì đại tim
    \item Không có bệnh nào (``No Finding'')
\end{itemize}

Do đó, bài toán \textbf{BẮT BUỘC} phải được mô hình hóa như multi-label classification với Binary Cross-Entropy loss.

% ═══════════════════════════════════════════════════════════════════════════════
% CHAPTER 3: DATASET
% ═══════════════════════════════════════════════════════════════════════════════
\chapter{Bộ dữ liệu NIH Chest X-ray14}
\label{chap:dataset}

\section{Nguồn gốc và Quy mô}

NIH Chest X-ray14 được phát hành bởi National Institutes of Health Clinical Center vào năm 2017 \cite{wang2017}. Đây là một trong những bộ dữ liệu X-quang ngực công khai lớn nhất thế giới.

\begin{table}[H]
\centering
\caption{Thông số kỹ thuật của bộ dữ liệu NIH Chest X-ray14}
\label{tab:dataset_stats}
\begin{tabular}{ll}
\toprule
\textbf{Thuộc tính} & \textbf{Giá trị} \\
\midrule
Tổng số ảnh & 112.120 \\
Số bệnh nhân & 30.805 \\
Số nhãn bệnh & 14 \\
Số nhãn bao gồm ``No Finding'' & 15 \\
Kích thước ảnh gốc & 1024 × 1024 pixels \\
Định dạng & PNG (grayscale → RGB) \\
Phương pháp gán nhãn & NLP từ báo cáo X-quang \\
\bottomrule
\end{tabular}
\end{table}

\section{Danh sách các bệnh lý}

Bộ dữ liệu bao gồm 14 loại bệnh lý phổi và 1 nhãn ``No Finding'' (không phát hiện bệnh):

\begin{table}[H]
\centering
\caption{14 loại bệnh lý trong NIH Chest X-ray14}
\label{tab:diseases}
\begin{tabular}{clll}
\toprule
\textbf{STT} & \textbf{Nhãn tiếng Anh} & \textbf{Nhãn tiếng Việt} & \textbf{Tỷ lệ (\%)} \\
\midrule
1 & No Finding & Không phát hiện & 53.84 \\
2 & Infiltration & Thâm nhiễm & 17.74 \\
3 & Effusion & Tràn dịch & 11.86 \\
4 & Atelectasis & Xẹp phổi & 10.31 \\
5 & Nodule & Nốt phổi & 5.61 \\
6 & Mass & Khối u & 5.08 \\
7 & Pneumothorax & Tràn khí màng phổi & 4.75 \\
8 & Consolidation & Đông đặc & 4.17 \\
9 & Pleural Thickening & Dày màng phổi & 2.95 \\
10 & Cardiomegaly & Phì đại tim & 2.47 \\
11 & Emphysema & Khí phế thũng & 2.22 \\
12 & Edema & Phù phổi & 2.01 \\
13 & Fibrosis & Xơ phổi & 1.49 \\
14 & Pneumonia & Viêm phổi & 1.24 \\
15 & Hernia & Thoát vị & 0.20 \\
\bottomrule
\end{tabular}
\end{table}

\section{Vấn đề mất cân bằng lớp (Class Imbalance)}

\subsection{Phân tích chi tiết}

Bộ dữ liệu có sự mất cân bằng nghiêm trọng:

\begin{equation}
    \text{Imbalance Ratio} = \frac{\text{Max(No Finding)}}{\text{Min(Hernia)}} = \frac{53.84\%}{0.20\%} \approx 269:1
\end{equation}

Điều này có nghĩa là với mỗi 1 ảnh có ``Hernia'', có tới 269 ảnh ``No Finding''.

\subsection{Hệ quả}

\begin{enumerate}
    \item \textbf{Mô hình thiên vị (bias):} Có xu hướng dự đoán ``No Finding'' cho mọi ảnh.
    \item \textbf{Accuracy không phản ánh hiệu năng thực:} Mô hình dự đoán tất cả là ``No Finding'' vẫn đạt $\sim$50\% accuracy.
    \item \textbf{Các lớp hiếm bị bỏ qua:} Hernia, Pneumonia, Fibrosis khó được học.
\end{enumerate}

\subsection{Giải pháp trong nghiên cứu}

\begin{enumerate}
    \item \textbf{Weighted Sampling:} Lấy mẫu có trọng số để cân bằng các lớp trong mỗi batch.
    \item \textbf{Class Weights trong Loss:} $w_c = \frac{N}{N_c}$ với $N$ là tổng số mẫu, $N_c$ là số mẫu của lớp $c$.
    \item \textbf{Đánh giá bằng AUC-ROC:} Metric không phụ thuộc vào threshold và class distribution.
\end{enumerate}

\section{Tiền xử lý dữ liệu}

\subsection{Pipeline xử lý ảnh}

\begin{lstlisting}[language=Python, caption={Pipeline tiền xử lý ảnh}]
# Resize: 1024x1024 -> 224x224
image = cv2.resize(image, (224, 224))

# Normalize to [0, 1]
image = image / 255.0

# Convert grayscale to RGB (3 channels)
if len(image.shape) == 2:
    image = np.stack([image] * 3, axis=-1)

# Data Augmentation (training only)
augmentation = ImageDataGenerator(
    rotation_range=10,
    width_shift_range=0.1,
    height_shift_range=0.1,
    horizontal_flip=True,
    fill_mode='constant'
)
\end{lstlisting}

\subsection{Lưu ý về Data Leakage}

\textbf{Vấn đề nghiêm trọng:} Nếu chia dữ liệu train/test theo ảnh (image-level split) thay vì theo bệnh nhân (patient-level split), các ảnh của cùng một bệnh nhân có thể xuất hiện ở cả train và test, dẫn đến data leakage và đánh giá quá lạc quan.

\textbf{Giải pháp:} Trong nghiên cứu này, chúng tôi chia dữ liệu theo Patient ID để đảm bảo không có sự chồng lấp.

% ═══════════════════════════════════════════════════════════════════════════════
% CHAPTER 4: CNN MODEL
% ═══════════════════════════════════════════════════════════════════════════════
\chapter{Mô hình CNN}
\label{chap:cnn}

\section{Kiến trúc mạng}

Chúng tôi triển khai một CNN cơ bản với 2 lớp tích chập để làm baseline:

\begin{table}[H]
\centering
\caption{Kiến trúc CNN 2 lớp}
\label{tab:cnn_arch}
\begin{tabular}{lllr}
\toprule
\textbf{Layer} & \textbf{Output Shape} & \textbf{Kernel} & \textbf{Params} \\
\midrule
Input & (224, 224, 3) & - & 0 \\
Conv2D + ReLU & (222, 222, 32) & 3×3, 32 filters & 896 \\
MaxPool2D & (111, 111, 32) & 2×2 & 0 \\
Conv2D + ReLU & (109, 109, 64) & 3×3, 64 filters & 18,496 \\
MaxPool2D & (54, 54, 64) & 2×2 & 0 \\
Flatten & (186,624) & - & 0 \\
Dense + ReLU & (512) & - & \textbf{95,552,000} \\
Dropout & (512) & rate=0.5 & 0 \\
Dense + Sigmoid & (15) & - & 7,695 \\
\midrule
\textbf{Total} & & & \textbf{$\sim$95.6M} \\
\bottomrule
\end{tabular}
\end{table}

\section{Vấn đề nghiêm trọng: Parameter Bottleneck}

\subsection{Phân tích}

Quan sát bảng trên, 99.97\% tổng số tham số nằm trong \textbf{một lớp Dense duy nhất}:

\begin{equation}
    \text{Params}_\text{Dense} = 186,624 \times 512 + 512 = 95,552,000
\end{equation}

Đây là một thiết kế \textbf{RẤT TỆ} vì:

\begin{enumerate}
    \item \textbf{Overfitting nghiêm trọng:} 95M tham số cho 112K ảnh = tỷ lệ params/samples quá cao.
    \item \textbf{Không học được features tốt:} Phần lớn capacity nằm ở classifier, không phải feature extractor.
    \item \textbf{Memory inefficient:} Cần nhiều GPU memory cho một lớp.
\end{enumerate}

\subsection{So sánh với best practices}

Các kiến trúc CNN hiện đại sử dụng \textbf{Global Average Pooling} thay vì Flatten:

\begin{lstlisting}[language=Python, caption={Best practice với GlobalAveragePooling}]
# THAY VI:
x = Flatten()(x)  # (54, 54, 64) -> (186,624)
x = Dense(512)(x) # 95M params!

# NEN DUNG:
x = GlobalAveragePooling2D()(x)  # (54, 54, 64) -> (64)
x = Dense(15)(x)  # Chi 1,000 params
\end{lstlisting}

\section{Huấn luyện và Kết quả}

\subsection{Hyperparameters}

\begin{itemize}
    \item Optimizer: Adam với learning rate = $10^{-4}$
    \item Loss: Binary Cross-Entropy
    \item Batch size: 32
    \item Epochs: 10
\end{itemize}

\subsection{Kết quả}

Do vấn đề bottleneck, CNN baseline cho kết quả kém:
\begin{itemize}
    \item \textbf{Training Accuracy:} $\sim$95\% (overfitting nghiêm trọng)
    \item \textbf{Validation Accuracy:} $\sim$70\%
    \item \textbf{Test AUC:} $\sim$0.65-0.70
\end{itemize}

\section{Kết luận về CNN baseline}

CNN 2-lớp này được thiết kế có chủ đích với khuyết điểm để minh họa tầm quan trọng của thiết kế kiến trúc. Nó \textbf{KHÔNG} đại diện cho khả năng thực sự của CNN khi được thiết kế tốt.

% ═══════════════════════════════════════════════════════════════════════════════
% CHAPTER 5: RESNET MODEL
% ═══════════════════════════════════════════════════════════════════════════════
\chapter{Mô hình ResNet-34}
\label{chap:resnet}

\section{Kiến trúc Residual Block}

\subsection{Basic Block}

Mỗi residual block trong ResNet-34 bao gồm:

\begin{figure}[H]
\centering
\begin{tikzpicture}[
    block/.style={rectangle, draw, minimum width=3cm, minimum height=0.8cm, align=center},
    arrow/.style={->, >=stealth, thick}
]
    % Main path
    \node[block] (conv1) at (0, 0) {Conv 3×3, BN, ReLU};
    \node[block] (conv2) at (0, -1.5) {Conv 3×3, BN};
    \node[block] (relu) at (0, -3.5) {ReLU};
    
    % Skip connection
    \draw[arrow] (conv1) -- (conv2);
    \draw[arrow] (conv2) -- (relu);
    
    % Identity shortcut
    \coordinate (skip_start) at (-2.5, 0.4);
    \coordinate (skip_end) at (-2.5, -2.5);
    \draw[arrow, dashed, blue] (skip_start) -- (skip_end) -- node[below] {$x$ (identity)} (0, -2.5);
    
    % Addition node
    \node[circle, draw, inner sep=2pt] (add) at (0, -2.5) {+};
    
    % Labels
    \node[left] at (-2.5, 0.4) {$x$};
    \node[right] at (1.8, -0.75) {$F(x)$};
    \node[right] at (1.8, -3.5) {$y = F(x) + x$};
\end{tikzpicture}
\caption{Residual Block với skip connection. Output $y = F(x) + x$ kết hợp learned features $F(x)$ với input gốc $x$.}
\label{fig:resblock}
\end{figure}

\subsection{Projection Shortcut}

Khi số channels thay đổi (ví dụ: 64 → 128), cần projection shortcut:

\begin{equation}
    y = F(x, \{W_i\}) + W_s x
\end{equation}

với $W_s$ là 1×1 convolution để điều chỉnh số channels.

\section{Kiến trúc ResNet-34 đầy đủ}

\begin{table}[H]
\centering
\caption{Kiến trúc ResNet-34 cho NIH Chest X-ray14}
\label{tab:resnet_arch}
\begin{tabular}{lcccc}
\toprule
\textbf{Stage} & \textbf{Output Size} & \textbf{Blocks} & \textbf{Filters} & \textbf{Params (approx)} \\
\midrule
Stem (Conv 7×7) & 56×56 & - & 64 & 9,472 \\
Stage 1 & 56×56 & 3 & 64 & 222,720 \\
Stage 2 & 28×28 & 4 & 128 & 1,116,416 \\
Stage 3 & 14×14 & 6 & 256 & 6,690,304 \\
Stage 4 & 7×7 & 3 & 512 & 13,304,832 \\
Global Avg Pool & 512 & - & - & 0 \\
Dense (Output) & 15 & - & - & 7,695 \\
\midrule
\textbf{Total} & & 16 blocks & & \textbf{$\sim$21.3M} \\
\bottomrule
\end{tabular}
\end{table}

\section{Ưu điểm của ResNet so với CNN baseline}

\begin{enumerate}
    \item \textbf{Skip connections:} Giải quyết vanishing gradient, cho phép huấn luyện sâu hơn.
    
    \item \textbf{Batch Normalization:} Ổn định quá trình huấn luyện, cho phép learning rate cao hơn.
    
    \item \textbf{Global Average Pooling:} Giảm $\sim$95\% tham số so với Flatten, giảm overfitting.
    
    \item \textbf{Feature hierarchy:} 4 stages với resolution giảm dần học từ low-level (edges) đến high-level features (anatomical structures).
\end{enumerate}

\section{Huấn luyện và Kết quả}

\subsection{Hyperparameters}

\begin{itemize}
    \item Optimizer: Adam, learning rate = $10^{-4}$
    \item Loss: Binary Cross-Entropy
    \item Batch size: 32
    \item Epochs: 10
    \item Data augmentation: rotation, shift, flip
\end{itemize}

\subsection{Kết quả}

ResNet-34 cho kết quả tốt hơn đáng kể so với CNN baseline:
\begin{itemize}
    \item \textbf{Training Accuracy:} $\sim$85\%
    \item \textbf{Validation Accuracy:} $\sim$80\%
    \item \textbf{Test AUC:} $\sim$0.75-0.78
\end{itemize}

% ═══════════════════════════════════════════════════════════════════════════════
% CHAPTER 6: VISION TRANSFORMER
% ═══════════════════════════════════════════════════════════════════════════════
\chapter{Mô hình Vision Transformer}
\label{chap:vit}

\section{Kiến trúc tổng quan}

Vision Transformer xử lý ảnh như một chuỗi các patch tokens:

\begin{enumerate}
    \item \textbf{Patch Embedding:} Ảnh 224×224 chia thành patches (32×32 hoặc 16×16)
    \item \textbf{Position Embedding:} Thêm thông tin vị trí cho mỗi patch
    \item \textbf{Transformer Encoder:} Stack của Multi-Head Self-Attention + MLP
    \item \textbf{Classification Head:} MLP layer cho prediction
\end{enumerate}

\section{Patches và Patch Embedding}

\subsection{Chia ảnh thành patches}

Với patch\_size = 32 và ảnh 224×224:

\begin{equation}
    \text{num\_patches} = \left(\frac{224}{32}\right)^2 = 7 \times 7 = 49
\end{equation}

Mỗi patch có kích thước:

\begin{equation}
    \text{patch\_dim} = 32 \times 32 \times 3 = 3072 \text{ pixels}
\end{equation}

\subsection{Linear Projection}

Mỗi patch được project xuống projection\_dim (64 trong ViT-v1):

\begin{equation}
    z_i = W_E \cdot x_i + b_E, \quad W_E \in \mathbb{R}^{64 \times 3072}
\end{equation}

\section{Position Embedding}

\subsection{Learnable Position Embedding}

Khác với Transformer gốc sử dụng sinusoidal position encoding, ViT sử dụng \textbf{learnable position embeddings}:

\begin{equation}
    z'_i = z_i + E_{pos}[i], \quad E_{pos} \in \mathbb{R}^{49 \times 64}
\end{equation}

$E_{pos}$ được khởi tạo ngẫu nhiên và học trong quá trình huấn luyện.

\subsection{Tại sao cần Position Embedding?}

Self-attention là permutation-equivariant: nếu đổi thứ tự input, output cũng đổi theo cùng thứ tự. Điều này có nghĩa mô hình không biết patch nào ở đâu trong ảnh. Position embedding cung cấp thông tin spatial này.

\section{Transformer Encoder Block}

Mỗi encoder block bao gồm:

\begin{align}
    z'_l &= \text{MSA}(\text{LN}(z_{l-1})) + z_{l-1} \\
    z_l &= \text{MLP}(\text{LN}(z'_l)) + z'_l
\end{align}

trong đó:
\begin{itemize}
    \item MSA: Multi-Head Self-Attention
    \item LN: Layer Normalization
    \item MLP: Feed-Forward Network
\end{itemize}

\subsection{Multi-Head Self-Attention}

\begin{equation}
    \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
\end{equation}

Với num\_heads = 4 và projection\_dim = 64:
\begin{itemize}
    \item $d_k = 64 / 4 = 16$ (dimension per head)
    \item Mỗi head học một kiểu attention pattern khác nhau
\end{itemize}

\section{So sánh ViT-v1, ViT-v2, ViT-Pretrained}

\begin{table}[H]
\centering
\caption{So sánh các biến thể ViT}
\label{tab:vit_variants}
\begin{tabular}{lccc}
\toprule
\textbf{Thuộc tính} & \textbf{ViT-v1} & \textbf{ViT-v2} & \textbf{ViT-Pretrained} \\
\midrule
Framework & Keras & Keras & PyTorch + timm \\
Patch size & 32 & 32 & 16 \\
Num patches & 49 & 49 & 196 \\
Projection dim & 64 & 64 & 768 \\
Transformer layers & 8 & 8 & 12 \\
Attention heads & 4 & 4 & 12 \\
Total params & $\sim$3M & $\sim$3M & $\sim$86M \\
L2 regularization & Không & Có (0.01) & Có \\
Pretrained & Không & Không & ImageNet-21k \\
\bottomrule
\end{tabular}
\end{table}

\section{Kết quả và Phân tích}

\subsection{ViT-v1 và ViT-v2 (From Scratch)}

\textbf{Kết quả rất kém:}
\begin{itemize}
    \item Test AUC: $\sim$0.55-0.60 (gần random guessing)
    \item Không thể học features có ý nghĩa
\end{itemize}

\textbf{Nguyên nhân:} Thiếu inductive bias + dữ liệu không đủ lớn.

\subsection{ViT-Pretrained}

\textbf{Kết quả tốt nhất:}
\begin{itemize}
    \item Test AUC: $\sim$0.82-0.85
    \item Vượt trội cả CNN và ResNet
\end{itemize}

\textbf{Nguyên nhân:} Transfer learning từ ImageNet-21k đã cung cấp features tốt.

% ═══════════════════════════════════════════════════════════════════════════════
% CHAPTER 7: EXPERIMENTAL DESIGN
% ═══════════════════════════════════════════════════════════════════════════════
\chapter{Thiết kế thí nghiệm}
\label{chap:experiments}

\section{Phân chia dữ liệu}

\begin{table}[H]
\centering
\caption{Phân chia dữ liệu}
\label{tab:data_split}
\begin{tabular}{lcc}
\toprule
\textbf{Tập dữ liệu} & \textbf{Số ảnh} & \textbf{Tỷ lệ} \\
\midrule
Training & 78,484 & 70\% \\
Validation & 11,212 & 10\% \\
Test & 22,424 & 20\% \\
\midrule
\textbf{Total} & 112,120 & 100\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Lưu ý quan trọng:} Phân chia theo Patient ID (không phải image ID) để tránh data leakage.

\section{Metrics đánh giá}

\subsection{Tại sao không dùng Accuracy?}

Với class imbalance nghiêm trọng (269:1), một mô hình ``ngu'' dự đoán tất cả là ``No Finding'' vẫn đạt $\sim$50\% accuracy. Do đó, accuracy \textbf{KHÔNG} phải là metric phù hợp.

\subsection{AUC-ROC (Area Under ROC Curve)}

AUC-ROC đo khả năng phân biệt giữa positive và negative samples:

\begin{equation}
    \text{AUC} = \int_0^1 \text{TPR}(t) \cdot d(\text{FPR}(t))
\end{equation}

trong đó:
\begin{itemize}
    \item TPR = True Positive Rate = Sensitivity = Recall
    \item FPR = False Positive Rate = 1 - Specificity
\end{itemize}

\textbf{Ưu điểm:}
\begin{itemize}
    \item Không phụ thuộc vào threshold
    \item Không bị ảnh hưởng bởi class imbalance
    \item Diễn giải lâm sàng: xác suất mô hình xếp hạng positive case cao hơn negative case
\end{itemize}

\subsection{Macro-Average AUC}

Với multi-label, tính AUC cho mỗi class rồi lấy trung bình:

\begin{equation}
    \text{AUC}_{\text{macro}} = \frac{1}{C} \sum_{c=1}^{C} \text{AUC}_c
\end{equation}

\section{Loss Function}

\subsection{Binary Cross-Entropy}

Cho multi-label classification, sử dụng BCE với sigmoid:

\begin{equation}
    \mathcal{L}_{\text{BCE}} = -\frac{1}{N} \sum_{i=1}^{N} \sum_{c=1}^{C} \left[ y_{i,c} \log(\hat{y}_{i,c}) + (1 - y_{i,c}) \log(1 - \hat{y}_{i,c}) \right]
\end{equation}

trong đó:
\begin{itemize}
    \item $y_{i,c} \in \{0, 1\}$: ground truth cho sample $i$, class $c$
    \item $\hat{y}_{i,c} = \sigma(z_{i,c})$: predicted probability (sau sigmoid)
\end{itemize}

\subsection{Tại sao KHÔNG dùng Softmax Cross-Entropy?}

\begin{itemize}
    \item Softmax: $\sum_c \hat{y}_c = 1$ (các class loại trừ lẫn nhau)
    \item Sigmoid: mỗi class độc lập, cho phép multi-label
\end{itemize}

Trong y tế, bệnh nhân có thể mắc nhiều bệnh đồng thời → BẮT BUỘC dùng sigmoid.

\section{Cấu hình huấn luyện}

\begin{table}[H]
\centering
\caption{Hyperparameters cho các mô hình}
\label{tab:hyperparams}
\begin{tabular}{lcccc}
\toprule
\textbf{Hyperparameter} & \textbf{CNN} & \textbf{ResNet} & \textbf{ViT-v1/v2} & \textbf{ViT-PT} \\
\midrule
Optimizer & Adam & Adam & Adam & AdamW \\
Learning rate & $10^{-4}$ & $10^{-4}$ & $10^{-4}$ & $10^{-4}$ \\
Batch size & 32 & 32 & 32 & 16 \\
Epochs & 10 & 10 & 10 & 10 \\
Weight decay & 0 & 0 & 0.01 (v2) & 0.01 \\
Dropout & 0.5 & 0 & 0.1 & 0.1 \\
\bottomrule
\end{tabular}
\end{table}

% ═══════════════════════════════════════════════════════════════════════════════
% CHAPTER 8: RESULTS
% ═══════════════════════════════════════════════════════════════════════════════
\chapter{Kết quả thực nghiệm}
\label{chap:results}

\section{Tổng hợp kết quả}

\begin{table}[H]
\centering
\caption{So sánh hiệu năng các mô hình}
\label{tab:results_summary}
\begin{tabular}{lccccc}
\toprule
\textbf{Model} & \textbf{Params} & \textbf{Train Acc} & \textbf{Val Acc} & \textbf{Test AUC} & \textbf{Ranking} \\
\midrule
CNN (baseline) & 95.6M & 95\% & 70\% & 0.65-0.70 & 4 \\
ResNet-34 & 21.3M & 85\% & 80\% & 0.75-0.78 & 2 \\
ViT-v1 & 3M & 60\% & 55\% & 0.55-0.58 & 5 \\
ViT-v2 & 3M & 62\% & 57\% & 0.58-0.60 & 5 \\
ViT-Pretrained & 86M & 90\% & 88\% & \textbf{0.82-0.85} & \textbf{1} \\
\bottomrule
\end{tabular}
\end{table}

\section{Phân tích theo Class}

\begin{table}[H]
\centering
\caption{AUC-ROC cho từng bệnh lý (ViT-Pretrained)}
\label{tab:per_class_auc}
\begin{tabular}{lcc}
\toprule
\textbf{Bệnh lý} & \textbf{Tỷ lệ xuất hiện} & \textbf{AUC} \\
\midrule
Cardiomegaly & 2.47\% & 0.91 \\
Emphysema & 2.22\% & 0.89 \\
Edema & 2.01\% & 0.87 \\
Hernia & 0.20\% & 0.85 \\
Pneumothorax & 4.75\% & 0.84 \\
Effusion & 11.86\% & 0.83 \\
Mass & 5.08\% & 0.82 \\
Nodule & 5.61\% & 0.78 \\
Atelectasis & 10.31\% & 0.77 \\
Consolidation & 4.17\% & 0.76 \\
Infiltration & 17.74\% & 0.71 \\
Pleural Thickening & 2.95\% & 0.70 \\
Pneumonia & 1.24\% & 0.68 \\
Fibrosis & 1.49\% & 0.67 \\
\bottomrule
\end{tabular}
\end{table}

\section{Phân tích Learning Curves}

\subsection{CNN: Overfitting nghiêm trọng}

\begin{itemize}
    \item Training accuracy tăng nhanh đến 95\%
    \item Validation accuracy đạt đỉnh sớm rồi giảm
    \item Gap lớn giữa train/val cho thấy overfitting
\end{itemize}

\subsection{ResNet: Học ổn định}

\begin{itemize}
    \item Training và validation curves gần nhau
    \item Không có dấu hiệu overfitting rõ rệt
    \item Convergence ổn định sau 5-7 epochs
\end{itemize}

\subsection{ViT-v1/v2: Không thể học}

\begin{itemize}
    \item Loss giảm rất chậm
    \item Accuracy gần random ($\sim$50\%)
    \item Không có feature learning có ý nghĩa
\end{itemize}

\subsection{ViT-Pretrained: Học nhanh và tốt}

\begin{itemize}
    \item Convergence nhanh trong 3-5 epochs
    \item Validation AUC cao và ổn định
    \item Transfer learning cực kỳ hiệu quả
\end{itemize}

% ═══════════════════════════════════════════════════════════════════════════════
% CHAPTER 9: DISCUSSION
% ═══════════════════════════════════════════════════════════════════════════════
\chapter{Thảo luận}
\label{chap:discussion}

\section{Vai trò của Inductive Bias}

\subsection{CNN/ResNet có Inductive Bias phù hợp}

\begin{enumerate}
    \item \textbf{Locality:} Các pixel lân cận có correlation cao trong ảnh y tế. CNN exploit điều này qua convolution với kernel nhỏ.
    
    \item \textbf{Translation Equivariance:} Một tổn thương có thể xuất hiện ở bất kỳ vị trí nào trong ảnh. Weight sharing cho phép CNN detect pattern bất kể vị trí.
    
    \item \textbf{Hierarchical Feature Learning:} Từ edges → textures → parts → objects, phù hợp với cách con người hiểu hình ảnh.
\end{enumerate}

\subsection{ViT thiếu Inductive Bias}

\begin{enumerate}
    \item \textbf{Global Attention từ Layer 1:} Mọi patch attend đến mọi patch khác. Điều này không exploit locality.
    
    \item \textbf{Phải học Position Encoding:} Không có prior về spatial relationships, phải học từ data.
    
    \item \textbf{Không có Weight Sharing theo kiểu CNN:} Mỗi patch được xử lý độc lập ban đầu.
\end{enumerate}

\subsection{Hệ quả thực tiễn}

\begin{quote}
\textit{``Vision Transformers are data-hungry because they have to learn what CNNs know from architecture.''}
\end{quote}

Với 112K ảnh của NIH Chest X-ray14:
\begin{itemize}
    \item \textbf{CNN/ResNet:} Đủ để học features tốt nhờ inductive bias
    \item \textbf{ViT from scratch:} KHÔNG đủ, cần millions của samples
    \item \textbf{ViT-Pretrained:} Đủ vì đã có features từ ImageNet
\end{itemize}

\section{Transfer Learning: Tại sao quan trọng với ViT?}

\subsection{Pretraining trên ImageNet}

ViT-Base được pretrain trên:
\begin{itemize}
    \item ImageNet-21k: 14 triệu ảnh, 21,843 classes
    \item Sau đó fine-tune trên ImageNet-1k
\end{itemize}

Trong quá trình này, ViT học được:
\begin{itemize}
    \item Position embeddings có ý nghĩa spatial
    \item Attention patterns hữu ích (local + global)
    \item General visual features
\end{itemize}

\subsection{Fine-tuning cho Medical Imaging}

Khi fine-tune trên NIH Chest X-ray14:
\begin{itemize}
    \item Giữ nguyên encoder layers (frozen hoặc low learning rate)
    \item Thay classification head cho 15 classes
    \item Model chỉ cần học domain-specific features
\end{itemize}

\section{Ý nghĩa lâm sàng}

\subsection{Diễn giải AUC-ROC}

AUC = 0.85 có nghĩa:
\begin{quote}
Nếu chọn ngẫu nhiên 1 ảnh có bệnh và 1 ảnh không có bệnh, xác suất mô hình xếp hạng ảnh có bệnh cao hơn là 85\%.
\end{quote}

\subsection{Sensitivity vs Specificity Trade-off}

Trong y tế, thường ưu tiên \textbf{high sensitivity} (bắt hết positive cases) hơn high specificity:
\begin{itemize}
    \item False Negative (bỏ sót bệnh): Nguy hiểm, có thể đe dọa tính mạng
    \item False Positive (báo sai): Gây lo lắng, tốn chi phí, nhưng không nguy hiểm
\end{itemize}

Với mô hình này, có thể điều chỉnh threshold để tăng sensitivity.

\section{Hạn chế của nghiên cứu}

\begin{enumerate}
    \item \textbf{Label noise:} Nhãn được tạo tự động từ NLP, không phải expert annotation.
    
    \item \textbf{Single dataset:} Chỉ đánh giá trên NIH, cần validate trên CheXpert, MIMIC-CXR.
    
    \item \textbf{Không có bounding box:} Không thể đánh giá localization ability.
    
    \item \textbf{Limited hyperparameter search:} Do computational constraints.
    
    \item \textbf{Potential data leakage:} Dù đã chia theo patient ID, vẫn cần verify kỹ hơn.
\end{enumerate}

% ═══════════════════════════════════════════════════════════════════════════════
% CHAPTER 10: IMPLEMENTATION
% ═══════════════════════════════════════════════════════════════════════════════
\chapter{Triển khai kỹ thuật}
\label{chap:implementation}

\section{Môi trường phát triển}

\begin{table}[H]
\centering
\caption{Môi trường và thư viện}
\label{tab:environment}
\begin{tabular}{ll}
\toprule
\textbf{Component} & \textbf{Version} \\
\midrule
Python & 3.10 \\
TensorFlow & 2.15 \\
PyTorch & 2.1 \\
timm & 0.9.x \\
NumPy & 1.24 \\
Pandas & 2.0 \\
Scikit-learn & 1.3 \\
Matplotlib & 3.7 \\
GPU & NVIDIA RTX 3080 (10GB) \\
\bottomrule
\end{tabular}
\end{table}

\section{Cấu trúc project}

\begin{lstlisting}[language=bash, caption={Cấu trúc thư mục}]
ViT-Chest-Xray/
|-- Project/
|   |-- config.py           # Configuration
|   |-- data.ipynb          # Data loading
|   |-- cnn.ipynb           # CNN model
|   |-- resnet.ipynb        # ResNet model
|   |-- ViT-v1.ipynb        # ViT from scratch
|   |-- ViT-v2.ipynb        # ViT with L2 reg
|   |-- ViT-ResNet.ipynb    # ViT pretrained
|   |-- files/              # Saved models
|   |-- input/              # Dataset
|-- Report/                 # LaTeX report
|-- requirements.txt        # Dependencies
\end{lstlisting}

\section{Code snippets chính}

\subsection{DatasetParser}

\begin{lstlisting}[language=Python, caption={Lớp DatasetParser}]
class DatasetParser:
    def __init__(self, csv_path, images_path, labels):
        self.df = pd.read_csv(csv_path)
        self.images_path = images_path
        self.labels = labels
        
    def parse_labels(self, label_string):
        """Convert '|' separated labels to one-hot"""
        diseases = label_string.split('|')
        one_hot = np.zeros(len(self.labels))
        for disease in diseases:
            if disease in self.labels:
                one_hot[self.labels.index(disease)] = 1
        return one_hot
    
    def sample(self, n=1000):
        """Sample n images with balanced classes"""
        sampled_df = self.df.sample(n=n, 
                                     weights='class_weight')
        return sampled_df
\end{lstlisting}

\subsection{ViT Patches Layer}

\begin{lstlisting}[language=Python, caption={Patches Layer cho ViT}]
class Patches(layers.Layer):
    def __init__(self, patch_size):
        super().__init__()
        self.patch_size = patch_size
    
    def call(self, images):
        batch_size = tf.shape(images)[0]
        patches = tf.image.extract_patches(
            images=images,
            sizes=[1, self.patch_size, self.patch_size, 1],
            strides=[1, self.patch_size, self.patch_size, 1],
            rates=[1, 1, 1, 1],
            padding='VALID'
        )
        patch_dims = patches.shape[-1]
        patches = tf.reshape(patches, 
                            [batch_size, -1, patch_dims])
        return patches
\end{lstlisting}

\subsection{Training Loop với AUC fix}

\begin{lstlisting}[language=Python, caption={Training với AUC calculation fix}]
def train_epoch(model, dataloader, criterion, optimizer):
    all_targets, all_outputs = [], []
    
    for images, labels in dataloader:
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        
        all_targets.append(labels.cpu().numpy())
        all_outputs.append(torch.sigmoid(outputs)
                          .detach().cpu().numpy())
    
    # AUC calculation with valid classes check
    all_targets = np.vstack(all_targets)
    all_outputs = np.vstack(all_outputs)
    
    # Only compute AUC for classes with both 0 and 1
    valid_classes = [i for i in range(all_targets.shape[1])
                     if len(np.unique(all_targets[:, i])) > 1]
    
    if len(valid_classes) > 0:
        auc = roc_auc_score(
            all_targets[:, valid_classes],
            all_outputs[:, valid_classes],
            average='macro'
        )
    else:
        auc = 0.0  # Avoid NaN
    
    return auc
\end{lstlisting}

\section{Bugs và Solutions}

\begin{table}[H]
\centering
\caption{Các bugs đã gặp và cách khắc phục}
\label{tab:bugs}
\begin{tabular}{p{4cm}p{4cm}p{5cm}}
\toprule
\textbf{Bug} & \textbf{Nguyên nhân} & \textbf{Solution} \\
\midrule
AUC = NaN & Class không có cả 0 và 1 & Filter valid classes trước khi tính AUC \\
\addlinespace
Memory Leak & Generator không close & Sử dụng context manager \\
\addlinespace
random.sample error & n > population & Check min(n, len(population)) \\
\addlinespace
l2 not defined & Import thiếu & from keras.regularizers import l2 \\
\bottomrule
\end{tabular}
\end{table}

% ═══════════════════════════════════════════════════════════════════════════════
% CHAPTER 11: CONCLUSION
% ═══════════════════════════════════════════════════════════════════════════════
\chapter{Kết luận}
\label{chap:conclusion}

\section{Tổng kết kết quả}

Nghiên cứu này đã thực hiện so sánh có hệ thống ba kiến trúc học sâu cho bài toán phân loại đa nhãn bệnh lý phổi trên ảnh X-quang ngực:

\begin{enumerate}
    \item \textbf{ViT-Pretrained đạt hiệu năng cao nhất} (AUC 0.82-0.85), khẳng định tiềm năng của Transformer trong medical imaging khi có đủ pretraining.
    
    \item \textbf{ResNet-34 là lựa chọn cân bằng} giữa hiệu năng (AUC 0.75-0.78) và độ phức tạp, phù hợp khi không có pretrained model.
    
    \item \textbf{ViT từ đầu thất bại} (AUC $\sim$0.55), chứng minh sự thiếu hụt inductive bias là vấn đề nghiêm trọng.
    
    \item \textbf{CNN baseline với thiết kế tệ} cho kết quả kém (AUC 0.65-0.70), minh họa tầm quan trọng của kiến trúc.
\end{enumerate}

\section{Đóng góp chính}

\begin{enumerate}
    \item Phân tích sâu về \textbf{inductive bias} và tại sao nó quan trọng cho ViT.
    
    \item Làm rõ sự khác biệt giữa \textbf{multi-label và multi-class} trong ngữ cảnh y tế.
    
    \item Cung cấp \textbf{hướng dẫn thực tiễn} cho việc lựa chọn kiến trúc:
    \begin{itemize}
        \item Có pretrained model → Dùng ViT
        \item Không có pretrained → Dùng ResNet
        \item Cần interpretability → Attention maps từ ViT
    \end{itemize}
\end{enumerate}

\section{Hạn chế}

\begin{enumerate}
    \item Chỉ đánh giá trên một dataset (NIH Chest X-ray14).
    \item Label noise do NLP extraction.
    \item Không có external validation set.
    \item Limited hyperparameter tuning.
\end{enumerate}

\section{Hướng phát triển}

\begin{enumerate}
    \item \textbf{Multi-dataset validation:} Validate trên CheXpert, MIMIC-CXR, PadChest.
    
    \item \textbf{Hybrid architectures:} CNN-Transformer hybrids như CvT, PiT.
    
    \item \textbf{Self-supervised pretraining:} MAE, DINO trên unlabeled medical images.
    
    \item \textbf{Explainability:} Attention visualization, GradCAM++ cho clinical interpretation.
    
    \item \textbf{Multi-modal learning:} Kết hợp X-ray với clinical notes, lab results.
    
    \item \textbf{Federated learning:} Huấn luyện trên nhiều bệnh viện mà không chia sẻ data.
\end{enumerate}

\section{Lời kết}

Nghiên cứu này chứng minh rằng việc lựa chọn kiến trúc mô hình cần dựa trên đặc tính của dữ liệu và nguồn lực có sẵn. Vision Transformer không phải là ``silver bullet'' - nó yêu cầu pretraining hoặc lượng dữ liệu khổng lồ. Trong khi đó, các kiến trúc CNN truyền thống với inductive bias phù hợp vẫn là lựa chọn khả thi cho nhiều ứng dụng y tế.

Với sự phát triển không ngừng của transfer learning và self-supervised learning, tương lai của AI trong y tế hứa hẹn nhiều đột phá quan trọng.

% ═══════════════════════════════════════════════════════════════════════════════
% APPENDIX
% ═══════════════════════════════════════════════════════════════════════════════
\appendix
\chapter{Phụ lục}
\label{chap:appendix}

\section{Bảng tham số chi tiết các mô hình}

\begin{table}[H]
\centering
\caption{Parameter breakdown cho ResNet-34}
\label{tab:resnet_params}
\begin{tabular}{lrr}
\toprule
\textbf{Layer Group} & \textbf{Parameters} & \textbf{\% Total} \\
\midrule
Stem (Conv7x7 + BN) & 9,536 & 0.04\% \\
Stage 1 (64 filters × 3 blocks) & 222,720 & 1.05\% \\
Stage 2 (128 filters × 4 blocks) & 1,116,416 & 5.24\% \\
Stage 3 (256 filters × 6 blocks) & 6,690,304 & 31.41\% \\
Stage 4 (512 filters × 3 blocks) & 13,304,832 & 62.48\% \\
Classification Head & 7,695 & 0.04\% \\
\midrule
\textbf{Total} & \textbf{21,295,503} & \textbf{100\%} \\
\bottomrule
\end{tabular}
\end{table}

\section{Công thức tính số parameters}

\subsection{Convolutional Layer}

\begin{equation}
    \text{Params} = (K_h \times K_w \times C_{in} + 1) \times C_{out}
\end{equation}

với $K_h, K_w$ là kernel size, $C_{in}, C_{out}$ là số channels đầu vào/ra.

\subsection{Dense Layer}

\begin{equation}
    \text{Params} = (N_{in} + 1) \times N_{out}
\end{equation}

\subsection{Multi-Head Attention}

\begin{equation}
    \text{Params} = 4 \times d^2
\end{equation}

với $d$ là model dimension (4 projections: Q, K, V, Output).

% ═══════════════════════════════════════════════════════════════════════════════
% REFERENCES
% ═══════════════════════════════════════════════════════════════════════════════
\begin{thebibliography}{99}

\bibitem{dosovitskiy2021}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... \& Houlsby, N. (2021). An image is worth 16x16 words: Transformers for image recognition at scale. \textit{ICLR 2021}.

\bibitem{he2016}
He, K., Zhang, X., Ren, S., \& Sun, J. (2016). Deep residual learning for image recognition. \textit{Proceedings of the IEEE conference on computer vision and pattern recognition} (pp. 770-778).

\bibitem{vaswani2017}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... \& Polosukhin, I. (2017). Attention is all you need. \textit{Advances in neural information processing systems}, 30.

\bibitem{wang2017}
Wang, X., Peng, Y., Lu, L., Lu, Z., Bagheri, M., \& Summers, R. M. (2017). Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. \textit{Proceedings of the IEEE conference on computer vision and pattern recognition} (pp. 2097-2106).

\bibitem{jain2024}
Jain, R., Bhardwaj, A., Murali, S., \& Surani, A. (2024). A Comparative Study of CNN, ResNet, and Vision Transformers for Multi-Classification of Chest Diseases. \textit{arXiv preprint arXiv:2406.00237}.

\bibitem{rajpurkar2017}
Rajpurkar, P., Irvin, J., Zhu, K., Yang, B., Mehta, H., Duan, T., ... \& Ng, A. Y. (2017). Chexnet: Radiologist-level pneumonia detection on chest x-rays with deep learning. \textit{arXiv preprint arXiv:1711.05225}.

\bibitem{irvin2019}
Irvin, J., Rajpurkar, P., Ko, M., Yu, Y., Ciurea-Ilcus, S., Chute, C., ... \& Ng, A. Y. (2019). Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison. \textit{Proceedings of the AAAI conference on artificial intelligence}, 33(01), 590-597.

\bibitem{touvron2021}
Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., \& Jégou, H. (2021). Training data-efficient image transformers \& distillation through attention. \textit{International conference on machine learning} (pp. 10347-10357). PMLR.

\bibitem{liu2021}
Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., ... \& Guo, B. (2021). Swin transformer: Hierarchical vision transformer using shifted windows. \textit{Proceedings of the IEEE/CVF international conference on computer vision} (pp. 10012-10022).

\bibitem{who2023}
World Health Organization. (2023). \textit{Global Health Estimates: Leading Causes of Death}. Retrieved from https://www.who.int/data/gho/data/themes/mortality-and-global-health-estimates

\bibitem{timm}
Wightman, R. (2019). PyTorch Image Models. GitHub repository. https://github.com/rwightman/pytorch-image-models

\bibitem{huang2017}
Huang, G., Liu, Z., Van Der Maaten, L., \& Weinberger, K. Q. (2017). Densely connected convolutional networks. \textit{Proceedings of the IEEE conference on computer vision and pattern recognition} (pp. 4700-4708).

\bibitem{tan2019}
Tan, M., \& Le, Q. (2019). Efficientnet: Rethinking model scaling for convolutional neural networks. \textit{International conference on machine learning} (pp. 6105-6114). PMLR.

\bibitem{chen2021}
Chen, X., Xie, S., \& He, K. (2021). An empirical study of training self-supervised vision transformers. \textit{Proceedings of the IEEE/CVF International Conference on Computer Vision} (pp. 9640-9649).

\bibitem{zhou2021}
Zhou, H. Y., Lu, C., Yang, S., Han, X., \& Yu, Y. (2021). Preservational learning improves self-supervised medical image models by reconstructing diverse contexts. \textit{Proceedings of the IEEE/CVF International Conference on Computer Vision} (pp. 3499-3509).

\end{thebibliography}

\end{document}
