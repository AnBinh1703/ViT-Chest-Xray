{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a51a9f88",
   "metadata": {},
   "source": [
    "# ðŸ”¬ Final ViT Chest X-ray Multi-Label Classification\n",
    "\n",
    "**Comprehensive Implementation - Group 1 Deep Learning Project**\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup & Configuration](#1-setup--configuration)\n",
    "2. [Data Pipeline](#2-data-pipeline)\n",
    "3. [Model Architectures](#3-model-architectures)\n",
    "4. [Training Infrastructure](#4-training-infrastructure)\n",
    "5. [Evaluation & Visualization](#5-evaluation--visualization)\n",
    "6. [Results Export](#6-results-export)\n",
    "\n",
    "---\n",
    "\n",
    "**Reference Paper:** [arXiv:2406.00237](https://arxiv.org/abs/2406.00237)  \n",
    "**Original Repo:** [Aviral-03/ViT-Chest-Xray](https://github.com/Aviral-03/ViT-Chest-Xray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a370842d",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f29f110e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to: 42\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# REPRODUCIBILITY SETUP\n",
    "# =============================================================================\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(f\"Random seed set to: {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c539c755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.13.7 (tags/v3.13.7:bcee1c3, Aug 14 2025, 14:15:11) [MSC v.1944 64 bit (AMD64)]\n",
      "PyTorch: 2.10.0+cu126\n",
      "NumPy: 2.4.1\n",
      "Pandas: 3.0.0\n",
      "timm: 1.0.24\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS\n",
    "# =============================================================================\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import math\n",
    "import json\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Models\n",
    "try:\n",
    "    import timm\n",
    "    TIMM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TIMM_AVAILABLE = False\n",
    "    print(\"Warning: timm not installed. Pre-trained ViT models unavailable.\")\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, precision_recall_curve\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import cycle\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Print versions\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "if TIMM_AVAILABLE:\n",
    "    print(f\"timm: {timm.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03952ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CONFIGURATION\n",
      "============================================================\n",
      "  image_size: 224\n",
      "  batch_size: 32\n",
      "  learning_rate: 0.0001\n",
      "  weight_decay: 1e-05\n",
      "  num_epochs: 10\n",
      "  patch_size: 32\n",
      "  projection_dim: 64\n",
      "  num_heads: 4\n",
      "  transformer_layers: 8\n",
      "  num_classes: 15\n",
      "  threshold: 0.5\n",
      "  seed: 42\n",
      "  device: cuda\n",
      "  timestamp: 2026-02-04T03:48:08.806784\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "class Config:\n",
    "    \"\"\"Centralized configuration for the experiment.\"\"\"\n",
    "    \n",
    "    # Paths (use pathlib for cross-platform compatibility)\n",
    "    PROJECT_ROOT = Path(os.getcwd()).parent if Path(os.getcwd()).name == \"Project\" else Path(os.getcwd())\n",
    "    DATA_ROOT = PROJECT_ROOT / \"Project\" / \"input\"\n",
    "    IMAGES_DIR = DATA_ROOT  # Images are in input folder directly\n",
    "    LABELS_CSV = DATA_ROOT / \"Data_Entry_2017_v2020.csv\"\n",
    "    OUTPUT_DIR = PROJECT_ROOT / \"Project\" / \"files\"\n",
    "    ARTIFACTS_DIR = PROJECT_ROOT / \"Project\" / \"artifacts\"\n",
    "    \n",
    "    # Create directories\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Dataset labels (NIH Chest X-ray 14 + No Finding)\n",
    "    LABELS = [\n",
    "        'Cardiomegaly', 'Emphysema', 'Effusion', 'Hernia', 'Nodule',\n",
    "        'Pneumothorax', 'Atelectasis', 'Pleural_Thickening', 'Mass',\n",
    "        'Edema', 'Consolidation', 'Infiltration', 'Fibrosis', 'Pneumonia',\n",
    "        'No Finding'\n",
    "    ]\n",
    "    NUM_CLASSES = len(LABELS)\n",
    "    \n",
    "    # Image settings\n",
    "    IMAGE_SIZE = 224\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    BATCH_SIZE = 32\n",
    "    LEARNING_RATE = 1e-4\n",
    "    WEIGHT_DECAY = 1e-5\n",
    "    NUM_EPOCHS = 10\n",
    "    EARLY_STOPPING_PATIENCE = 5\n",
    "    \n",
    "    # ViT specific\n",
    "    PATCH_SIZE = 32\n",
    "    PROJECTION_DIM = 64\n",
    "    NUM_HEADS = 4\n",
    "    TRANSFORMER_LAYERS = 8\n",
    "    MLP_HEAD_UNITS = [2048, 1024]\n",
    "    \n",
    "    # Evaluation\n",
    "    THRESHOLD = 0.5\n",
    "    \n",
    "    # Device\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Sample size (for demo/testing)\n",
    "    SAMPLE_SIZE = None  # Set to integer for quick testing, None for full data\n",
    "    \n",
    "    @classmethod\n",
    "    def to_dict(cls):\n",
    "        \"\"\"Export configuration as dictionary.\"\"\"\n",
    "        return {\n",
    "            'image_size': cls.IMAGE_SIZE,\n",
    "            'batch_size': cls.BATCH_SIZE,\n",
    "            'learning_rate': cls.LEARNING_RATE,\n",
    "            'weight_decay': cls.WEIGHT_DECAY,\n",
    "            'num_epochs': cls.NUM_EPOCHS,\n",
    "            'patch_size': cls.PATCH_SIZE,\n",
    "            'projection_dim': cls.PROJECTION_DIM,\n",
    "            'num_heads': cls.NUM_HEADS,\n",
    "            'transformer_layers': cls.TRANSFORMER_LAYERS,\n",
    "            'num_classes': cls.NUM_CLASSES,\n",
    "            'threshold': cls.THRESHOLD,\n",
    "            'seed': SEED,\n",
    "            'device': str(cls.DEVICE),\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def save(cls, filepath=None):\n",
    "        \"\"\"Save configuration to JSON.\"\"\"\n",
    "        if filepath is None:\n",
    "            filepath = cls.ARTIFACTS_DIR / 'config.json'\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(cls.to_dict(), f, indent=2)\n",
    "        print(f\"Config saved to: {filepath}\")\n",
    "\n",
    "# Print configuration\n",
    "print(\"=\" * 60)\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in Config.to_dict().items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a4fd155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "CUDA Version: 12.6\n",
      "GPU Memory: 6.4 GB\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DEVICE CONFIGURATION\n",
    "# =============================================================================\n",
    "device = Config.DEVICE\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e99b5b6",
   "metadata": {},
   "source": [
    "## 2. Data Pipeline\n",
    "\n",
    "### âš ï¸ Important Notes:\n",
    "- **Patient-Level Split**: NIH dataset should be split by Patient ID to avoid data leakage\n",
    "- **Horizontal Flip Warning**: Avoid aggressive horizontal flipping as it reverses anatomical orientation\n",
    "- **Grayscale to RGB**: X-rays are grayscale but models expect RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ea6d519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA PARSER\n",
    "# =============================================================================\n",
    "class DatasetParser:\n",
    "    \"\"\"Parse and prepare NIH Chest X-ray dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_root, labels_csv, labels_list):\n",
    "        self.data_root = Path(data_root)\n",
    "        self.labels_list = labels_list\n",
    "        \n",
    "        # Load labels CSV\n",
    "        self.labels_df = pd.read_csv(labels_csv)\n",
    "        \n",
    "        # Find available images\n",
    "        self.image_paths = list(self.data_root.glob('*.png'))\n",
    "        self.available_images = {p.name for p in self.image_paths}\n",
    "        \n",
    "        # Filter to available images only\n",
    "        self.labels_df = self.labels_df[\n",
    "            self.labels_df['Image Index'].isin(self.available_images)\n",
    "        ].reset_index(drop=True)\n",
    "        \n",
    "        # Parse multi-labels\n",
    "        self.labels_df['Labels'] = self.labels_df['Finding Labels'].apply(\n",
    "            lambda x: x.split('|')\n",
    "        )\n",
    "        \n",
    "        # Extract Patient ID from filename (format: 00000001_000.png)\n",
    "        # Note: NIH dataset doesn't have Patient ID column, we extract from filename\n",
    "        self.labels_df['Patient ID'] = self.labels_df['Image Index'].apply(\n",
    "            lambda x: x.split('_')[0]\n",
    "        )\n",
    "        \n",
    "    def get_patient_level_split(self, test_size=0.2, val_size=0.125, random_state=42):\n",
    "        \"\"\"\n",
    "        Split data by Patient ID to avoid data leakage.\n",
    "        \n",
    "        Returns:\n",
    "            train_df, val_df, test_df: DataFrames with image-level data\n",
    "        \"\"\"\n",
    "        # Get unique patient IDs\n",
    "        patient_ids = self.labels_df['Patient ID'].unique()\n",
    "        print(f\"Total patients: {len(patient_ids)}\")\n",
    "        \n",
    "        # Split patients\n",
    "        train_val_patients, test_patients = train_test_split(\n",
    "            patient_ids, test_size=test_size, random_state=random_state\n",
    "        )\n",
    "        train_patients, val_patients = train_test_split(\n",
    "            train_val_patients, test_size=val_size, random_state=random_state\n",
    "        )\n",
    "        \n",
    "        # Split dataframes\n",
    "        train_df = self.labels_df[self.labels_df['Patient ID'].isin(train_patients)].reset_index(drop=True)\n",
    "        val_df = self.labels_df[self.labels_df['Patient ID'].isin(val_patients)].reset_index(drop=True)\n",
    "        test_df = self.labels_df[self.labels_df['Patient ID'].isin(test_patients)].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Train: {len(train_df)} images from {len(train_patients)} patients\")\n",
    "        print(f\"Val: {len(val_df)} images from {len(val_patients)} patients\")\n",
    "        print(f\"Test: {len(test_df)} images from {len(test_patients)} patients\")\n",
    "        \n",
    "        return train_df, val_df, test_df\n",
    "    \n",
    "    def get_image_level_split(self, test_size=0.2, val_size=0.125, random_state=42):\n",
    "        \"\"\"\n",
    "        Standard image-level split (may cause patient leakage).\n",
    "        Use only if patient-level split is not possible.\n",
    "        \"\"\"\n",
    "        print(\"âš ï¸ WARNING: Image-level split may cause patient data leakage!\")\n",
    "        \n",
    "        train_val_df, test_df = train_test_split(\n",
    "            self.labels_df, test_size=test_size, random_state=random_state\n",
    "        )\n",
    "        train_df, val_df = train_test_split(\n",
    "            train_val_df, test_size=val_size, random_state=random_state\n",
    "        )\n",
    "        \n",
    "        return train_df.reset_index(drop=True), val_df.reset_index(drop=True), test_df.reset_index(drop=True)\n",
    "    \n",
    "    def sample(self, n_samples, weighted=True, random_state=42):\n",
    "        \"\"\"Sample a subset of data.\"\"\"\n",
    "        if weighted:\n",
    "            weights = self.labels_df['Labels'].apply(len).values + 0.04\n",
    "            weights /= weights.sum()\n",
    "            return self.labels_df.sample(n_samples, weights=weights, random_state=random_state)\n",
    "        return self.labels_df.sample(n_samples, random_state=random_state)\n",
    "    \n",
    "    def get_class_distribution(self):\n",
    "        \"\"\"Get label distribution for class imbalance analysis.\"\"\"\n",
    "        counts = {label: 0 for label in self.labels_list}\n",
    "        for labels in self.labels_df['Labels']:\n",
    "            for label in labels:\n",
    "                if label in counts:\n",
    "                    counts[label] += 1\n",
    "        return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "308015f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Data root: d:\\MSE\\10.Deep Learning\\Group_Final\\ViT-Chest-Xray\\Project\\input\n",
      "Labels CSV: d:\\MSE\\10.Deep Learning\\Group_Final\\ViT-Chest-Xray\\Project\\input\\Data_Entry_2017_v2020.csv\n",
      "\n",
      "Total images found: 112120\n",
      "\n",
      "Class distribution:\n",
      "  No Finding: 60361\n",
      "  Infiltration: 19894\n",
      "  Effusion: 13317\n",
      "  Atelectasis: 11559\n",
      "  Nodule: 6331\n",
      "  Mass: 5782\n",
      "  Pneumothorax: 5302\n",
      "  Consolidation: 4667\n",
      "  Pleural_Thickening: 3385\n",
      "  Cardiomegaly: 2776\n",
      "  Emphysema: 2516\n",
      "  Edema: 2303\n",
      "  Fibrosis: 1686\n",
      "  Pneumonia: 1431\n",
      "  Hernia: 227\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD AND PARSE DATA\n",
    "# =============================================================================\n",
    "print(\"Loading dataset...\")\n",
    "print(f\"Data root: {Config.DATA_ROOT}\")\n",
    "print(f\"Labels CSV: {Config.LABELS_CSV}\")\n",
    "\n",
    "# Check if data exists\n",
    "if not Config.LABELS_CSV.exists():\n",
    "    print(f\"\\nâŒ ERROR: Labels CSV not found at {Config.LABELS_CSV}\")\n",
    "    print(\"Please download the NIH Chest X-ray dataset first.\")\n",
    "else:\n",
    "    parser = DatasetParser(\n",
    "        data_root=Config.IMAGES_DIR,\n",
    "        labels_csv=Config.LABELS_CSV,\n",
    "        labels_list=Config.LABELS\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTotal images found: {len(parser.labels_df)}\")\n",
    "    \n",
    "    # Show class distribution\n",
    "    class_dist = parser.get_class_distribution()\n",
    "    print(\"\\nClass distribution:\")\n",
    "    for label, count in sorted(class_dist.items(), key=lambda x: -x[1]):\n",
    "        print(f\"  {label}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "652e5aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Using full dataset with patient-level split\n",
      "Total patients: 30805\n",
      "Train: 78614 images from 21563 patients\n",
      "Val: 11212 images from 3081 patients\n",
      "Test: 22294 images from 6161 patients\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PREPARE DATA SPLITS\n",
    "# =============================================================================\n",
    "# Use sample for quick testing, or full data for real training\n",
    "if Config.SAMPLE_SIZE is not None:\n",
    "    print(f\"\\nâš¡ Using sample of {Config.SAMPLE_SIZE} images for quick testing\")\n",
    "    df = parser.sample(Config.SAMPLE_SIZE, weighted=True)\n",
    "    # For sample, use image-level split\n",
    "    train_df, val_df, test_df = train_test_split(\n",
    "        df, test_size=0.2, random_state=SEED\n",
    "    )\n",
    "    train_df, val_df = train_test_split(\n",
    "        train_df, test_size=0.125, random_state=SEED\n",
    "    )\n",
    "else:\n",
    "    print(\"\\nðŸ“Š Using full dataset with patient-level split\")\n",
    "    train_df, val_df, test_df = parser.get_patient_level_split(\n",
    "        test_size=0.2, val_size=0.125, random_state=SEED\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbe49e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PYTORCH DATASET\n",
    "# =============================================================================\n",
    "class ChestXrayDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for NIH Chest X-ray multi-label classification.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe, images_path, labels_list, transform=None):\n",
    "        self.dataframe = dataframe.reset_index(drop=True)\n",
    "        self.images_path = Path(images_path)\n",
    "        self.labels_list = labels_list\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        img_path = self.images_path / row['Image Index']\n",
    "        \n",
    "        # Load image (grayscale X-ray converted to RGB)\n",
    "        image = cv2.imread(str(img_path))\n",
    "        if image is None:\n",
    "            # Handle missing image\n",
    "            image = np.zeros((Config.IMAGE_SIZE, Config.IMAGE_SIZE, 3), dtype=np.uint8)\n",
    "        else:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = Image.fromarray(image)\n",
    "        \n",
    "        # Multi-label encoding\n",
    "        labels = row['Labels'] if isinstance(row['Labels'], list) else row['Labels'].split('|')\n",
    "        one_hot = torch.zeros(len(self.labels_list), dtype=torch.float32)\n",
    "        for label in labels:\n",
    "            if label in self.labels_list:\n",
    "                one_hot[self.labels_list.index(label)] = 1.0\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b38c49bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforms defined:\n",
      "  Train: Compose(\n",
      "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    RandomHorizontalFlip(p=0.3)\n",
      "    RandomRotation(degrees=[-5.0, 5.0], interpolation=nearest, expand=False, fill=0)\n",
      "    ColorJitter(brightness=(0.9, 1.1), contrast=(0.9, 1.1), saturation=None, hue=None)\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "  Val/Test: Compose(\n",
      "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DATA TRANSFORMS\n",
    "# =============================================================================\n",
    "# Training augmentation\n",
    "# âš ï¸ Note: Horizontal flip is included but should be used carefully for medical images\n",
    "#          as it reverses anatomical orientation (e.g., heart position)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((Config.IMAGE_SIZE, Config.IMAGE_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.3),  # Reduced probability\n",
    "    transforms.RandomRotation(degrees=5),    # Small rotation\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# Validation/Test transform (no augmentation)\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((Config.IMAGE_SIZE, Config.IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "print(\"Transforms defined:\")\n",
    "print(f\"  Train: {train_transform}\")\n",
    "print(f\"  Val/Test: {val_test_transform}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e1aadbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Datasets created:\n",
      "  Train: 78614 samples, 2457 batches\n",
      "  Val: 11212 samples, 351 batches\n",
      "  Test: 22294 samples, 697 batches\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CREATE DATASETS AND DATALOADERS\n",
    "# =============================================================================\n",
    "train_dataset = ChestXrayDataset(\n",
    "    train_df, Config.IMAGES_DIR, Config.LABELS, transform=train_transform\n",
    ")\n",
    "val_dataset = ChestXrayDataset(\n",
    "    val_df, Config.IMAGES_DIR, Config.LABELS, transform=val_test_transform\n",
    ")\n",
    "test_dataset = ChestXrayDataset(\n",
    "    test_df, Config.IMAGES_DIR, Config.LABELS, transform=val_test_transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=Config.BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=Config.BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\nDatasets created:\")\n",
    "print(f\"  Train: {len(train_dataset)} samples, {len(train_loader)} batches\")\n",
    "print(f\"  Val: {len(val_dataset)} samples, {len(val_loader)} batches\")\n",
    "print(f\"  Test: {len(test_dataset)} samples, {len(test_loader)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e82c22",
   "metadata": {},
   "source": [
    "## 3. Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f475bbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CNN BASELINE MODEL\n",
    "# =============================================================================\n",
    "class CNNClassifier(nn.Module):\n",
    "    \"\"\"Simple CNN baseline for comparison.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=15):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        # Calculate flatten size: 224->222->111->109->54\n",
    "        self.flatten_size = 64 * 54 * 54\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.flatten_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de0a7545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RESNET-34 MODEL\n",
    "# =============================================================================\n",
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"ResNet Basic Block.\"\"\"\n",
    "    expansion = 1\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \"\"\"ResNet-34 Architecture.\"\"\"\n",
    "    \n",
    "    def __init__(self, block, layers, num_classes=15):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * block.expansion),\n",
    "            )\n",
    "        \n",
    "        layers = [block(self.in_channels, out_channels, stride, downsample)]\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.maxpool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.layer4(self.layer3(self.layer2(self.layer1(x))))\n",
    "        x = torch.flatten(self.avgpool(x), 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def create_resnet34(num_classes=15):\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdeac4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISION TRANSFORMER (FROM SCRATCH)\n",
    "# =============================================================================\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"MLP block for Transformer.\"\"\"\n",
    "    def __init__(self, in_features, hidden_features, out_features, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.gelu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Patch Embedding Layer.\"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=32, in_channels=3, embed_dim=64):\n",
    "        super().__init__()\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"Transformer Encoder Block.\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.mlp = MLP(embed_dim, int(embed_dim * mlp_ratio), embed_dim, dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_norm = self.ln1(x)\n",
    "        attn_out, _ = self.attn(x_norm, x_norm, x_norm)\n",
    "        x = x + attn_out\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"Vision Transformer from scratch.\"\"\"\n",
    "    \n",
    "    def __init__(self, img_size=224, patch_size=32, in_channels=3, num_classes=15,\n",
    "                 embed_dim=64, depth=8, num_heads=4, mlp_ratio=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches, embed_dim))\n",
    "        self.pos_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerEncoderBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        self.ln = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(embed_dim * self.num_patches, 2048),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "        \n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.ones_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_dropout(x)\n",
    "        \n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.ln(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20c681b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architectures defined:\n",
      "  - CNNClassifier\n",
      "  - ResNet-34\n",
      "  - VisionTransformer (scratch)\n",
      "  - Pre-trained ViT (timm)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PRE-TRAINED VIT (using timm)\n",
    "# =============================================================================\n",
    "def create_pretrained_vit(num_classes=15, model_name='vit_base_patch16_224'):\n",
    "    \"\"\"Create pre-trained ViT model using timm.\"\"\"\n",
    "    if not TIMM_AVAILABLE:\n",
    "        raise ImportError(\"timm library required for pre-trained ViT\")\n",
    "    \n",
    "    model = timm.create_model(model_name, pretrained=True)\n",
    "    model.head = nn.Linear(model.head.in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "print(\"Model architectures defined:\")\n",
    "print(\"  - CNNClassifier\")\n",
    "print(\"  - ResNet-34\")\n",
    "print(\"  - VisionTransformer (scratch)\")\n",
    "print(\"  - Pre-trained ViT (timm)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af223e2f",
   "metadata": {},
   "source": [
    "## 4. Training Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e7ff74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EVALUATION METRICS\n",
    "# =============================================================================\n",
    "def compute_metrics(targets, outputs, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Compute multi-label classification metrics.\n",
    "    \n",
    "    Returns:\n",
    "        dict with accuracy, macro_auc, per_class_auc\n",
    "    \"\"\"\n",
    "    preds = (outputs > threshold).astype(int)\n",
    "    \n",
    "    # Element-wise accuracy\n",
    "    accuracy = (preds == targets).mean() * 100\n",
    "    \n",
    "    # Per-class AUC (only for classes with both 0 and 1)\n",
    "    per_class_auc = {}\n",
    "    valid_classes = []\n",
    "    for i in range(targets.shape[1]):\n",
    "        if len(np.unique(targets[:, i])) > 1:\n",
    "            valid_classes.append(i)\n",
    "            per_class_auc[Config.LABELS[i]] = roc_auc_score(targets[:, i], outputs[:, i])\n",
    "    \n",
    "    # Macro AUC (only valid classes)\n",
    "    if len(valid_classes) > 0:\n",
    "        macro_auc = roc_auc_score(\n",
    "            targets[:, valid_classes],\n",
    "            outputs[:, valid_classes],\n",
    "            average='macro'\n",
    "        )\n",
    "    else:\n",
    "        macro_auc = 0.0\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'macro_auc': macro_auc,\n",
    "        'per_class_auc': per_class_auc,\n",
    "        'valid_classes': len(valid_classes)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98575327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAINING FUNCTION\n",
    "# =============================================================================\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, \n",
    "                scheduler=None, num_epochs=10, patience=5, save_path=None, model_name='model'):\n",
    "    \"\"\"\n",
    "    Train model with early stopping and comprehensive logging.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        train_loader, val_loader: DataLoaders\n",
    "        criterion: Loss function (BCEWithLogitsLoss recommended)\n",
    "        optimizer: Optimizer\n",
    "        scheduler: Learning rate scheduler (optional)\n",
    "        num_epochs: Maximum number of epochs\n",
    "        patience: Early stopping patience\n",
    "        save_path: Path to save best model\n",
    "        model_name: Name for logging\n",
    "        \n",
    "    Returns:\n",
    "        history: Training history dictionary\n",
    "    \"\"\"\n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_acc': [], 'val_acc': [],\n",
    "        'train_auc': [], 'val_auc': [],\n",
    "        'lr': []\n",
    "    }\n",
    "    \n",
    "    best_val_auc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        all_train_targets = []\n",
    "        all_train_outputs = []\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=f'[{model_name}] Epoch {epoch+1}/{num_epochs} Train')\n",
    "        for images, labels in train_pbar:\n",
    "            images, labels = images.to(device), labels.to(device).float()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * images.size(0)\n",
    "            all_train_targets.append(labels.cpu().numpy())\n",
    "            all_train_outputs.append(torch.sigmoid(outputs).detach().cpu().numpy())\n",
    "            \n",
    "            train_pbar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        # Compute training metrics\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_targets = np.vstack(all_train_targets)\n",
    "        train_outputs = np.vstack(all_train_outputs)\n",
    "        train_metrics = compute_metrics(train_targets, train_outputs)\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_metrics['accuracy'])\n",
    "        history['train_auc'].append(train_metrics['macro_auc'])\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_val_targets = []\n",
    "        all_val_outputs = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device).float()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                all_val_targets.append(labels.cpu().numpy())\n",
    "                all_val_outputs.append(torch.sigmoid(outputs).cpu().numpy())\n",
    "        \n",
    "        # Compute validation metrics\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_targets = np.vstack(all_val_targets)\n",
    "        val_outputs = np.vstack(all_val_outputs)\n",
    "        val_metrics = compute_metrics(val_targets, val_outputs)\n",
    "        \n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_metrics['accuracy'])\n",
    "        history['val_auc'].append(val_metrics['macro_auc'])\n",
    "        \n",
    "        # Learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        history['lr'].append(current_lr)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} (lr={current_lr:.6f}):')\n",
    "        print(f'  Train - Loss: {train_loss:.4f}, Acc: {train_metrics[\"accuracy\"]:.2f}%, AUC: {train_metrics[\"macro_auc\"]:.4f}')\n",
    "        print(f'  Val   - Loss: {val_loss:.4f}, Acc: {val_metrics[\"accuracy\"]:.2f}%, AUC: {val_metrics[\"macro_auc\"]:.4f}')\n",
    "        \n",
    "        # Scheduler step\n",
    "        if scheduler is not None:\n",
    "            if isinstance(scheduler, ReduceLROnPlateau):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        \n",
    "        # Save best model (by AUC)\n",
    "        if val_metrics['macro_auc'] > best_val_auc:\n",
    "            best_val_auc = val_metrics['macro_auc']\n",
    "            patience_counter = 0\n",
    "            if save_path:\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                print(f'  >> Saved best model with val_auc: {best_val_auc:.4f}')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'\\n>> Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6ab22a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EVALUATION FUNCTION\n",
    "# =============================================================================\n",
    "def evaluate_model(model, test_loader, criterion):\n",
    "    \"\"\"\n",
    "    Evaluate model on test set.\n",
    "    \n",
    "    Returns:\n",
    "        test_loss, metrics_dict, all_targets, all_outputs\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc='Evaluating'):\n",
    "            images, labels = images.to(device), labels.to(device).float()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            test_loss += loss.item() * images.size(0)\n",
    "            all_targets.append(labels.cpu().numpy())\n",
    "            all_outputs.append(torch.sigmoid(outputs).cpu().numpy())\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    targets = np.vstack(all_targets)\n",
    "    outputs = np.vstack(all_outputs)\n",
    "    metrics = compute_metrics(targets, outputs)\n",
    "    \n",
    "    print(f\"\\nTest Results:\")\n",
    "    print(f\"  Loss: {test_loss:.4f}\")\n",
    "    print(f\"  Accuracy: {metrics['accuracy']:.2f}%\")\n",
    "    print(f\"  Macro AUC: {metrics['macro_auc']:.4f}\")\n",
    "    print(f\"\\nPer-class AUC:\")\n",
    "    for label, auc_val in sorted(metrics['per_class_auc'].items(), key=lambda x: -x[1]):\n",
    "        print(f\"    {label}: {auc_val:.4f}\")\n",
    "    \n",
    "    return test_loss, metrics, targets, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78447d4f",
   "metadata": {},
   "source": [
    "## 5. Evaluation & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f2d7ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZATION FUNCTIONS\n",
    "# =============================================================================\n",
    "def plot_training_history(history, model_name='Model', save_path=None):\n",
    "    \"\"\"Plot training history.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0].plot(history['train_loss'], label='Train')\n",
    "    axes[0].plot(history['val_loss'], label='Val')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title(f'{model_name}: Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[1].plot(history['train_acc'], label='Train')\n",
    "    axes[1].plot(history['val_acc'], label='Val')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy (%)')\n",
    "    axes[1].set_title(f'{model_name}: Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    # AUC\n",
    "    axes[2].plot(history['train_auc'], label='Train')\n",
    "    axes[2].plot(history['val_auc'], label='Val')\n",
    "    axes[2].set_xlabel('Epoch')\n",
    "    axes[2].set_ylabel('AUC')\n",
    "    axes[2].set_title(f'{model_name}: AUC')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_curves(targets, outputs, class_labels, title='ROC Curves', save_path=None):\n",
    "    \"\"\"Plot ROC curves for multi-label classification.\"\"\"\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, len(class_labels)))\n",
    "    \n",
    "    macro_fpr = []\n",
    "    macro_tpr = []\n",
    "    \n",
    "    for i, (label, color) in enumerate(zip(class_labels, colors)):\n",
    "        if len(np.unique(targets[:, i])) > 1:\n",
    "            fpr, tpr, _ = roc_curve(targets[:, i], outputs[:, i])\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.plot(fpr, tpr, color=color, lw=2, label=f'{label} (AUC = {roc_auc:.2f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=12)\n",
    "    plt.ylabel('True Positive Rate', fontsize=12)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.legend(loc='lower right', fontsize=8)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bc42ee",
   "metadata": {},
   "source": [
    "## 6. Results Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16142abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RESULTS EXPORT FUNCTIONS\n",
    "# =============================================================================\n",
    "def save_results(results_dict, filepath):\n",
    "    \"\"\"Save results to JSON.\"\"\"\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(results_dict, f, indent=2)\n",
    "    print(f\"Results saved to: {filepath}\")\n",
    "\n",
    "def create_summary_table(all_results):\n",
    "    \"\"\"Create summary DataFrame from all model results.\"\"\"\n",
    "    rows = []\n",
    "    for model_name, results in all_results.items():\n",
    "        rows.append({\n",
    "            'Model': model_name,\n",
    "            'Test Loss': results.get('test_loss', 'N/A'),\n",
    "            'Accuracy (%)': results.get('accuracy', 'N/A'),\n",
    "            'Macro AUC': results.get('macro_auc', 'N/A'),\n",
    "            'Parameters': results.get('parameters', 'N/A')\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def export_artifacts():\n",
    "    \"\"\"Export all artifacts to artifacts directory.\"\"\"\n",
    "    print(f\"\\nExporting artifacts to: {Config.ARTIFACTS_DIR}\")\n",
    "    \n",
    "    # Save config\n",
    "    Config.save()\n",
    "    \n",
    "    print(\"Artifacts exported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6048b08",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸš€ Run Training (Example)\n",
    "\n",
    "Below is an example of how to train and evaluate a model. Uncomment and modify as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40b185d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "315a8306f60e4ec5a6b30074f86dc118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[ViT] Epoch 1/10 Train:   0%|          | 0/2457 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXAMPLE: TRAIN VIT MODEL\n",
    "# =============================================================================\n",
    "\n",
    "# Create model\n",
    "vit_model = VisionTransformer(\n",
    "    img_size=Config.IMAGE_SIZE,\n",
    "    patch_size=Config.PATCH_SIZE,\n",
    "    num_classes=Config.NUM_CLASSES,\n",
    "    embed_dim=Config.PROJECTION_DIM,\n",
    "    depth=Config.TRANSFORMER_LAYERS,\n",
    "    num_heads=Config.NUM_HEADS\n",
    ").to(device)\n",
    "\n",
    "# Setup training\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(vit_model.parameters(), lr=Config.LEARNING_RATE, weight_decay=Config.WEIGHT_DECAY)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
    "\n",
    "# Train\n",
    "history = train_model(\n",
    "    vit_model, train_loader, val_loader, criterion, optimizer,\n",
    "    scheduler=scheduler, num_epochs=Config.NUM_EPOCHS,\n",
    "    patience=Config.EARLY_STOPPING_PATIENCE,\n",
    "    save_path=Config.OUTPUT_DIR / 'vit_best.pth',\n",
    "    model_name='ViT'\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "test_loss, metrics, targets, outputs = evaluate_model(vit_model, test_loader, criterion)\n",
    "\n",
    "# Visualize\n",
    "plot_training_history(history, 'ViT', Config.ARTIFACTS_DIR / 'vit_training.png')\n",
    "plot_roc_curves(targets, outputs, Config.LABELS, 'ViT ROC Curves', Config.ARTIFACTS_DIR / 'vit_roc.png')\n",
    "\n",
    "print(\"Training example code ready. Uncomment to run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3bf585bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exporting artifacts to: d:\\MSE\\10.Deep Learning\\Group_Final\\ViT-Chest-Xray\\Project\\artifacts\n",
      "Config saved to: d:\\MSE\\10.Deep Learning\\Group_Final\\ViT-Chest-Xray\\Project\\artifacts\\config.json\n",
      "Artifacts exported successfully!\n",
      "\n",
      "============================================================\n",
      "NOTEBOOK EXECUTION COMPLETE\n",
      "============================================================\n",
      "Configuration saved to: d:\\MSE\\10.Deep Learning\\Group_Final\\ViT-Chest-Xray\\Project\\artifacts\\config.json\n",
      "Model checkpoints in: d:\\MSE\\10.Deep Learning\\Group_Final\\ViT-Chest-Xray\\Project\\files\n",
      "Visualizations in: d:\\MSE\\10.Deep Learning\\Group_Final\\ViT-Chest-Xray\\Project\\artifacts\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXPORT CONFIGURATION AND SUMMARY\n",
    "# =============================================================================\n",
    "export_artifacts()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NOTEBOOK EXECUTION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Configuration saved to: {Config.ARTIFACTS_DIR / 'config.json'}\")\n",
    "print(f\"Model checkpoints in: {Config.OUTPUT_DIR}\")\n",
    "print(f\"Visualizations in: {Config.ARTIFACTS_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv313 (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
