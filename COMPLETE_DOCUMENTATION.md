# ğŸ“ TÃ€I LIá»†U HOÃ€N CHá»ˆNH: ViT-Chest-Xray Deep Learning Project

## TÃ¡c giáº£: AI Expert Analysis System
## Dá»± Ã¡n: NIH Chest X-ray14 Classification using Vision Transformer
## Cáº­p nháº­t: January 2025

---

# ğŸ“‘ Má»¤C Lá»¤C Tá»”NG Há»¢P

## PHáº¦N A: Tá»”NG QUAN Dá»° ÃN
- [A1. Dataset NIH Chest X-ray14](#a1-dataset-nih-chest-x-ray14)
- [A2. Kiáº¿n trÃºc Models](#a2-kiáº¿n-trÃºc-models)
- [A3. So sÃ¡nh Performance](#a3-so-sÃ¡nh-performance)
- [A4. Bugs & Recommendations](#a4-bugs--recommendations)

## PHáº¦N B: LÃ THUYáº¾T DEEP LEARNING
- [B1. Neural Network CÆ¡ báº£n](#b1-neural-network-cÆ¡-báº£n)
- [B2. Convolution Chi tiáº¿t](#b2-convolution-chi-tiáº¿t)
- [B3. Pooling Layers](#b3-pooling-layers)
- [B4. Activation Functions](#b4-activation-functions)
- [B5. Loss Functions](#b5-loss-functions)
- [B6. Optimizer Chi tiáº¿t](#b6-optimizer-chi-tiáº¿t)
- [B7. Transformer & Attention](#b7-transformer--attention)
- [B8. Transfer Learning](#b8-transfer-learning)
- [B9. Evaluation Metrics](#b9-evaluation-metrics)

## PHáº¦N C: GIáº¢I THÃCH CODE
- [C1. Data Pipeline](#c1-data-pipeline)
- [C2. CNN Model](#c2-cnn-model)
- [C3. ResNet Model](#c3-resnet-model)
- [C4. ViT Models](#c4-vit-models)

---

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PHáº¦N A: Tá»”NG QUAN Dá»° ÃN
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# A1. Dataset NIH Chest X-ray14

## ThÃ´ng tin cÆ¡ báº£n

| Thuá»™c tÃ­nh | GiÃ¡ trá»‹ |
|------------|---------|
| **TÃªn** | ChestX-ray14 (NIH Clinical Center) |
| **NÄƒm phÃ¡t hÃ nh** | 2017 |
| **Tá»•ng sá»‘ áº£nh** | 112,120 |
| **Sá»‘ bá»‡nh nhÃ¢n** | 30,805 |
| **Sá»‘ bá»‡nh lÃ½** | 14 + "No Finding" = 15 classes |
| **Dung lÆ°á»£ng** | ~42 GB |
| **Format** | PNG (grayscale â†’ RGB) |
| **Resolution gá»‘c** | ~2000Ã—2000 pixels |

## 14 Bá»‡nh lÃ½ (Pathologies)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  14 PATHOLOGICAL CONDITIONS                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  CARDIAC:                    PULMONARY:                        â”‚
â”‚  â””â”€â”€ Cardiomegaly (Tim to)   â”œâ”€â”€ Emphysema (KhÃ­ pháº¿ thÅ©ng)    â”‚
â”‚                              â”œâ”€â”€ Pneumothorax (TrÃ n khÃ­)       â”‚
â”‚  PLEURAL:                    â”œâ”€â”€ Pneumonia (ViÃªm phá»•i)         â”‚
â”‚  â”œâ”€â”€ Effusion (TrÃ n dá»‹ch)    â”œâ”€â”€ Consolidation (ÄÃ´ng Ä‘áº·c)     â”‚
â”‚  â””â”€â”€ Pleural Thickening      â”œâ”€â”€ Infiltration (ThÃ¢m nhiá»…m)    â”‚
â”‚                              â””â”€â”€ Atelectasis (Xáº¹p phá»•i)        â”‚
â”‚  MASSES:                                                        â”‚
â”‚  â”œâ”€â”€ Mass (Khá»‘i u lá»›n)       OTHERS:                           â”‚
â”‚  â””â”€â”€ Nodule (Ná»‘t nhá»)        â”œâ”€â”€ Fibrosis (XÆ¡ hÃ³a)            â”‚
â”‚                              â”œâ”€â”€ Edema (PhÃ¹ phá»•i)              â”‚
â”‚  DIAPHRAGM:                  â””â”€â”€ No Finding (BÃ¬nh thÆ°á»ng)      â”‚
â”‚  â””â”€â”€ Hernia (ThoÃ¡t vá»‹)                                         â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Äáº·c Ä‘iá»ƒm Multi-label

```python
# Má»™t áº£nh cÃ³ thá»ƒ cÃ³ nhiá»u bá»‡nh cÃ¹ng lÃºc
"Cardiomegaly|Emphysema"     # 2 bá»‡nh
"Hernia|Infiltration"        # 2 bá»‡nh

# PhÃ¢n bá»‘: 1 label (~75%), 2 labels (~20%), 3+ labels (~5%)
```

## Data Quality Issues

| Váº¥n Ä‘á» | MÃ´ táº£ | Giáº£i phÃ¡p |
|--------|-------|-----------|
| Label Noise | NLP extracted, ~90% accuracy | Focal Loss, Label smoothing |
| Class Imbalance | Ratio 300:1 (No Finding vs Hernia) | Weighted sampling, Focal Loss |
| Patient Overlap | Same patient in train/test | Split by Patient ID |
| View Position Bias | PA vs AP quality differs | Stratified sampling |

---

# A2. Kiáº¿n trÃºc Models

## So sÃ¡nh tá»•ng quan

| Model | Params | Framework | Pretrained | Expected AUC |
|-------|--------|-----------|------------|--------------|
| CNN | 95.6M | TensorFlow | âŒ | 0.55-0.65 |
| ResNet-34 | 21.3M | TensorFlow | âŒ | 0.70-0.78 |
| ViT-v1 | ~3M | TensorFlow | âŒ | 0.60-0.68 |
| ViT-v2 | ~3M | TensorFlow | âŒ | 0.68-0.75 |
| **ViT-Pretrained** | **86M** | **PyTorch** | **âœ…** | **0.82-0.88** |

## CNN Architecture

```
Input (224, 224, 3)
       â”‚
Conv2D(32, 3Ã—3, relu)     â†’  (222, 222, 32)    params: 896
MaxPool2D(2Ã—2)            â†’  (111, 111, 32)
Conv2D(64, 3Ã—3, relu)     â†’  (109, 109, 64)    params: 18,496
MaxPool2D(2Ã—2)            â†’  (54, 54, 64)
Flatten()                 â†’  (186,624)
Dense(512, relu)          â†’  (512)              params: 95,552,000 â† 99%!
Dense(15, sigmoid)        â†’  (15)

TOTAL: ~95.6M parameters
ğŸ”´ ISSUE: 99% params in Dense layer â†’ severe overfitting
```

## ResNet-34 Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEM: Conv 7Ã—7, 64, stride 2 â†’ BN â†’ ReLU â†’ MaxPool            â”‚
â”‚  STAGE 1: 3 Ã— ResBlock(64)   â†’ (56, 56, 64)                    â”‚
â”‚  STAGE 2: 4 Ã— ResBlock(128)  â†’ (28, 28, 128)                   â”‚
â”‚  STAGE 3: 6 Ã— ResBlock(256)  â†’ (14, 14, 256)                   â”‚
â”‚  STAGE 4: 3 Ã— ResBlock(512)  â†’ (7, 7, 512)                     â”‚
â”‚  GlobalAveragePooling â†’ Dense(15, sigmoid)                     â”‚
â”‚  TOTAL: ~21.3M parameters                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Skip Connection: y = F(x) + x
â†’ Gradient always has identity term â†’ no vanishing!
```

## Vision Transformer Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. PATCH EMBEDDING:                                           â”‚
â”‚     - Split into patches (16Ã—16 or 32Ã—32)                      â”‚
â”‚     - Linear projection to embedding dim                       â”‚
â”‚     - Add position embeddings                                   â”‚
â”‚                                                                 â”‚
â”‚  2. TRANSFORMER ENCODER Ã— N:                                   â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚     â”‚  LayerNorm â†’ Multi-Head Attention â†’ + Skip               â”‚
â”‚     â”‚  LayerNorm â†’ MLP (FFN) â†’ + Skip                          â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                                                                 â”‚
â”‚  3. CLASSIFICATION: [CLS] token â†’ MLP Head â†’ 15 classes        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ViT-v1: 49 patches (32Ã—32), 8 layers, 4 heads, ~3M params
ViT-Pretrained: 196 patches (16Ã—16), 12 layers, 12 heads, ~86M params
```

---

# A3. So sÃ¡nh Performance

## Architecture Comparison

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RECEPTIVE FIELD:                                               â”‚
â”‚  â”œâ”€â”€ CNN:    Local (3Ã—3) â†’ grows slowly with depth             â”‚
â”‚  â”œâ”€â”€ ResNet: Local â†’ larger due to depth                       â”‚
â”‚  â””â”€â”€ ViT:    GLOBAL from layer 1! (attention to all patches)   â”‚
â”‚                                                                 â”‚
â”‚  DATA EFFICIENCY:                                               â”‚
â”‚  â”œâ”€â”€ CNN/ResNet: High (strong inductive bias)                  â”‚
â”‚  â””â”€â”€ ViT:        Low (needs lots of data or pretraining)       â”‚
â”‚                                                                 â”‚
â”‚  SCALABILITY:                                                   â”‚
â”‚  â”œâ”€â”€ CNN:    Limited (stacking convs)                          â”‚
â”‚  â””â”€â”€ ViT:    Excellent (just add more layers/heads)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Expected Results

| Model | Train Acc | Val Acc | AUC | Notes |
|-------|-----------|---------|-----|-------|
| CNN | ~95% | ~70% | 0.55-0.65 | Severe overfitting |
| ResNet | ~88% | ~78% | 0.70-0.78 | Good balance |
| ViT-v1 | ~92% | ~72% | 0.60-0.68 | Too small |
| ViT-v2 | ~85% | ~78% | 0.68-0.75 | Better regularization |
| **ViT-Pretrained** | **~90%** | **~86%** | **0.82-0.88** | **BEST** |

---

# A4. Bugs & Recommendations

## Critical Bugs

| File | Bug | Fix |
|------|-----|-----|
| `data.ipynb` | `idxs = random` (wrong) | `idxs = random.sample(idxs, num_images)` |
| `data_download.ipynb` | Wrong dest_path | `os.path.join(sub_dir, file)` |
| `ViT-v1.ipynb` | `ops` not defined | Use `tf.expand_dims` |
| `ViT-v2.ipynb` | `l2` not imported | `from keras.regularizers import l2` |
| `ViT-v2.ipynb` | `restore_best_weights=False` | Set to `True` |

## Recommendations

### Data Pipeline
```python
# Fix: Split by Patient ID to avoid data leakage
patient_ids = df['Patient ID'].unique()
train_patients, test_patients = train_test_split(patient_ids, test_size=0.2)
train_df = df[df['Patient ID'].isin(train_patients)]
test_df = df[df['Patient ID'].isin(test_patients)]
```

### Model Selection
```python
# Recommended: Use pretrained ViT
import timm
model = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=15)
```

### Loss Function for Imbalance
```python
# Focal Loss
class FocalLoss(nn.Module):
    def __init__(self, alpha=0.25, gamma=2):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
    
    def forward(self, inputs, targets):
        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
        pt = torch.exp(-BCE_loss)
        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss
        return F_loss.mean()
```

---

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PHáº¦N B: LÃ THUYáº¾T DEEP LEARNING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# B1. Neural Network CÆ¡ báº£n

## Neuron Sinh há»c vs NhÃ¢n táº¡o

```
NEURON SINH Há»ŒC:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Dendrites (nháº­n tÃ­n hiá»‡u) â†’ Cell Body (tá»•ng há»£p) â†’            â”‚
â”‚  Axon (truyá»n) â†’ Synapses (káº¿t ná»‘i neuron khÃ¡c)                â”‚
â”‚                                                                 â”‚
â”‚  Náº¿u tá»•ng tÃ­n hiá»‡u > ngÆ°á»¡ng â†’ phÃ¡t xung Ä‘iá»‡n                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

NEURON NHÃ‚N Táº O:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  xâ‚ â”€â”€wâ‚â”€â”€â”                                                     â”‚
â”‚  xâ‚‚ â”€â”€wâ‚‚â”€â”€â”¼â”€â”€â†’ Î£(wáµ¢xáµ¢) + b â”€â”€â†’ f(z) â”€â”€â†’ output               â”‚
â”‚  xâ‚ƒ â”€â”€wâ‚ƒâ”€â”€â”˜                                                     â”‚
â”‚                                                                 â”‚
â”‚  z = wâ‚xâ‚ + wâ‚‚xâ‚‚ + wâ‚ƒxâ‚ƒ + b                                    â”‚
â”‚  a = f(z)  â† activation function                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Weights vÃ  Bias - VÃ­ dá»¥ Spam Classification

```
INPUT FEATURES (email):
xâ‚ = CÃ³ tá»« "FREE"?        (1 = cÃ³, 0 = khÃ´ng)
xâ‚‚ = CÃ³ tá»« "WINNER"?
xâ‚ƒ = CÃ³ tÃªn ngÆ°á»i nháº­n?

SAU KHI TRAIN:
wâ‚ = +2.5   â† "FREE" â†’ spam
wâ‚‚ = +1.8   â† "WINNER" â†’ spam
wâ‚ƒ = -1.5   â† CÃ³ tÃªn â†’ NOT spam

EMAIL: "FREE WINNER offer" (khÃ´ng cÃ³ tÃªn)
x = [1, 1, 0]
z = 2.5Ã—1 + 1.8Ã—1 + (-1.5)Ã—0 + 0.5 = 4.8
Å· = sigmoid(4.8) = 0.992 â†’ 99.2% SPAM!
```

## Weight Initialization - Xavier

### Váº¥n Ä‘á»

```
âŒ All zeros: Symmetry problem - all neurons learn same thing
âŒ Too large: Exploding activations â†’ vanishing gradients
âŒ Too small: Vanishing activations â†’ no learning
```

### Chá»©ng minh Xavier Initialization

```
GIáº¢ THIáº¾T:
- Input x ~ N(0, 1)
- Weights w ~ N(0, ÏƒÂ²)

Má»¤C TIÃŠU: Var[z] = Var[x] = 1

TÃNH TOÃN:
z = Î£áµ¢ wáµ¢xáµ¢ vá»›i n inputs

Var[z] = Î£áµ¢ Var[wáµ¢xáµ¢]
       = n Ã— E[wáµ¢Â²] Ã— E[xáµ¢Â²]
       = n Ã— ÏƒÂ² Ã— 1
       = n Ã— ÏƒÂ²

Äá»‚ Var[z] = 1:
n Ã— ÏƒÂ² = 1
Ïƒ = 1/âˆšn  â† XAVIER INITIALIZATION!

CODE:
W = np.random.randn(n_in, n_out) * np.sqrt(1.0 / n_in)
```

### He Initialization (cho ReLU)

```
ReLU "giáº¿t" 50% activations (z < 0 â†’ 0)
â†’ Variance giáº£m 50%
â†’ Cáº§n Ïƒ = âˆš(2/n) Ä‘á»ƒ compensate

CODE:
W = np.random.randn(n_in, n_out) * np.sqrt(2.0 / n_in)
```

---

# B2. Convolution Chi tiáº¿t

## VÃ­ dá»¥ tÃ­nh tay: 6Ã—6 input, 3Ã—3 kernel

```
INPUT (6Ã—6):                    KERNEL (3Ã—3) - Edge Detector:
â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”
â”‚  1 â”‚  2 â”‚  3 â”‚  0 â”‚  1 â”‚  2 â”‚ â”‚ -1 â”‚ -1 â”‚ -1 â”‚
â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤
â”‚  0 â”‚  1 â”‚  2 â”‚  3 â”‚  0 â”‚  1 â”‚ â”‚ -1 â”‚  8 â”‚ -1 â”‚
â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤
â”‚  1 â”‚  2 â”‚  1 â”‚  0 â”‚  1 â”‚  2 â”‚ â”‚ -1 â”‚ -1 â”‚ -1 â”‚
â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤ â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜
â”‚  2 â”‚  1 â”‚  0 â”‚  1 â”‚  2 â”‚  0 â”‚
â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤
â”‚  1 â”‚  0 â”‚  2 â”‚  1 â”‚  1 â”‚  3 â”‚
â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤
â”‚  0 â”‚  1 â”‚  1 â”‚  2 â”‚  0 â”‚  1 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜
```

### Step 1: Position (0,0)

```
VÃ¹ng 3Ã—3 táº¡i (0,0):    Ã— Kernel:         = Element-wise:
â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
â”‚  1 â”‚  2 â”‚  3 â”‚   Ã—   â”‚ -1 â”‚ -1 â”‚ -1 â”‚ = â”‚  -1 â”‚  -2 â”‚  -3 â”‚
â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤       â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤   â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚  0 â”‚  1 â”‚  2 â”‚   Ã—   â”‚ -1 â”‚  8 â”‚ -1 â”‚ = â”‚   0 â”‚   8 â”‚  -2 â”‚
â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤       â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤   â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚  1 â”‚  2 â”‚  1 â”‚   Ã—   â”‚ -1 â”‚ -1 â”‚ -1 â”‚ = â”‚  -1 â”‚  -2 â”‚  -1 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜

SUM = -1 -2 -3 + 0 +8 -2 -1 -2 -1 = -4

Output[0,0] = -4
```

### Output Size Formula

$$\text{Output} = \frac{N - K + 2P}{S} + 1$$

- N = Input size, K = Kernel size, P = Padding, S = Stride

```
VÃ­ dá»¥: 6Ã—6 input, 3Ã—3 kernel, no padding, stride 1
Output = (6 - 3 + 0)/1 + 1 = 4Ã—4
```

## Padding vÃ  Stride

```
PADDING='same' (giá»¯ kÃ­ch thÆ°á»›c):
Input 6Ã—6 + pad 1 = 8Ã—8
Conv 3Ã—3 â†’ Output 6Ã—6 âœ“

STRIDE=2 (giáº£m kÃ­ch thÆ°á»›c):
Kernel nháº£y 2 pixel má»—i bÆ°á»›c
â†’ Output size giáº£m ~50%
```

## Multiple Channels vÃ  Filters

```
INPUT RGB (6Ã—6Ã—3):
- 3 channels (R, G, B)

KERNEL (3Ã—3Ã—3):
- Má»™t slice cho má»—i channel
- Total params: 3Ã—3Ã—3 + 1 = 28

32 FILTERS:
- 32 kernels khÃ¡c nhau
- Output: 4Ã—4Ã—32 feature maps
- Total params: 32 Ã— 28 = 896
```

## Receptive Field

```
CÃ”NG THá»¨C:
RF_n = RF_{n-1} + (K_n - 1) Ã— Î  S_i

VÃ Dá»¤: 3 Conv layers 3Ã—3, stride 1:
RFâ‚ = 3
RFâ‚‚ = 3 + (3-1) Ã— 1 = 5
RFâ‚ƒ = 5 + (3-1) Ã— 1 = 7

ViT: Má»—i patch "nhÃ¬n" 16Ã—16 hoáº·c 32Ã—32 pixels ngay tá»« Ä‘áº§u!
+ Attention â†’ Global RF ngay láº­p tá»©c!
```

---

# B3. Pooling Layers

## Max Pooling Step-by-Step

```
INPUT (4Ã—4):                OUTPUT (2Ã—2) vá»›i MaxPool 2Ã—2, stride 2:
â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”
â”‚  1 â”‚  3 â”‚  2 â”‚  1 â”‚       â”‚  9 â”‚  4 â”‚
â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤  â†’    â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤
â”‚  2 â”‚  9 â”‚  1 â”‚  4 â”‚       â”‚  4 â”‚  8 â”‚
â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤       â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜
â”‚  3 â”‚  2 â”‚  8 â”‚  3 â”‚
â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤       VÃ¹ng [1,3,2,9] â†’ max = 9
â”‚  1 â”‚  4 â”‚  2 â”‚  6 â”‚       VÃ¹ng [2,1,1,4] â†’ max = 4
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜       VÃ¹ng [3,2,1,4] â†’ max = 4
                            VÃ¹ng [8,3,2,6] â†’ max = 8
```

### Táº¡i sao Max Pooling?

1. **Translation Invariance**: Feature dá»‹ch trong vÃ¹ng 2Ã—2, max váº«n giá»¯
2. **Dimensionality Reduction**: 4Ã—4 â†’ 2Ã—2 (giáº£m 4 láº§n)
3. **Giá»¯ feature máº¡nh nháº¥t**: Loáº¡i bá» noise

## Global Average Pooling vs Flatten

```
FLATTEN:
Feature map 7Ã—7Ã—512 â†’ Flatten â†’ 25,088 neurons
Dense(256) â†’ 25,088 Ã— 256 = 6,422,528 params! ğŸ˜±

GLOBAL AVERAGE POOLING:
Feature map 7Ã—7Ã—512 â†’ GAP â†’ 512 neurons
Dense(256) â†’ 512 Ã— 256 = 131,072 params âœ“

â†’ Giáº£m 50 láº§n sá»‘ parameters!
```

---

# B4. Activation Functions

## Sigmoid

```
Ïƒ(z) = 1/(1 + e^(-z))

RANGE: (0, 1) - Tá»‘t cho probability output

Äáº O HÃ€M:
Ïƒ'(z) = Ïƒ(z) Ã— (1 - Ïƒ(z))

CHá»¨NG MINH:
Ïƒ(z) = (1 + e^(-z))^(-1)
Ïƒ'(z) = -1 Ã— (1 + e^(-z))^(-2) Ã— (-e^(-z))
      = e^(-z) / (1 + e^(-z))Â²
      = [1/(1+e^(-z))] Ã— [e^(-z)/(1+e^(-z))]
      = Ïƒ(z) Ã— (1 - Ïƒ(z)) âœ“
```

### Vanishing Gradient Problem

```
Ïƒ'(z) = Ïƒ(z) Ã— (1 - Ïƒ(z))

Maximum táº¡i z=0: Ïƒ'(0) = 0.5 Ã— 0.5 = 0.25

QUA 10 LAYERS:
âˆ‚L/âˆ‚wâ‚ = âˆ‚L/âˆ‚aâ‚â‚€ Ã— Ïƒ'(zâ‚â‚€) Ã— ... Ã— Ïƒ'(zâ‚)
       â‰¤ C Ã— 0.25Â¹â°
       = C Ã— 0.000001

â†’ Gradient gáº§n nhÆ° = 0!
â†’ Layers Ä‘áº§u khÃ´ng há»c Ä‘Æ°á»£c!
```

## ReLU

```
ReLU(z) = max(0, z)

Äáº O HÃ€M:
ReLU'(z) = 1 náº¿u z > 0
         = 0 náº¿u z â‰¤ 0

Æ¯U ÄIá»‚M:
- KhÃ´ng vanishing gradient (z > 0)
- TÃ­nh toÃ¡n nhanh
- Sparse activation (z < 0 â†’ 0)

NHÆ¯á»¢C ÄIá»‚M - Dead Neurons:
Náº¿u z luÃ´n < 0 â†’ gradient = 0 â†’ neuron "cháº¿t"
```

## Softmax vs Sigmoid

```
SOFTMAX (Multi-class, mutually exclusive):
softmax(záµ¢) = e^(záµ¢) / Î£â±¼ e^(zâ±¼)
â†’ Tá»•ng = 1, chá»‰ 1 class Ä‘Ãºng
â†’ Output: Dog=0.7, Cat=0.2, Bird=0.1

SIGMOID (Multi-label, independent):
Ïƒ(záµ¢) = 1/(1 + e^(-záµ¢)) cho má»—i class
â†’ Má»—i class independent
â†’ Output: Cardiomegaly=0.8, Effusion=0.9, Pneumonia=0.3
â†’ CÃ³ thá»ƒ nhiá»u bá»‡nh cÃ¹ng lÃºc!

CHEST X-RAY â†’ SIGMOID (multi-label)
```

---

# B5. Loss Functions

## Binary Cross-Entropy (BCE)

$$L = -\frac{1}{N}\sum_{i=1}^{N}[y_i \log(\hat{y}_i) + (1-y_i)\log(1-\hat{y}_i)]$$

### VÃ­ dá»¥ tÃ­nh toÃ¡n

```
Case 1: y=1 (cÃ³ bá»‡nh), Å·=0.9
L = -[1Ã—log(0.9) + 0Ã—log(0.1)] = -log(0.9) = 0.105 â† Nhá», tá»‘t!

Case 2: y=1 (cÃ³ bá»‡nh), Å·=0.1  
L = -[1Ã—log(0.1) + 0Ã—log(0.9)] = -log(0.1) = 2.303 â† Lá»›n, tá»‡!

Case 3: y=0 (khÃ´ng bá»‡nh), Å·=0.1
L = -[0Ã—log(0.1) + 1Ã—log(0.9)] = -log(0.9) = 0.105 â† Nhá», tá»‘t!
```

### BCE tá»« Maximum Likelihood

```
CHá»¨NG MINH:
P(y|x) = Å·^y Ã— (1-Å·)^(1-y)  (Bernoulli distribution)

Maximum Likelihood:
max P(Y|X) = max Î  P(yáµ¢|xáµ¢)

Log-likelihood:
log P(Y|X) = Î£ [yáµ¢ log(Å·áµ¢) + (1-yáµ¢) log(1-Å·áµ¢)]

Negative log-likelihood (minimize):
L = -Î£ [yáµ¢ log(Å·áµ¢) + (1-yáµ¢) log(1-Å·áµ¢)]

â†’ BCE = Negative Log-Likelihood of Bernoulli!
```

### Gradient cá»§a BCE + Sigmoid

```
TÃNH âˆ‚L/âˆ‚z:

L = -[y log(Ïƒ) + (1-y) log(1-Ïƒ)]

âˆ‚L/âˆ‚Ïƒ = -y/Ïƒ + (1-y)/(1-Ïƒ)

âˆ‚Ïƒ/âˆ‚z = Ïƒ(1-Ïƒ)

âˆ‚L/âˆ‚z = âˆ‚L/âˆ‚Ïƒ Ã— âˆ‚Ïƒ/âˆ‚z
      = [-y/Ïƒ + (1-y)/(1-Ïƒ)] Ã— Ïƒ(1-Ïƒ)
      = -y(1-Ïƒ) + (1-y)Ïƒ
      = -y + yÏƒ + Ïƒ - yÏƒ
      = Ïƒ - y
      = Å· - y â† Äáº¸P!

â†’ Gradient Ä‘Æ¡n giáº£n: prediction - target
```

## Focal Loss (cho Class Imbalance)

```
BCE: L = -log(pâ‚œ)
Focal: L = -Î±â‚œ(1-pâ‚œ)^Î³ log(pâ‚œ)

Î³ = 2 (focusing parameter)
Î± = 0.25 (class weight)

VÃ Dá»¤:
Easy example: pâ‚œ = 0.9
(1 - 0.9)Â² = 0.01 â†’ weight giáº£m 100 láº§n!

Hard example: pâ‚œ = 0.1
(1 - 0.1)Â² = 0.81 â†’ weight gáº§n nhÆ° giá»¯ nguyÃªn

â†’ Focus vÃ o hard examples!
â†’ Down-weight easy examples!
```

---

# B6. Optimizer Chi tiáº¿t

## SGD vÃ  váº¥n Ä‘á» Oscillation

```
w_{t+1} = w_t - Î±âˆ‡L(w_t)

Váº¤N Äá»€: Narrow Valleys
- Loss dá»‘c theo wâ‚‚ (gradient lá»›n) â†’ nháº£y lá»›n â†’ vÆ°á»£t quÃ¡ â†’ oscillate!
- Loss thoáº£i theo wâ‚ (gradient nhá») â†’ nháº£y nhá» â†’ tiáº¿n cháº­m
```

## Momentum - "Quáº£ bÃ³ng lÄƒn"

```
v_{t+1} = Î²v_t + âˆ‡L(w_t)
w_{t+1} = w_t - Î±v_{t+1}

INTUITION:
Quáº£ bÃ³ng lÄƒn xuá»‘ng Ä‘á»“i:
1. TÃ­ch lÅ©y váº­n tá»‘c (momentum)
2. QuÃ¡n tÃ­nh giÃºp vÆ°á»£t qua local minimum
3. Oscillation bá»‹ triá»‡t tiÃªu (gradient Ä‘á»•i dáº¥u â†’ v trung bÃ¬nh â‰ˆ 0)

VÃ Dá»¤ (Î²=0.9, Î±=0.1):
Step 1: gradient = [2, 1]
v = 0.9Ã—[0,0] + [2,1] = [2, 1]

Step 2: gradient = [2, -1]  â† y oscillate!
v = 0.9Ã—[2,1] + [2,-1] = [3.8, -0.1]

HÆ°á»›ng x (consistent): tÃ­ch lÅ©y!
HÆ°á»›ng y (oscillate): triá»‡t tiÃªu!
```

## Adam - Adaptive Moment Estimation

```python
# ADAM = MOMENTUM + RMSPROP + BIAS CORRECTION

m = Î²â‚Ã—m + (1-Î²â‚)Ã—g      # First moment (mean)
v = Î²â‚‚Ã—v + (1-Î²â‚‚)Ã—gÂ²     # Second moment (variance)
mÌ‚ = m / (1 - Î²â‚^t)       # Bias correction
vÌ‚ = v / (1 - Î²â‚‚^t)       # Bias correction
w = w - Î± Ã— mÌ‚ / (âˆšvÌ‚ + Îµ)

Default: Î²â‚=0.9, Î²â‚‚=0.999, Î±=0.001

Táº I SAO HIá»†U QUáº¢:
1. Adaptive LR cho tá»«ng parameter
2. Momentum giÃºp vÆ°á»£t local minima
3. Works out of the box
```

### Bias Correction

```
Váº¤N Äá»€:
Ban Ä‘áº§u m = 0
mâ‚ = Î²â‚Ã—0 + (1-Î²â‚)Ã—gâ‚ = 0.1Ã—gâ‚ â† BIASED!

GIáº¢I PHÃP:
E[m_t] = (1 - Î²â‚^t) Ã— E[g]
mÌ‚_t = m_t / (1 - Î²â‚^t) â† UNBIASED!

t=1: correction = 1/0.1 = 10Ã— (lá»›n)
t=100: correction â‰ˆ 1Ã— (nhá»)
```

---

# B7. Transformer & Attention

## Self-Attention - TÃ­nh toÃ¡n vá»›i sá»‘

```
INPUT: 4 patches, embedding dim = 3
X = [[1,0,1], [0,1,0], [1,1,0], [0,0,1]]

WEIGHT MATRICES (learned):
W_Q, W_K, W_V = 3Ã—3 matrices

STEP 1: TÃ­nh Q, K, V
Q = X @ W_Q  # (4, 3)
K = X @ W_K  # (4, 3)
V = X @ W_V  # (4, 3)

STEP 2: Attention scores = Q @ K^T
scores[i][j] = "patch i attend Ä‘áº¿n patch j bao nhiÃªu?"

STEP 3: Scale bá»Ÿi âˆšd_k
scaled_scores = scores / âˆš3

STEP 4: Softmax (theo hÃ ng)
attention_weights = softmax(scaled_scores)
â†’ Má»—i hÃ ng sum = 1

STEP 5: Weighted sum
output = attention_weights @ V

â†’ Má»—i patch output = weighted combination cá»§a táº¥t cáº£ patches!
```

## CÃ´ng thá»©c Self-Attention

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

### Táº¡i sao chia âˆšd_k?

```
Khi d_k lá»›n, dot product QÂ·K cÃ³ thá»ƒ ráº¥t lá»›n
â†’ softmax saturate (output gáº§n 0 hoáº·c 1)
â†’ Gradient gáº§n 0

Chia âˆšd_k giá»¯ variance á»•n Ä‘á»‹nh:
Var(QÂ·K) â‰ˆ d_k
Var(QÂ·K / âˆšd_k) â‰ˆ 1 âœ“
```

## Multi-Head Attention

```
1 HEAD = 1 loáº¡i relationship

4 HEADS cÃ³ thá»ƒ há»c 4 loáº¡i khÃ¡c nhau:
- Head 1: "Texture similarity"
- Head 2: "Spatial proximity"
- Head 3: "Contrast detection"
- Head 4: "Abnormality clustering"

COMPUTATION:
head_i = Attention(XW_Qi, XW_Ki, XW_Vi)
MultiHead = Concat(head_1, ..., head_h) @ W_O
```

## Position Embedding

```
Váº¤N Äá»€: Attention lÃ  permutation invariant!
Input [A, B, C] vÃ  [C, A, B] cho cÃ¹ng output náº¿u khÃ´ng cÃ³ position info.

GIáº¢I PHÃP: Learnable position embeddings
pos_embed = Parameter(shape=(num_patches, embed_dim))
output = patch_embed + pos_embed

â†’ Network Tá»° Há»ŒC Ä‘Æ°á»£c 2D spatial structure!
```

## Layer Norm vs Batch Norm

```
BATCH NORM: Normalize across BATCH (vertical)
- Cáº§n batch statistics
- KhÃ¡c training/inference
- KhÃ´ng tá»‘t cho sequence

LAYER NORM: Normalize across FEATURES (horizontal)  
- Independent of batch size
- Same training/inference
- Tá»‘t cho Transformers!
```

---

# B8. Transfer Learning

## Feature Extraction vs Fine-tuning

```
FEATURE EXTRACTION (Freeze all):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pretrained layers              â”‚ â† FROZEN
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  New classification head        â”‚ â† TRAINABLE
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
âœ“ Fast, works with small data
âœ— Cannot adapt features

FINE-TUNING (Unfreeze some/all):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Early layers (generic)         â”‚ â† Small LR
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Later layers (task-specific)   â”‚ â† Medium LR
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Classification head            â”‚ â† Large LR
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
âœ“ Better performance
âœ— Needs careful tuning
```

## Gradual Unfreezing Strategy

```
Epoch 1-3: Only train head
Epoch 4-6: Unfreeze top 2 layers
Epoch 7+:  Unfreeze all with small LR

â†’ Avoid destroying pretrained features!
```

## Domain Adaptation: ImageNet â†’ X-ray

```
SOURCE (ImageNet):           TARGET (X-ray):
- Natural images             - Medical images
- RGB color                  - Grayscale
- Objects centered           - Subtle differences
- Large viewpoint variation  - Fixed viewpoint

BRIDGING THE GAP:
Early layers: edges, textures âœ“ (universal)
Middle layers: need adapt
Top layers: must relearn

STRATEGIES:
1. Grayscale â†’ RGB: Copy channel 3 times
2. Use ImageNet normalization
3. Domain-specific augmentation
4. Progressive training
```

---

# B9. Evaluation Metrics

## Confusion Matrix

```
                 Predicted
              Positive  Negative
Actual    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
Positive  â”‚   TP    â”‚   FN    â”‚
          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
Negative  â”‚   FP    â”‚   TN    â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TP: CÃ³ bá»‡nh, dá»± Ä‘oÃ¡n cÃ³ bá»‡nh âœ“
TN: KhÃ´ng bá»‡nh, dá»± Ä‘oÃ¡n khÃ´ng bá»‡nh âœ“
FP: KhÃ´ng bá»‡nh, dá»± Ä‘oÃ¡n cÃ³ bá»‡nh âœ— (False Alarm)
FN: CÃ³ bá»‡nh, dá»± Ä‘oÃ¡n khÃ´ng bá»‡nh âœ— (Missed!)
```

## Metrics tá»« Confusion Matrix

```
ACCURACY = (TP + TN) / (TP + TN + FP + FN)
âš ï¸ Misleading vá»›i imbalanced data!

PRECISION = TP / (TP + FP)
"Trong dá»± Ä‘oÃ¡n positive, bao nhiÃªu % Ä‘Ãºng?"
â†’ Quan trá»ng khi FP costly

RECALL = TP / (TP + FN)
"Trong actual positive, bao nhiÃªu % Ä‘Æ°á»£c detect?"
â†’ Quan trá»ng khi FN costly (MEDICAL!)

F1-SCORE = 2 Ã— Precision Ã— Recall / (Precision + Recall)
â†’ Harmonic mean, balance cáº£ hai

SPECIFICITY = TN / (TN + FP)
"Trong actual negative, bao nhiÃªu % Ä‘Ãºng?"
```

## ROC vÃ  AUC

```
ROC Curve:
- X-axis: FPR = FP / (FP + TN) = 1 - Specificity
- Y-axis: TPR = TP / (TP + FN) = Recall

AUC (Area Under Curve):
- 0.5 = Random guess
- 0.7 = Acceptable
- 0.8 = Good
- 0.9 = Excellent
- 1.0 = Perfect

INTERPRETATION:
AUC = P(random positive ranked higher than random negative)
```

## Multi-label Metrics

```
HAMMING LOSS:
= % labels predicted wrong
True:  [1, 0, 1, 1, 0]
Pred:  [1, 1, 1, 0, 0]
        âœ“  âœ—  âœ“  âœ—  âœ“  â†’ 2/5 = 0.4

MACRO AVERAGING:
TÃ­nh metric cho Má»–I class, rá»“i average
â†’ Treats all classes equally
â†’ Good for rare classes

MICRO AVERAGING:
Pool ALL TP, FP, FN rá»“i tÃ­nh
â†’ Dominated by frequent classes
â†’ Good for overall performance
```

## Medical Context: Recall vs Precision

```
SCREENING (detect disease):
FN = Miss bá»‡nh â†’ NGUY HIá»‚M!
â†’ Optimize RECALL
â†’ Lower threshold (0.3)

SURGERY DECISION:
FP = Unnecessary surgery â†’ NGUY HIá»‚M!
â†’ Optimize PRECISION
â†’ Higher threshold (0.7)

CHEST X-RAY PROJECT:
- Screening â†’ High Recall
- Diagnosis â†’ Balance (F1-score)
- Comparison â†’ AUC-ROC
```

---

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PHáº¦N C: GIáº¢I THÃCH CODE CHI TIáº¾T
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# C1. Data Pipeline - Giáº£i thÃ­ch chuyÃªn sÃ¢u

## C1.1. DatasetParser Class - PhÃ¢n tÃ­ch tá»«ng dÃ²ng

```python
class DatasetParser():
    """
    Class quáº£n lÃ½ vÃ  xá»­ lÃ½ dataset NIH Chest X-ray14.
    Chá»©c nÄƒng chÃ­nh:
    1. Load vÃ  index táº¥t cáº£ áº£nh PNG
    2. Parse labels tá»« CSV
    3. Chuyá»ƒn Ä‘á»•i multi-label sang one-hot encoding
    4. Há»— trá»£ weighted sampling cho class imbalance
    """
    
    def __init__(self, root_dir, images_dir, labels_csv):
        """
        PARAMETERS:
        - root_dir: ThÆ° má»¥c gá»‘c chá»©a data (vd: '/path/to/archive/sample')
        - images_dir: ThÆ° má»¥c con chá»©a áº£nh (vd: 'sample/images')
        - labels_csv: File CSV chá»©a labels (vd: 'sample_labels.csv')
        """
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # BÆ¯á»šC 1: Load táº¥t cáº£ Ä‘Æ°á»ng dáº«n áº£nh PNG
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        self.image_paths = sorted(glob.glob(os.path.join(root_dir, images_dir, "*.png")))
        # 
        # GIáº¢I THÃCH:
        # - glob.glob(): TÃ¬m táº¥t cáº£ files match pattern "*.png"
        # - os.path.join(): Ná»‘i path an toÃ n (tá»± xá»­ lÃ½ / hay \)
        # - sorted(): Sáº¯p xáº¿p Ä‘á»ƒ Ä‘áº£m báº£o reproducibility
        #
        # VÃ Dá»¤:
        # root_dir = '/data/chest-xray'
        # images_dir = 'images'
        # Pattern = '/data/chest-xray/images/*.png'
        # Káº¿t quáº£: ['00000001_000.png', '00000001_001.png', ...]
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # BÆ¯á»šC 2: Load vÃ  parse labels tá»« CSV
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        self.labels_df = self._labels_by_task(root_dir=root_dir, labels_csv=labels_csv)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # BÆ¯á»šC 3: Äá»‹nh nghÄ©a 15 class labels
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        self.labels = [
            'Cardiomegaly',       # 0: Tim to
            'Emphysema',          # 1: KhÃ­ pháº¿ thÅ©ng
            'Effusion',           # 2: TrÃ n dá»‹ch mÃ ng phá»•i
            'Hernia',             # 3: ThoÃ¡t vá»‹ cÆ¡ hoÃ nh
            'Nodule',             # 4: Ná»‘t phá»•i
            'Pneumothorax',       # 5: TrÃ n khÃ­ mÃ ng phá»•i
            'Atelectasis',        # 6: Xáº¹p phá»•i
            'Pleural_Thickening', # 7: DÃ y mÃ ng phá»•i
            'Mass',               # 8: Khá»‘i u
            'Edema',              # 9: PhÃ¹ phá»•i
            'Consolidation',      # 10: ÄÃ´ng Ä‘áº·c phá»•i
            'Infiltration',       # 11: ThÃ¢m nhiá»…m
            'Fibrosis',           # 12: XÆ¡ phá»•i
            'Pneumonia',          # 13: ViÃªm phá»•i
            'No Finding'          # 14: BÃ¬nh thÆ°á»ng
        ]
        # Thá»© tá»± QUAN TRá»ŒNG: One-hot encoding sáº½ theo thá»© tá»± nÃ y!
```

### _labels_by_task() - Parse CSV Labels

```python
    def _labels_by_task(self, root_dir=None, labels_csv=None):
        """
        Parse file CSV vÃ  táº¡o DataFrame vá»›i cá»™t ['Id', 'Label']
        Label lÃ  LIST cÃ¡c bá»‡nh (cho multi-label)
        """
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # BÆ¯á»šC 1: Load CSV gá»‘c
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        labels_df = pd.read_csv(os.path.join(root_dir, labels_csv))
        #
        # CSV FORMAT (Data_Entry_2017_v2020.csv):
        # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        # â”‚ Image Index       â”‚ Finding Labels              â”‚ Patient ID  â”‚
        # â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        # â”‚ 00000001_000.png  â”‚ Cardiomegaly|Emphysema     â”‚ 1           â”‚
        # â”‚ 00000002_000.png  â”‚ No Finding                  â”‚ 2           â”‚
        # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # BÆ¯á»šC 2: Táº¡o dictionary {filename: full_path}
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        image_path = {
            os.path.basename(x): x 
            for x in glob.glob(os.path.join(root_dir, 'images', '*.png'))
        }
        # VÃ Dá»¤: {'00000001_000.png': '/data/images/00000001_000.png', ...}
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # BÆ¯á»šC 3: Lá»c chá»‰ giá»¯ áº£nh cÃ³ trong thÆ° má»¥c
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        labels_df = labels_df[
            labels_df['Image Index'].map(os.path.basename).isin(image_path)
        ]
        # âš ï¸ QUAN TRá»ŒNG: CSV cÃ³ thá»ƒ chá»©a nhiá»u áº£nh hÆ¡n thÆ° má»¥c thá»±c táº¿
        # BÆ°á»›c nÃ y loáº¡i bá» cÃ¡c entries khÃ´ng cÃ³ áº£nh tÆ°Æ¡ng á»©ng
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # BÆ¯á»šC 4: Táº¡o DataFrame má»›i vá»›i format chuáº©n
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        new_labels_df = pd.DataFrame()
        new_labels_df['Id'] = labels_df['Image Index'].copy()
        
        # Chuyá»ƒn "Cardiomegaly|Emphysema" â†’ ['Cardiomegaly', 'Emphysema']
        new_labels_df['Label'] = labels_df['Finding Labels'].apply(
            lambda val: val.split('|')
        )
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # BÆ¯á»šC 5: Giáº£i phÃ³ng bá»™ nhá»›
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        del labels_df  # CSV gá»‘c cÃ³ thá»ƒ ráº¥t lá»›n (~112K rows)
        
        return new_labels_df
```

### get_labels_df() - One-Hot Encoding

```python
    def get_labels_df(self):
        """
        Chuyá»ƒn Ä‘á»•i Label tá»« LIST bá»‡nh sang ONE-HOT VECTOR
        
        VÃ Dá»¤:
        Input:  ['Cardiomegaly', 'Emphysema']
        Output: [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
                 â†‘  â†‘
                 Cardiomegaly (index 0)
                    Emphysema (index 1)
        """
        new_labels_df = self.labels_df.copy()
        
        for i in range(len(new_labels_df)):
            # Khá»Ÿi táº¡o vector zeros
            one_hot = [0 for element in self.labels]  # [0,0,0,...,0] (15 zeros)
            
            # Set 1 cho má»—i bá»‡nh cÃ³ trong list
            for element in new_labels_df['Label'][i]:
                one_hot[self.labels.index(element)] = 1
            
            # Ghi Ä‘Ã¨ cá»™t Label
            new_labels_df['Label'][i] = one_hot
        
        return new_labels_df
        
        # âš ï¸ PERFORMANCE NOTE:
        # Code nÃ y cháº­m do dÃ¹ng iterative approach
        # CÃ³ thá»ƒ optimize vá»›i sklearn.preprocessing.MultiLabelBinarizer:
        # 
        # from sklearn.preprocessing import MultiLabelBinarizer
        # mlb = MultiLabelBinarizer(classes=self.labels)
        # one_hot_labels = mlb.fit_transform(new_labels_df['Label'])
```

### sample() - Weighted Sampling cho Class Imbalance

```python
    def sample(self, num_samples, is_weighted=False):
        """
        Láº¥y máº«u tá»« dataset vá»›i option weighted sampling.
        
        PARAMETERS:
        - num_samples: Sá»‘ lÆ°á»£ng máº«u cáº§n láº¥y
        - is_weighted: Náº¿u True, Æ°u tiÃªn áº£nh cÃ³ nhiá»u bá»‡nh hÆ¡n
        
        WHY WEIGHTED SAMPLING?
        Dataset cÃ³ ~75% áº£nh "No Finding" (chá»‰ 1 label)
        Weighted sampling giÃºp model há»c Ä‘Æ°á»£c cÃ¡c trÆ°á»ng há»£p multi-label
        """
        
        if not is_weighted:
            # Random sampling Ä‘á»u
            return self.labels_df.sample(num_samples)
        else:
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # Weighted sampling: Æ¯u tiÃªn áº£nh cÃ³ nhiá»u labels
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            
            # TÃ­nh weight = sá»‘ lÆ°á»£ng labels + smoothing factor
            sample_weights = self.labels_df['Label'].map(
                lambda x: len(x)  # Sá»‘ bá»‡nh trong áº£nh
            ).values + 4e-2       # Smoothing Ä‘á»ƒ trÃ¡nh division by zero
            
            # Normalize thÃ nh probability distribution
            sample_weights /= sample_weights.sum()
            
            # Sample vá»›i weights
            return self.labels_df.sample(num_samples, weights=sample_weights)
            
            # VÃ Dá»¤:
            # áº¢nh A: 1 bá»‡nh â†’ weight = 1 + 0.04 = 1.04
            # áº¢nh B: 3 bá»‡nh â†’ weight = 3 + 0.04 = 3.04
            # áº¢nh B cÃ³ xÃ¡c suáº¥t Ä‘Æ°á»£c chá»n cao hÆ¡n ~3 láº§n!
```

## C1.2. ImageDataGenerator - Data Augmentation Chi Tiáº¿t

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TRAINING DATA AUGMENTATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
train_datagen = ImageDataGenerator(
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 1. RESCALE: Normalize pixel values
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    rescale=1./255,
    # Original: [0, 255] integers
    # After:    [0.0, 1.0] floats
    #
    # Táº I SAO?
    # - Neural networks há»c tá»‘t hÆ¡n vá»›i small values
    # - TrÃ¡nh exploding gradients
    # - Consistent vá»›i pretrained models (Ä‘á»u dÃ¹ng [0,1])
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 2. HORIZONTAL FLIP: Láº­t ngang
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    horizontal_flip=True,
    # âœ“ VALID cho X-ray vÃ¬:
    # - CÆ¡ thá»ƒ ngÆ°á»i Ä‘á»‘i xá»©ng (gáº§n nhÆ°)
    # - Bá»‡nh cÃ³ thá»ƒ xuáº¥t hiá»‡n á»Ÿ phá»•i trÃ¡i hoáº·c pháº£i
    # - TÄƒng data diversity 2x
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 3. VERTICAL FLIP: Láº­t dá»c
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    vertical_flip=False,
    # âœ— INVALID cho X-ray vÃ¬:
    # - X-ray cÃ³ orientation cá»‘ Ä‘á»‹nh (Ä‘áº§u trÃªn, chÃ¢n dÆ°á»›i)
    # - Láº­t dá»c táº¡o ra áº£nh khÃ´ng realistic
    # - Tim á»Ÿ vá»‹ trÃ­ cá»‘ Ä‘á»‹nh (trÃ¡i-dÆ°á»›i)
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 4. HEIGHT/WIDTH SHIFT: Dá»‹ch chuyá»ƒn
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    height_shift_range=0.05,   # Dá»‹ch dá»c Â±5%
    width_shift_range=0.1,     # Dá»‹ch ngang Â±10%
    # Giáº£ láº­p:
    # - Patient positioning variations
    # - Different X-ray machine setups
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 5. ROTATION: Xoay nháº¹
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    rotation_range=5,          # Â±5 degrees
    # CHá»ˆ xoay NHáº¸ vÃ¬:
    # - X-ray thÆ°á»ng Ä‘Æ°á»£c chá»¥p tháº³ng
    # - Xoay nhiá»u táº¡o artifacts khÃ´ng realistic
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 6. SHEAR: Biáº¿n dáº¡ng gÃ³c
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    shear_range=0.1,           # Shear intensity 0.1
    # Giáº£ láº­p oblique X-ray angles
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 7. ZOOM: PhÃ³ng to/thu nhá»
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    zoom_range=0.15,           # Zoom Â±15%
    # Giáº£ láº­p:
    # - Different patient distances from X-ray source
    # - Different lung sizes
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 8. FILL MODE: CÃ¡ch Ä‘iá»n pixels trá»‘ng
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    fill_mode='reflect'
    # Khi shift/rotate, sáº½ cÃ³ vÃ¹ng trá»‘ng
    # 'reflect': Mirror pixels á»Ÿ boundary
    # Alternatives: 'constant' (Ä‘en), 'nearest', 'wrap'
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# VALIDATION/TEST DATA: KHÃ”NG AUGMENTATION!
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
val_datagen = ImageDataGenerator(rescale=1./255)
# Chá»‰ rescale, khÃ´ng augment
# Táº I SAO? Validation pháº£i pháº£n Ã¡nh real-world performance
```

## C1.3. Data Generators - Flow from DataFrame

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TRAINING GENERATOR
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
train_generator = train_datagen.flow_from_dataframe(
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # DataFrame chá»©a file paths vÃ  labels
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    dataframe=train,
    # Columns: ['Id', 'Label']
    # Id: '00000001_000.png'
    # Label: ['Cardiomegaly', 'Emphysema']  # hoáº·c one-hot vector
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ThÆ° má»¥c chá»©a áº£nh
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    directory='/path/to/images',
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Column mapping
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    x_col="Id",        # Column chá»©a filename
    y_col="Label",     # Column chá»©a labels
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Batch size cho training
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    batch_size=32,
    # 32 lÃ  common choice:
    # - Äá»§ lá»›n Ä‘á»ƒ stable gradients
    # - Äá»§ nhá» Ä‘á»ƒ fit GPU memory
    # - Good balance speed vs convergence
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Target image size
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    target_size=(224, 224),
    # Táº I SAO 224Ã—224?
    # - Standard size cho ImageNet pretrained models
    # - ViT-B/16 expects 224Ã—224
    # - ResNet expects 224Ã—224
    # - CÃ¢n báº±ng giá»¯a detail vÃ  computation
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Class labels (thá»© tá»± quan trá»ng!)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    classes=parser.labels
    # Äáº£m báº£o one-hot encoding consistent vá»›i model output
)

# OUTPUT cá»§a generator:
# images: (batch_size, 224, 224, 3) - float32 [0,1]
# labels: (batch_size, 15) - one-hot vectors
```

---

# C2. CNN Model - Giáº£i thÃ­ch chuyÃªn sÃ¢u

## C2.1. Kiáº¿n trÃºc CNN Ä‘áº§y Ä‘á»§

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

def create_cnn_classifier():
    """
    Táº¡o CNN classifier Ä‘Æ¡n giáº£n cho multi-label classification.
    
    ARCHITECTURE OVERVIEW:
    Input (224,224,3) â†’ Conv â†’ Pool â†’ Conv â†’ Pool â†’ Flatten â†’ Dense â†’ Dense â†’ Output (15)
    """
    
    model = Sequential([
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # LAYER 1: First Convolution Block
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        Conv2D(
            filters=32,              # Sá»‘ lÆ°á»£ng kernels/filters
            kernel_size=(3, 3),      # KÃ­ch thÆ°á»›c má»—i kernel
            activation='relu',        # Activation function
            input_shape=(224, 224, 3) # HÃ—WÃ—C (RGB)
        ),
        # 
        # INPUT:  (batch, 224, 224, 3)
        # OUTPUT: (batch, 222, 222, 32)
        #
        # TÃNH TOÃN OUTPUT SIZE:
        # out = (input - kernel + 2*padding) / stride + 1
        # out = (224 - 3 + 0) / 1 + 1 = 222
        #
        # PARAMETERS:
        # weights: 3Ã—3Ã—3Ã—32 = 864
        # bias: 32
        # Total: 896 params
        #
        # CONV2D Há»ŒC GÃŒ?
        # Layer Ä‘áº§u há»c LOW-LEVEL FEATURES:
        # - Edges (cáº¡nh)
        # - Corners (gÃ³c)
        # - Textures (káº¿t cáº¥u)
        # - Color gradients
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # LAYER 2: First Max Pooling
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        MaxPooling2D(pool_size=(2, 2)),
        #
        # INPUT:  (batch, 222, 222, 32)
        # OUTPUT: (batch, 111, 111, 32)
        #
        # HOáº T Äá»˜NG:
        # Láº¥y MAX trong má»—i vÃ¹ng 2Ã—2, stride=2
        # 222 / 2 = 111
        #
        # Táº I SAO MAX POOLING?
        # 1. Giáº£m spatial dimensions (computation)
        # 2. Giá»¯ features máº¡nh nháº¥t
        # 3. TÄƒng receptive field
        # 4. Translation invariance
        #
        # PARAMETERS: 0 (khÃ´ng há»c Ä‘Æ°á»£c!)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # LAYER 3: Second Convolution Block
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        Conv2D(
            filters=64,
            kernel_size=(3, 3),
            activation='relu',
            name="last_conv_layer"  # Name cho visualization
        ),
        #
        # INPUT:  (batch, 111, 111, 32)
        # OUTPUT: (batch, 109, 109, 64)
        #
        # PARAMETERS:
        # weights: 3Ã—3Ã—32Ã—64 = 18,432
        # bias: 64
        # Total: 18,496 params
        #
        # LAYER 2 Há»ŒC GÃŒ?
        # MID-LEVEL FEATURES:
        # - Combinations of edges â†’ shapes
        # - Patterns specific to lungs
        # - Rib cage structures
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # LAYER 4: Second Max Pooling
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        MaxPooling2D(pool_size=(2, 2)),
        #
        # INPUT:  (batch, 109, 109, 64)
        # OUTPUT: (batch, 54, 54, 64)
        #
        # 109 / 2 = 54 (integer division)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # LAYER 5: Flatten
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        Flatten(),
        #
        # INPUT:  (batch, 54, 54, 64)
        # OUTPUT: (batch, 186624)
        #
        # TÃNH TOÃN:
        # 54 Ã— 54 Ã— 64 = 186,624 neurons!
        #
        # âš ï¸ ÄÃ‚Y LÃ€ Váº¤N Äá»€!
        # Flatten táº¡o ra vector quÃ¡ lá»›n
        # â†’ Dense layer tiáº¿p theo sáº½ cÃ³ quÃ¡ nhiá»u params
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # LAYER 6: Dense (Fully Connected)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        Dense(512, activation='relu'),
        #
        # INPUT:  (batch, 186624)
        # OUTPUT: (batch, 512)
        #
        # PARAMETERS:
        # weights: 186,624 Ã— 512 = 95,551,488
        # bias: 512
        # Total: 95,552,000 params
        #
        # ğŸ”´ CRITICAL ISSUE:
        # 95.5M params trong 1 layer!
        # = 99% tá»•ng sá»‘ params cá»§a model!
        # â†’ SEVERE OVERFITTING
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # LAYER 7: Output Layer
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        Dense(num_classes, activation='sigmoid')
        #
        # INPUT:  (batch, 512)
        # OUTPUT: (batch, 15)
        #
        # PARAMETERS:
        # weights: 512 Ã— 15 = 7,680
        # bias: 15
        # Total: 7,695 params
        #
        # Táº I SAO SIGMOID (khÃ´ng pháº£i SOFTMAX)?
        # - Multi-label classification
        # - Má»—i output INDEPENDENT
        # - CÃ³ thá»ƒ nhiá»u bá»‡nh cÃ¹ng lÃºc
        # - Output: probability cho Má»–I class
    ])
    
    return model
```

## C2.2. PhÃ¢n tÃ­ch váº¥n Ä‘á» CNN

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    CNN PARAMETER BREAKDOWN                     â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Layer              â”‚  Output Shape    â”‚  Parameters          â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Conv2D (32)        â”‚  (222, 222, 32)  â”‚  896                 â•‘
â•‘  MaxPooling2D       â”‚  (111, 111, 32)  â”‚  0                   â•‘
â•‘  Conv2D (64)        â”‚  (109, 109, 64)  â”‚  18,496              â•‘
â•‘  MaxPooling2D       â”‚  (54, 54, 64)    â”‚  0                   â•‘
â•‘  Flatten            â”‚  (186624,)       â”‚  0                   â•‘
â•‘  Dense (512)        â”‚  (512,)          â”‚  95,552,000  â† 99%!  â•‘
â•‘  Dense (15)         â”‚  (15,)           â”‚  7,695               â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  TOTAL              â”‚                  â”‚  95,579,087          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ”´ PROBLEMS:

1. PARAMETER IMBALANCE:
   - 99% params trong Dense layer
   - Conv layers chá»‰ cÃ³ ~20K params
   - Model chá»§ yáº¿u "memorize" thay vÃ¬ "learn features"

2. OVERFITTING:
   - Train accuracy: ~95%
   - Val accuracy: ~70%
   - Gap 25% = severe overfitting!

3. INSUFFICIENT DEPTH:
   - Chá»‰ 2 conv layers
   - Receptive field nhá»
   - KhÃ´ng há»c Ä‘Æ°á»£c high-level features

4. NO REGULARIZATION:
   - KhÃ´ng cÃ³ BatchNorm
   - KhÃ´ng cÃ³ Dropout
   - KhÃ´ng cÃ³ L2 regularization

âœ… RECOMMENDED FIXES:

1. REPLACE Flatten vá»›i GlobalAveragePooling:
   Before: (54,54,64) â†’ Flatten â†’ 186,624 neurons
   After:  (54,54,64) â†’ GAP â†’ 64 neurons
   â†’ Giáº£m 2900 láº§n params!

2. ADD BatchNormalization sau má»—i Conv

3. ADD Dropout trÆ°á»›c Dense layers

4. INCREASE Conv layers (4-5 layers)

5. HOáº¶C: DÃ¹ng Transfer Learning!
```

## C2.3. Training Loop - run_experiment()

```python
def run_experiment(model):
    """
    Compile vÃ  train model vá»›i cÃ¡c settings chuáº©n.
    """
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # OPTIMIZER: AdamW
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    optimizer = keras.optimizers.AdamW(
        learning_rate=learning_rate,  # 1e-4 = 0.0001
        weight_decay=weight_decay     # 1e-6 = L2 regularization
    )
    #
    # Táº I SAO AdamW?
    # - Adam: Adaptive learning rate (momentum + RMSprop)
    # - W: Weight decay (L2 regularization decoupled)
    # - Tá»‘t hÆ¡n Adam thÆ°á»ng cho regularization
    #
    # LEARNING RATE 1e-4:
    # - KhÃ´ng quÃ¡ lá»›n (unstable)
    # - KhÃ´ng quÃ¡ nhá» (slow convergence)
    # - Standard cho fine-tuning
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # COMPILE MODEL
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    model.compile(
        optimizer=optimizer,
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # LOSS FUNCTION: Binary Cross-Entropy
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        loss='binary_crossentropy',
        # Táº I SAO BCE?
        # - Multi-label: Má»—i output lÃ  binary classification
        # - 15 independent binary classifiers
        # - Sigmoid output + BCE loss
        #
        # BCE = -[y*log(Å·) + (1-y)*log(1-Å·)]
        # Average over all 15 classes
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # METRICS
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        metrics=[
            keras.metrics.BinaryAccuracy(name="accuracy"),
            # TÃ­nh % predictions Ä‘Ãºng
            # (Å· > 0.5) == y
            
            keras.metrics.AUC(name="auc"),
            # Area Under ROC Curve
            # KhÃ´ng phá»¥ thuá»™c threshold
            # 0.5 = random, 1.0 = perfect
        ]
    )
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # TRAINING
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    history = model.fit(
        train_generator,
        epochs=num_epochs,           # 10 epochs
        validation_data=validation_generator,
        
        callbacks=[
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # ModelCheckpoint: Save best model
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            ModelCheckpoint(
                os.path.join("files", "model.keras"),
                monitor='val_loss',   # Theo dÃµi validation loss
                verbose=1,
                save_best_only=True   # Chá»‰ save khi improve
            )
            # Táº I SAO monitor val_loss?
            # - PhÃ¡t hiá»‡n overfitting sá»›m
            # - Val_loss tÄƒng = overfitting
        ]
    )
    
    return history

# TRAINING OUTPUT EXAMPLE:
# Epoch 1/10
# 1875/1875 [==============================] - 45s 24ms/step
# loss: 0.2345 - accuracy: 0.8234 - auc: 0.7123
# val_loss: 0.3456 - val_accuracy: 0.7654 - val_auc: 0.6543
```

---

# C3. ResNet Model - Giáº£i thÃ­ch chuyÃªn sÃ¢u

## C3.1. Residual Block - TrÃ¡i tim cá»§a ResNet

```python
from tensorflow.keras.layers import (
    Input, Conv2D, BatchNormalization, Activation, 
    MaxPooling2D, GlobalAveragePooling2D, Dense, Add
)
from tensorflow.keras.models import Model

def block(x, filters, strides=1):
    """
    Basic Residual Block cho ResNet-34
    
    ARCHITECTURE:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                        INPUT x                              â”‚
    â”‚                           â”‚                                 â”‚
    â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
    â”‚           â”‚               â”‚               â”‚                â”‚
    â”‚           â–¼               â”‚               â”‚                â”‚
    â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚               â”‚                â”‚
    â”‚    â”‚ Conv 3Ã—3    â”‚        â”‚               â”‚                â”‚
    â”‚    â”‚ BatchNorm   â”‚        â”‚    Identity   â”‚                â”‚
    â”‚    â”‚ ReLU        â”‚        â”‚    Shortcut   â”‚                â”‚
    â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚               â”‚                â”‚
    â”‚           â”‚               â”‚               â”‚                â”‚
    â”‚           â–¼               â”‚               â”‚                â”‚
    â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚               â”‚                â”‚
    â”‚    â”‚ Conv 3Ã—3    â”‚        â”‚    (Projectionâ”‚                â”‚
    â”‚    â”‚ BatchNorm   â”‚        â”‚    if needed) â”‚                â”‚
    â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚               â”‚                â”‚
    â”‚           â”‚               â”‚               â”‚                â”‚
    â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
    â”‚                           â”‚                                 â”‚
    â”‚                           â–¼                                 â”‚
    â”‚                      ADD (x + F(x))                        â”‚
    â”‚                           â”‚                                 â”‚
    â”‚                           â–¼                                 â”‚
    â”‚                        ReLU                                 â”‚
    â”‚                           â”‚                                 â”‚
    â”‚                        OUTPUT                               â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    PARAMETERS:
    - x: Input tensor
    - filters: Sá»‘ filters cho conv layers
    - strides: 1 (same size) hoáº·c 2 (downsampling)
    """
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # SAVE IDENTITY FOR SKIP CONNECTION
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    identity = x
    # Giá»¯ láº¡i input gá»‘c Ä‘á»ƒ cá»™ng vÃ o sau
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # FIRST CONV-BN-RELU
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    x = Conv2D(
        filters=filters,
        kernel_size=3,
        strides=strides,      # CÃ³ thá»ƒ lÃ  1 hoáº·c 2
        padding='same'        # Giá»¯ spatial size (khi stride=1)
    )(x)
    #
    # VÃ Dá»¤ vá»›i strides=2:
    # Input: (56, 56, 64)
    # Output: (28, 28, 128)  â† Halved spatial, doubled channels
    
    x = BatchNormalization()(x)
    #
    # BATCH NORMALIZATION:
    # Normalize activations trong má»—i mini-batch
    # xÌ‚ = (x - Î¼) / Ïƒ
    # y = Î³xÌ‚ + Î² (learnable scale vÃ  shift)
    #
    # BENEFITS:
    # 1. Faster training
    # 2. Higher learning rates
    # 3. Reduces internal covariate shift
    # 4. Regularization effect
    
    x = Activation('relu')(x)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # SECOND CONV-BN (no activation yet!)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    x = Conv2D(
        filters=filters,
        kernel_size=3,
        strides=1,            # LuÃ´n lÃ  1 cho conv thá»© 2
        padding='same'
    )(x)
    x = BatchNormalization()(x)
    # KHÃ”NG cÃ³ ReLU á»Ÿ Ä‘Ã¢y!
    # ReLU sáº½ Ä‘Æ°á»£c apply SAU khi cá»™ng vá»›i identity
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PROJECTION SHORTCUT (if needed)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    if strides != 1 or identity.shape[-1] != filters:
        # Cáº¦N projection khi:
        # 1. strides != 1: Spatial size thay Ä‘á»•i
        # 2. channels thay Ä‘á»•i: identity.shape[-1] != filters
        
        identity = Conv2D(
            filters=filters,
            kernel_size=1,    # 1Ã—1 convolution
            strides=strides,  # Match stride cá»§a main path
            padding='same'
        )(identity)
        identity = BatchNormalization()(identity)
        
        # 1Ã—1 CONVOLUTION:
        # - Chá»‰ thay Ä‘á»•i sá»‘ channels
        # - Vá»›i strides=2: cÅ©ng downsampling
        # - Ráº¥t Ã­t params: filters Ã— in_channels
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # RESIDUAL CONNECTION: y = F(x) + x
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    x = x + identity
    # hoáº·c: x = Add()([x, identity])
    #
    # ÄÃ‚Y LÃ€ MAGIC Cá»¦A RESNET!
    #
    # MATHEMATICAL INSIGHT:
    # Gradient qua block:
    # âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚y Ã— (âˆ‚F/âˆ‚x + 1)
    #                        â†‘
    #                   IDENTITY TERM!
    #
    # Gradient LUÃ”N cÃ³ term "+1"
    # â†’ Gradient KHÃ”NG THá»‚ vanish hoÃ n toÃ n!
    # â†’ CÃ³ thá»ƒ train networks ráº¥t sÃ¢u (152+ layers)
    
    x = Activation('relu')(x)
    # ReLU cuá»‘i cÃ¹ng sau khi cá»™ng
    
    return x
```

## C3.2. Full ResNet-34 Architecture

```python
def create_resnet():
    """
    Táº¡o ResNet-34 tá»« scratch.
    
    ARCHITECTURE:
    - STEM: 7Ã—7 conv, maxpool
    - STAGE 1: 3 blocks, 64 filters
    - STAGE 2: 4 blocks, 128 filters
    - STAGE 3: 6 blocks, 256 filters
    - STAGE 4: 3 blocks, 512 filters
    - HEAD: GlobalAvgPool, Dense(15)
    
    WHY "34"?
    1 (stem conv) + 2Ã—(3+4+6+3) = 1 + 32 = 33 conv layers
    + 1 dense layer = 34 layers
    """
    
    inputs = Input(shape=input_shape)  # (224, 224, 3)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STEM: Initial Feature Extraction
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    x = Conv2D(64, (7, 7), strides=2, padding='same')(inputs)
    # Input: (224, 224, 3) â†’ Output: (112, 112, 64)
    # 7Ã—7 kernel captures large-scale features
    
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    
    x = MaxPooling2D(pool_size=3, strides=2, padding='same')(x)
    # (112, 112, 64) â†’ (56, 56, 64)
    # Aggressive downsampling á»Ÿ Ä‘áº§u
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STAGE 1: 3 Residual Blocks, 64 filters
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    x = block(x, 64)  # Giá»¯ nguyÃªn size: (56, 56, 64)
    x = block(x, 64)
    x = block(x, 64)
    # Spatial: 56Ã—56, Channels: 64
    # LOW-LEVEL FEATURES: edges, textures
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STAGE 2: 4 Residual Blocks, 128 filters
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    x = block(x, 128, strides=2)  # Downsample: (56,56,64) â†’ (28,28,128)
    x = block(x, 128)
    x = block(x, 128)
    x = block(x, 128)
    # Spatial: 28Ã—28, Channels: 128
    # MID-LEVEL FEATURES: shapes, local patterns
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STAGE 3: 6 Residual Blocks, 256 filters
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    x = block(x, 256, strides=2)  # Downsample: (28,28,128) â†’ (14,14,256)
    x = block(x, 256)
    x = block(x, 256)
    x = block(x, 256)
    x = block(x, 256)
    x = block(x, 256)
    # Spatial: 14Ã—14, Channels: 256
    # HIGH-LEVEL FEATURES: complex patterns, anatomical structures
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STAGE 4: 3 Residual Blocks, 512 filters
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    x = block(x, 512, strides=2)  # Downsample: (14,14,256) â†’ (7,7,512)
    x = block(x, 512)
    x = block(x, 512)
    # Spatial: 7Ã—7, Channels: 512
    # SEMANTIC FEATURES: disease-specific patterns
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # CLASSIFICATION HEAD
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    x = GlobalAveragePooling2D()(x)
    # (7, 7, 512) â†’ (512,)
    # Average má»—i 7Ã—7 feature map thÃ nh 1 sá»‘
    #
    # COMPARE WITH FLATTEN:
    # Flatten: 7Ã—7Ã—512 = 25,088 neurons
    # GAP:     512 neurons
    # â†’ Giáº£m 49Ã— sá»‘ params trong Dense layer!
    # â†’ Less overfitting
    
    outputs = Dense(num_classes, activation='sigmoid')(x)
    # (512,) â†’ (15,)
    # params: 512 Ã— 15 + 15 = 7,695
    
    model = Model(inputs, outputs)
    return model
```

## C3.3. ResNet Parameter Summary

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    RESNET-34 ARCHITECTURE                          â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Stage    â”‚ Output Size â”‚ Blocks â”‚ Filters â”‚ Params (approx)      â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  STEM     â”‚ 56Ã—56       â”‚ -      â”‚ 64      â”‚ 9,472                â•‘
â•‘  Stage 1  â”‚ 56Ã—56       â”‚ 3      â”‚ 64      â”‚ 222,720              â•‘
â•‘  Stage 2  â”‚ 28Ã—28       â”‚ 4      â”‚ 128     â”‚ 1,116,416            â•‘
â•‘  Stage 3  â”‚ 14Ã—14       â”‚ 6      â”‚ 256     â”‚ 6,690,304            â•‘
â•‘  Stage 4  â”‚ 7Ã—7         â”‚ 3      â”‚ 512     â”‚ 13,304,832           â•‘
â•‘  HEAD     â”‚ 15          â”‚ -      â”‚ -       â”‚ 7,695                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  TOTAL    â”‚             â”‚ 16     â”‚         â”‚ ~21.3M               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… ADVANTAGES OVER CNN:
1. Skip connections â†’ no vanishing gradient
2. Can go much deeper (34 vs 2 layers)
3. BatchNorm â†’ stable training
4. GlobalAvgPool â†’ less overfitting
5. Better feature hierarchy
```

---

# C4. ViT Models - Giáº£i thÃ­ch chuyÃªn sÃ¢u

## C4.1. Patches Layer - Chia áº£nh thÃ nh patches

```python
from tensorflow.keras import layers
import tensorflow as tf

class Patches(layers.Layer):
    """
    Custom Keras layer Ä‘á»ƒ chia áº£nh thÃ nh non-overlapping patches.
    
    VÃ Dá»¤:
    Image 224Ã—224 vá»›i patch_size=32
    â†’ 7Ã—7 = 49 patches
    â†’ Má»—i patch: 32Ã—32Ã—3 = 3072 pixels
    
    Image 224Ã—224 vá»›i patch_size=16
    â†’ 14Ã—14 = 196 patches
    â†’ Má»—i patch: 16Ã—16Ã—3 = 768 pixels
    """
    
    def __init__(self, patch_size):
        super(Patches, self).__init__()
        self.patch_size = patch_size
    
    def call(self, images):
        """
        Forward pass: Chia áº£nh thÃ nh patches.
        
        PARAMETERS:
        - images: Tensor shape (batch, height, width, channels)
        
        RETURNS:
        - patches: Tensor shape (batch, num_patches, patch_dim)
        """
        
        # Get batch size dynamically
        batch_size = tf.shape(images)[0]
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # EXTRACT PATCHES using TensorFlow built-in
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        patches = tf.image.extract_patches(
            images=images,
            sizes=[1, self.patch_size, self.patch_size, 1],
            # sizes: [batch, height, width, channels]
            # 1 image at a time, patch_sizeÃ—patch_size, all channels
            
            strides=[1, self.patch_size, self.patch_size, 1],
            # Non-overlapping: stride = patch_size
            # Náº¿u stride < patch_size â†’ overlapping patches
            
            rates=[1, 1, 1, 1],
            # Dilation rate (1 = no dilation)
            
            padding='VALID'
            # No padding â†’ pháº£i chia háº¿t
        )
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # RESHAPE: (batch, H', W', patch_dim) â†’ (batch, num_patches, patch_dim)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        #
        # SAU extract_patches:
        # patches.shape = (batch, num_patches_h, num_patches_w, patch_dim)
        # VD: (32, 7, 7, 3072) vá»›i 224Ã—224 image, 32Ã—32 patches
        #
        # patch_dim = patch_size Ã— patch_size Ã— channels
        #           = 32 Ã— 32 Ã— 3 = 3072
        
        patch_dims = patches.shape[-1]  # 3072
        
        # Reshape thÃ nh (batch, 49, 3072)
        patches = tf.reshape(patches, [batch_size, -1, patch_dims])
        # -1: Tá»± Ä‘á»™ng tÃ­nh = 7Ã—7 = 49
        
        return patches
    
    def get_config(self):
        """Cho serialization/deserialization cá»§a model."""
        config = super(Patches, self).get_config()
        config.update({"patch_size": self.patch_size})
        return config

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# VISUALIZATION: CÃ¡ch patches hoáº¡t Ä‘á»™ng
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
ORIGINAL IMAGE (224Ã—224):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                 â”‚
â”‚     P1      P2      P3      P4      P5      P6      P7        â”‚
â”‚   32Ã—32   32Ã—32   32Ã—32   32Ã—32   32Ã—32   32Ã—32   32Ã—32       â”‚
â”‚                                                                 â”‚
â”‚     P8      P9      P10     P11     P12     P13     P14       â”‚
â”‚   32Ã—32   32Ã—32   32Ã—32   32Ã—32   32Ã—32   32Ã—32   32Ã—32       â”‚
â”‚                                                                 â”‚
â”‚     ...    ...     ...     ...     ...     ...     ...        â”‚
â”‚                                                                 â”‚
â”‚     P43     P44     P45     P46     P47     P48     P49       â”‚
â”‚   32Ã—32   32Ã—32   32Ã—32   32Ã—32   32Ã—32   32Ã—32   32Ã—32       â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â†’ 7 Ã— 7 = 49 patches
â†’ Má»—i patch flatten: 32Ã—32Ã—3 = 3072 dimensions

OUTPUT TENSOR:
patches.shape = (batch_size, 49, 3072)
                     â†‘       â†‘     â†‘
                  samples  seq_len  embed_dim (raw)
"""
```

## C4.2. PatchEncoder - Linear Projection + Position Embedding

```python
class PatchEncoder(layers.Layer):
    """
    Encode patches vá»›i:
    1. Linear projection: Giáº£m dimensionality
    2. Position embedding: ThÃªm spatial information
    
    INTUITION:
    Sau Patches layer: Má»—i patch lÃ  3072 dims (quÃ¡ lá»›n!)
    PatchEncoder: Project xuá»‘ng projection_dim (64 hoáº·c 768)
    + Add position info Ä‘á»ƒ biáº¿t patch á»Ÿ Ä‘Ã¢u trong áº£nh
    """
    
    def __init__(self, num_patches, projection_dim):
        super().__init__()
        self.num_patches = num_patches      # 49 hoáº·c 196
        self.projection_dim = projection_dim # 64 hoáº·c 768
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # LINEAR PROJECTION
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        self.projection = layers.Dense(units=projection_dim)
        # Input: 3072 â†’ Output: projection_dim
        # Giá»‘ng nhÆ° "word embedding" trong NLP
        # Má»—i patch â†’ vector trong embedding space
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # POSITION EMBEDDING (Learnable)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        self.position_embedding = layers.Embedding(
            input_dim=num_patches,     # Sá»‘ vá»‹ trÃ­ cÃ³ thá»ƒ: 49
            output_dim=projection_dim  # Embedding dimension: 64
        )
        # Embedding table: (49, 64)
        # position_embedding[0] = vector cho patch Ä‘áº§u tiÃªn
        # position_embedding[48] = vector cho patch cuá»‘i
        #
        # Táº I SAO LEARNABLE (khÃ´ng dÃ¹ng sinusoidal)?
        # - Model cÃ³ thá»ƒ Tá»° Há»ŒC spatial relationships
        # - Thá»±c táº¿ há»c Ä‘Æ°á»£c 2D grid structure
        # - Simpler implementation
    
    def call(self, patch):
        """
        Forward pass: Project patches + add positions.
        
        INPUT: (batch, num_patches, patch_dim)  # (32, 49, 3072)
        OUTPUT: (batch, num_patches, projection_dim)  # (32, 49, 64)
        """
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Táº O POSITION INDICES
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        positions = tf.expand_dims(
            tf.range(start=0, limit=self.num_patches, delta=1),
            axis=0
        )
        # positions = [[0, 1, 2, ..., 48]]
        # Shape: (1, 49)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # LINEAR PROJECTION
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        projected_patches = self.projection(patch)
        # (batch, 49, 3072) @ W(3072, 64) â†’ (batch, 49, 64)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ADD POSITION EMBEDDINGS
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        encoded = projected_patches + self.position_embedding(positions)
        # projected_patches: (batch, 49, 64)
        # position_embedding(positions): (1, 49, 64) â†’ broadcast
        # Result: (batch, 49, 64)
        #
        # INTUITION:
        # Patch embedding: "ÄÃ¢y lÃ  patch cÃ³ lung tissue"
        # Position embedding: "Patch nÃ y á»Ÿ gÃ³c trÃªn trÃ¡i"
        # Combined: "Lung tissue á»Ÿ gÃ³c trÃªn trÃ¡i"
        
        return encoded
    
    def get_config(self):
        config = super().get_config()
        config.update({"num_patches": self.num_patches})
        return config
```

## C4.3. Transformer Encoder Block

```python
def create_vit_classifier():
    """
    Táº¡o complete ViT classifier.
    """
    
    inputs = Input(shape=input_shape)  # (224, 224, 3)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PATCH EMBEDDING
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    patches = Patches(patch_size)(inputs)  # (batch, 49, 3072)
    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)
    # (batch, 49, 64)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # TRANSFORMER ENCODER BLOCKS Ã— 8
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    for _ in range(transformer_layers):  # transformer_layers = 8
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # MULTI-HEAD SELF-ATTENTION SUBLAYER
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        
        # Layer Normalization TRÆ¯á»šC attention (Pre-LN)
        x1 = LayerNormalization(epsilon=1e-6)(encoded_patches)
        
        # Multi-Head Self-Attention
        attention_output = MultiHeadAttention(
            num_heads=num_heads,      # 4 heads
            key_dim=projection_dim,   # 64
            dropout=0.1
        )(x1, x1)  # Query=Key=Value=x1 â†’ SELF-attention
        #
        # MULTI-HEAD ATTENTION BREAKDOWN:
        #
        # 1. Split into heads:
        #    x1: (batch, 49, 64) â†’ 4 heads Ã— (batch, 49, 16)
        #
        # 2. Compute Q, K, V for each head:
        #    Q = x1 @ W_Q  (49, 16)
        #    K = x1 @ W_K  (49, 16)
        #    V = x1 @ W_V  (49, 16)
        #
        # 3. Attention per head:
        #    scores = Q @ K^T / sqrt(16)  # (49, 49)
        #    weights = softmax(scores)    # (49, 49)
        #    output = weights @ V         # (49, 16)
        #
        # 4. Concatenate heads:
        #    concat: 4 Ã— (49, 16) â†’ (49, 64)
        #
        # 5. Final projection:
        #    output = concat @ W_O  # (49, 64)
        #
        # PARAMS per MHA:
        # Q, K, V projections: 3 Ã— 64 Ã— 64 = 12,288
        # Output projection: 64 Ã— 64 = 4,096
        # Total: ~16K params
        
        # Residual connection
        x2 = Add()([attention_output, encoded_patches])
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # MLP (FEED-FORWARD) SUBLAYER
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        
        # Layer Normalization TRÆ¯á»šC MLP
        x3 = LayerNormalization(epsilon=1e-6)(x2)
        
        # MLP: Expand then contract
        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)
        # transformer_units = [128, 64]
        # 64 â†’ 128 (expand) â†’ 64 (contract)
        #
        # Táº I SAO EXPAND-CONTRACT?
        # - Larger intermediate dimension = more capacity
        # - Typical ratio: 4Ã— (64 â†’ 256 trong standard ViT)
        # - Giá»‘ng nhÆ° "bottleneck" ngÆ°á»£c
        
        # Residual connection
        encoded_patches = Add()([x3, x2])
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # CLASSIFICATION HEAD
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    # Final Layer Normalization
    representation = LayerNormalization(epsilon=1e-6)(encoded_patches)
    
    # Flatten all patches
    representation = Flatten()(representation)
    # (batch, 49, 64) â†’ (batch, 3136)
    #
    # âš ï¸ ALTERNATIVE: Use [CLS] token
    # Standard ViT thÃªm learnable [CLS] token á»Ÿ Ä‘áº§u
    # Chá»‰ dÃ¹ng [CLS] token output cho classification
    # ÄÆ¡n giáº£n hÆ¡n vÃ  tá»‘t hÆ¡n Flatten
    
    representation = Dropout(0.5)(representation)
    
    # MLP Head
    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)
    # mlp_head_units = [2048, 1024]
    # 3136 â†’ 2048 â†’ 1024
    
    # Output layer
    logits = Dense(num_classes, activation='sigmoid')(features)
    # 1024 â†’ 15
    
    model = Model(inputs=inputs, outputs=logits)
    return model
```

## C4.4. MLP Helper Function

```python
def mlp(x, hidden_units, dropout_rate, regularizer_rate=0.01):
    """
    Multi-Layer Perceptron vá»›i GELU activation.
    
    PARAMETERS:
    - x: Input tensor
    - hidden_units: List cÃ¡c layer sizes, vd [128, 64]
    - dropout_rate: Dropout probability
    - regularizer_rate: L2 regularization weight
    
    GELU (Gaussian Error Linear Unit):
    GELU(x) = x Ã— Î¦(x)
    
    Trong Ä‘Ã³ Î¦(x) = CDF cá»§a standard normal distribution
    
    Táº I SAO GELU thay vÃ¬ ReLU?
    - Smoother than ReLU
    - No "dead neurons" problem
    - Better for Transformers
    - Used in BERT, GPT, ViT
    """
    
    for units in hidden_units:
        x = Dense(
            units,
            activation=tf.nn.gelu,
            kernel_regularizer=l2(regularizer_rate)  # L2 regularization
        )(x)
        x = Dropout(dropout_rate)(x)
    
    return x
```

## C4.5. ViT-Pretrained (Best Model) - PyTorch

```python
import torch
import torch.nn as nn
from torch.optim import Adam
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
import timm

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CUSTOM DATASET CLASS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
class ChestXRayDataset(Dataset):
    """
    PyTorch Dataset cho Chest X-ray images.
    
    KhÃ¡c vá»›i TensorFlow ImageDataGenerator:
    - More control over loading process
    - Works with PyTorch DataLoader
    - Custom transforms pipeline
    """
    
    def __init__(self, image_paths, labels, transform=None):
        self.image_paths = image_paths  # List of file paths
        self.labels = labels            # NumPy array of one-hot labels
        self.transform = transform      # Torchvision transforms
    
    def __len__(self):
        """Sá»‘ lÆ°á»£ng samples trong dataset."""
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        """
        Láº¥y 1 sample (image, label) táº¡i index idx.
        
        ÄÆ¯á»¢C Gá»ŒI Bá»I: DataLoader khi iterate
        """
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # LOAD IMAGE
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        image_path = self.image_paths[idx]
        image = cv2.imread(image_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # OpenCV loads as BGR, convert to RGB
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # APPLY TRANSFORMS
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if self.transform:
            image = self.transform(image)
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # GET LABEL
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        label = self.labels[idx]
        
        return image, label

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DATA TRANSFORMS (PyTorch style)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
transform = transforms.Compose([
    transforms.ToPILImage(),
    # OpenCV â†’ PIL Image (required for torchvision transforms)
    
    transforms.Resize((224, 224)),
    # Resize vá» 224Ã—224 (ViT input size)
    
    transforms.RandomHorizontalFlip(),
    # Random flip vá»›i p=0.5
    
    transforms.RandomRotation(20),
    # Random rotation Â±20 degrees
    # âš ï¸ Nhiá»u hÆ¡n TensorFlow version (5 degrees)
    
    transforms.ToTensor(),
    # PIL Image â†’ Tensor
    # Chuyá»ƒn [0,255] â†’ [0,1]
    # Chuyá»ƒn (H,W,C) â†’ (C,H,W)
    
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
    # ImageNet normalization statistics!
    # QUAN TRá»ŒNG: Pretrained model Ä‘Æ°á»£c train vá»›i stats nÃ y
    # Pháº£i dÃ¹ng CÃ™NG normalization khi inference
])

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DATA LOADERS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
batch_size = 16  # Smaller than TF version do GPU memory

loader_train = DataLoader(
    train_dataset,
    batch_size=batch_size,
    shuffle=True,      # Shuffle training data
    num_workers=4      # Parallel data loading
)

loader_val = DataLoader(
    val_dataset,
    batch_size=batch_size,
    shuffle=False,     # Don't shuffle validation
    num_workers=4
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LOAD PRETRAINED ViT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
model = timm.create_model(
    'vit_base_patch16_224',  # ViT-Base with 16Ã—16 patches
    pretrained=True          # Load ImageNet-21k pretrained weights
)
#
# ARCHITECTURE DETAILS:
# - patch_size: 16Ã—16
# - num_patches: 14Ã—14 = 196
# - embed_dim: 768
# - depth: 12 transformer blocks
# - num_heads: 12
# - mlp_ratio: 4 (MLP hidden = 768Ã—4 = 3072)
# - Total params: ~86M
#
# PRETRAINED ON:
# - ImageNet-21k (14 million images, 21,843 classes)
# - Then fine-tuned on ImageNet-1k
# - Learned powerful visual representations!

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MODIFY CLASSIFICATION HEAD
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
num_classes = 15
model.head = nn.Linear(model.head.in_features, num_classes)
# Replace: Linear(768 â†’ 1000) vá»›i Linear(768 â†’ 15)
#
# GIá»® NGUYÃŠN:
# - Patch embedding
# - All 12 transformer blocks
# - Position embeddings
#
# CHá»ˆ THAY:
# - Classification head (final layer)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MOVE TO GPU
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LOSS & OPTIMIZER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
criterion = nn.BCEWithLogitsLoss()
# Binary Cross-Entropy WITH LOGITS
# = Sigmoid + BCELoss combined (numerically stable)
#
# QUAN TRá»ŒNG: Model output KHÃ”NG cÃ³ sigmoid
# BCEWithLogitsLoss tá»± apply sigmoid internally

optimizer = Adam(model.parameters(), lr=1e-4)
# Adam optimizer vá»›i learning rate nhá»
# VÃ¬ Ä‘ang fine-tuning, khÃ´ng cáº§n train from scratch
```

## C4.6. Training Loop (PyTorch)

```python
def train_model(model, criterion, optimizer, loader_train, loader_val, num_epochs=10):
    """
    Complete training loop cho PyTorch model.
    """
    
    train_losses = []
    val_losses = []
    train_accuracies = []
    val_accuracies = []
    
    for epoch in range(num_epochs):
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # TRAINING PHASE
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        model.train()  # Set model to training mode
        # Enables: Dropout, BatchNorm training behavior
        
        running_loss = 0.0
        running_corrects = 0
        total_samples = 0
        
        for inputs, labels in loader_train:
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # MOVE TO GPU
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            inputs = inputs.to(device)  # (batch, 3, 224, 224)
            labels = labels.to(device)  # (batch, 15)
            
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # FORWARD PASS
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            optimizer.zero_grad()  # Clear gradients
            
            outputs = model(inputs)  # (batch, 15) - raw logits
            loss = criterion(outputs, labels)
            
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # PREDICTIONS
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            preds = outputs.sigmoid() > 0.5  # Apply sigmoid, threshold
            # preds: (batch, 15) boolean tensor
            
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # BACKWARD PASS
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            loss.backward()   # Compute gradients
            optimizer.step()  # Update weights
            
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # ACCUMULATE METRICS
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            running_loss += loss.item() * inputs.size(0)
            running_corrects += (preds == labels.byte()).sum().item()
            total_samples += labels.numel()  # Total elements
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # EPOCH METRICS
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        epoch_loss = running_loss / len(loader_train.dataset)
        epoch_acc = running_corrects / total_samples * 100
        
        train_losses.append(epoch_loss)
        train_accuracies.append(epoch_acc)
        
        print(f'Epoch {epoch+1}/{num_epochs}')
        print(f'Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.2f}%')
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # VALIDATION PHASE
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        val_loss, val_acc = validate_model(model, loader_val, criterion)
        val_losses.append(val_loss)
        val_accuracies.append(val_acc)
    
    return train_losses, val_losses, train_accuracies, val_accuracies


def validate_model(model, loader_val, criterion, threshold=0.5):
    """
    Evaluate model on validation set.
    """
    
    model.eval()  # Set model to evaluation mode
    # Disables: Dropout, BatchNorm uses running stats
    
    total_samples = 0
    total_correct = 0
    running_loss = 0.0
    
    with torch.no_grad():  # Disable gradient computation
        # Saves memory, faster inference
        
        for inputs, labels in loader_val:
            inputs = inputs.to(device)
            labels = labels.to(device)
            
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            
            predicted = outputs.sigmoid() > threshold
            
            running_loss += loss.item() * inputs.size(0)
            total_correct += (predicted == labels.byte()).sum().item()
            total_samples += labels.numel()
    
    val_loss = running_loss / len(loader_val.dataset)
    accuracy = total_correct / total_samples * 100
    
    print(f'Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.2f}%')
    
    return val_loss, accuracy
```

## C4.7. So sÃ¡nh cÃ¡c ViT Versions

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                        VIT MODELS COMPARISON                                   â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Attribute         â”‚ ViT-v1        â”‚ ViT-v2        â”‚ ViT-Pretrained          â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Framework         â”‚ TensorFlow    â”‚ TensorFlow    â”‚ PyTorch + timm          â•‘
â•‘  Patch Size        â”‚ 32Ã—32         â”‚ 32Ã—32         â”‚ 16Ã—16                   â•‘
â•‘  Num Patches       â”‚ 49            â”‚ 49            â”‚ 196                     â•‘
â•‘  Embed Dim         â”‚ 64            â”‚ 64            â”‚ 768                     â•‘
â•‘  Transformer Blocksâ”‚ 8             â”‚ 8             â”‚ 12                      â•‘
â•‘  Attention Heads   â”‚ 4             â”‚ 4             â”‚ 12                      â•‘
â•‘  MLP Ratio         â”‚ 2Ã—            â”‚ 2Ã—            â”‚ 4Ã—                      â•‘
â•‘  Total Params      â”‚ ~3M           â”‚ ~3M           â”‚ ~86M                    â•‘
â•‘  Pretrained        â”‚ âŒ No         â”‚ âŒ No         â”‚ âœ… ImageNet-21k         â•‘
â•‘  Regularization    â”‚ Dropout only  â”‚ Dropout + L2  â”‚ Dropout                 â•‘
â•‘  Optimizer         â”‚ AdamW         â”‚ SGD + Scheduleâ”‚ Adam                    â•‘
â•‘  Expected AUC      â”‚ 0.60-0.68     â”‚ 0.68-0.75     â”‚ 0.82-0.88               â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Main Issues       â”‚ ops not       â”‚ l2 not        â”‚ Different data path     â•‘
â•‘                    â”‚ defined       â”‚ imported      â”‚ than others             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

KEY INSIGHTS:

1. PATCH SIZE MATTERS:
   32Ã—32 â†’ 49 patches â†’ Less tokens â†’ Less computation
   16Ã—16 â†’ 196 patches â†’ More tokens â†’ Better detail â†’ Better performance

2. PRETRAINED IS KEY:
   ViT needs LOTS of data to train from scratch
   ImageNet-21k pretraining provides:
   - Low-level features (edges, textures)
   - Mid-level features (shapes, patterns)
   - High-level visual concepts
   
   Only need to fine-tune for medical domain!

3. MODEL SIZE vs PERFORMANCE:
   Small ViT (3M params) without pretraining â†’ struggles
   Large ViT (86M params) with pretraining â†’ excellent
   
   "Pretrained > Size > Architecture"
```

---

# ğŸ“š TÃ€I LIá»†U THAM KHáº¢O

## Papers

1. **Attention Is All You Need** (Vaswani et al., 2017)
   - Giá»›i thiá»‡u Transformer architecture
   - Self-attention mechanism
   - arXiv:1706.03762

2. **An Image is Worth 16x16 Words** (Dosovitskiy et al., 2020)
   - Vision Transformer (ViT)
   - Patch-based image processing
   - arXiv:2010.11929

3. **Deep Residual Learning for Image Recognition** (He et al., 2015)
   - ResNet architecture
   - Skip connections
   - arXiv:1512.03385

4. **ChestX-ray8** (Wang et al., 2017)
   - NIH dataset paper
   - Multi-label classification
   - CVPR 2017

5. **CheXNet** (Rajpurkar et al., 2017)
   - Radiologist-level performance
   - DenseNet-121 baseline
   - arXiv:1711.05225

6. **Focal Loss for Dense Object Detection** (Lin et al., 2017)
   - Class imbalance solution
   - arXiv:1708.02002

7. **Adam: A Method for Stochastic Optimization** (Kingma & Ba, 2014)
   - Adam optimizer
   - arXiv:1412.6980

8. **Understanding the Difficulty of Training Deep Feedforward Neural Networks** (Glorot & Bengio, 2010)
   - Xavier initialization
   - AISTATS 2010

## Libraries & Frameworks

- **TensorFlow/Keras**: https://tensorflow.org
- **PyTorch**: https://pytorch.org
- **timm (PyTorch Image Models)**: https://github.com/huggingface/pytorch-image-models
- **scikit-learn**: https://scikit-learn.org

## Online Resources

- **ViT Paper Explained**: https://jalammar.github.io/illustrated-transformer/
- **ResNet Paper Explained**: https://towardsdatascience.com/residual-networks-resnets-cb474c7c834a
- **NIH Chest X-ray Dataset**: https://nihcc.app.box.com/v/ChestXray-NIHCC

---

# ğŸ“‹ PHá»¤ Lá»¤C: BUGS & FIXES

## Bug 1: data.ipynb - Random Sampling Error

```python
# BUG:
idxs = random  # Sai! random lÃ  module, khÃ´ng pháº£i list

# FIX:
idxs = random.sample(idxs, num_images)
```

## Bug 2: ViT-v1.ipynb - ops Not Defined

```python
# BUG:
positions = ops.expand_dims(...)  # 'ops' khÃ´ng Ä‘Æ°á»£c import

# FIX Option 1: Import keras.ops
from keras import ops
positions = ops.expand_dims(...)

# FIX Option 2: DÃ¹ng tf.expand_dims
positions = tf.expand_dims(
    tf.range(start=0, limit=self.num_patches, delta=1),
    axis=0
)
```

## Bug 3: ViT-v2.ipynb - l2 Not Imported

```python
# BUG:
kernel_regularizer=l2(regularizer_rate)  # l2 khÃ´ng Ä‘Æ°á»£c import

# FIX:
from keras.regularizers import l2
# hoáº·c
from tensorflow.keras.regularizers import l2
```

## Bug 4: ViT-v2.ipynb - EarlyStopping restore_best_weights

```python
# BUG:
restore_best_weights=False  # KhÃ´ng khÃ´i phá»¥c weights tá»‘t nháº¥t!

# FIX:
early_stopping_callback = EarlyStopping(
    monitor="val_accuracy",
    patience=3,
    restore_best_weights=True  # Quan trá»ng!
)
```

## Bug 5: CNN - Missing Imports

```python
# BUG: Sequential, Conv2D, etc. khÃ´ng Ä‘Æ°á»£c import

# FIX: Add imports
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
```

## Bug 6: ResNet - Missing GlobalAveragePooling2D Import

```python
# BUG: GlobalAveragePooling2D khÃ´ng Ä‘Æ°á»£c import

# FIX: Update import
from tensorflow.keras.layers import (
    Input, Conv2D, BatchNormalization, Activation, 
    MaxPooling2D, GlobalAveragePooling2D, Dense
)
```

---

*TÃ i liá»‡u tá»•ng há»£p hoÃ n chá»‰nh cho dá»± Ã¡n ViT-Chest-Xray*
*Bao gá»“m: LÃ½ thuyáº¿t Deep Learning + Giáº£i thÃ­ch Code Chi tiáº¿t*
*AI Expert Analysis System - January 2025*
