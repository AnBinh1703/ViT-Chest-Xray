% ============================================================
%  BÁO CÁO PHÂN TÍCH CHUYÊN SÂU (1 FILE LaTeX) — BẢN SỬA LỖI
%  Paper: A Comparative Study of CNN, ResNet, and Vision Transformers 
%         for Multi-Classification of Chest Diseases
%  Biên dịch: TeX Live (pdfLaTeX), UTF-8
%
%  Sửa lỗi chính:
%   (1) pdfLaTeX không đọc .webp  → đổi sang .png/.jpg và chặn lỗi nếu thiếu ảnh
%   (2) Sai tên ảnh outout-cnn.png → output-cnn.png (hoặc giữ file đúng tên)
%   (3) Tránh lỗi “Cannot determine size / Division by 0” do ảnh thiếu/hỏng
%   (4) Tránh xung đột \Bbbk giữa newtxmath và amssymb → bỏ newtxmath
%   (5) Thêm \end{document} và macro include ảnh an toàn
% ============================================================

\documentclass[12pt,a4paper]{report}

% =========================
% Vietnamese + Encoding (pdfLaTeX)
% =========================
\usepackage[utf8]{inputenc}
\usepackage[T5]{fontenc}
\usepackage[vietnamese]{babel}

% Rename default titles to Vietnamese
\addto\captionsvietnamese{
  \renewcommand{\contentsname}{MỤC LỤC}
  \renewcommand{\listfigurename}{DANH SÁCH HÌNH}
  \renewcommand{\listtablename}{DANH SÁCH BẢNG}
  \renewcommand{\chaptername}{Chương}
  \renewcommand{\appendixname}{Phụ lục}
  \renewcommand{\refname}{Tài liệu tham khảo}
  \renewcommand{\bibname}{Tài liệu tham khảo}
  \renewcommand{\indexname}{Chỉ mục}
  \renewcommand{\figurename}{Hình}
  \renewcommand{\tablename}{Bảng}
}

% =========================
% Fonts (Times-like text)
% (Bỏ newtxmath để tránh xung đột \Bbbk / AMS)
% =========================
\usepackage{newtxtext}

% =========================
% Page / Typography
% =========================
\usepackage[a4paper,top=1cm,bottom=1.5cm,left=1cm,right=1cm]{geometry}
\usepackage{setspace}
\onehalfspacing
\usepackage{parskip}
\usepackage{indentfirst}
\usepackage{microtype}

% Enhanced chapter/section formatting for academic presentation
\usepackage{titlesec}

% Chapter formatting - larger, more prominent
\titleformat{\chapter}
  {\normalfont\huge\bfseries\color{blue!70!black}}
  {\chaptertitlename\ \thechapter}{1em}
  {\Huge\color{blue!70!black}}
\titlespacing*{\chapter}{0pt}{-10pt}{10pt}

% Section formatting - professional blue color
\titleformat{\section}
  {\normalfont\Large\bfseries\color{blue!60!black}}
  {\thesection}{1em}{}
\titlespacing*{\section}{0pt}{6pt plus 2pt minus 1pt}{2pt plus 1pt minus 0.5pt}

% Subsection formatting - darker blue
\titleformat{\subsection}
  {\normalfont\large\bfseries\color{blue!50!black}}
  {\thesubsection}{1em}{}
\titlespacing*{\subsection}{0pt}{8pt plus 2pt minus 1pt}{4pt plus 1pt minus 0.5pt}

% Subsubsection formatting
\titleformat{\subsubsection}
  {\normalfont\normalsize\bfseries\color{blue!40!black}}
  {\thesubsubsection}{1em}{}
\titlespacing*{\subsubsection}{0pt}{10pt plus 2pt minus 1pt}{4pt plus 1pt minus 1pt}

% =========================
% Math / Tables / Figures
% =========================
\usepackage{amsmath,amssymb,mathtools}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{float}
\usepackage{subcaption}

% Enhanced caption formatting
\usepackage{caption}
\captionsetup{
  font={small,it},
  labelfont={bf,color=blue!70!black},
  textfont={it},
  format=hang,
  justification=justified,
  singlelinecheck=false,
  margin=10pt
}
\captionsetup[table]{position=top}
\captionsetup[figure]{position=bottom}

\usepackage{siunitx}
\usepackage{algorithm,algorithmic}
\usepackage{textcomp}

% Chỉ cho phép các định dạng ảnh mà pdfLaTeX đọc tốt
\DeclareGraphicsExtensions{.pdf,.png,.jpg,.jpeg}

% Include ảnh trực tiếp (bỏ macro vì gây lỗi LaTeX)

% TikZ for diagrams
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,decorations.pathreplacing,calc}

% Symbols
\usepackage{pifont}

% =========================
% Links (hyperref should be near the end)
% =========================
\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{
  colorlinks=true,
  linkcolor=blue!70!black,
  citecolor=green!50!black,
  urlcolor=blue
}

% =========================
% Page numbering with monospace font
% =========================
\usepackage{fancyhdr}
\fancypagestyle{plain}{
  \fancyhf{} % clear all header and footer fields
  \fancyfoot[C]{\ttfamily\footnotesize\thepage} % center footer with monospace font
  \renewcommand{\headrulewidth}{0pt} % no header rule
  \renewcommand{\footrulewidth}{0pt} % no footer rule
}
\pagestyle{plain}

% =========================
% Lists / Code
% =========================
\usepackage{enumitem}
\setlist[itemize]{leftmargin=1.2cm}
\setlist[enumerate]{leftmargin=1.2cm}

\usepackage{listings}
\usepackage{tcolorbox}
\tcbuselibrary{listings,skins,breakable}

% Code listing style
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{pythonstyle}{
  backgroundcolor=\color{backcolour},
  commentstyle=\normalfont\ttfamily\itshape\color{codegreen},
  keywordstyle=\color{magenta},
  numberstyle=\tiny\color{codegray},
  stringstyle=\color{codepurple},
  basicstyle=\ttfamily\footnotesize,
  breakatwhitespace=false,
  breaklines=true,
  captionpos=b,
  keepspaces=true,
  numbers=left,
  numbersep=5pt,
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2,
  language=Python,
  inputencoding=utf8,
  extendedchars=true,
  upquote=true
}
\lstset{style=pythonstyle}

\lstdefinestyle{plain}{
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=none,
  backgroundcolor=\color{backcolour}
}

% =========================
% Macros
% =========================
\newcommand{\paperref}[1]{\textcolor{blue!70!black}{[Paper: #1]}}
\newcommand{\coderef}[1]{\textcolor{green!50!black}{[Code: #1]}}
\newcommand{\important}[1]{\textbf{\textcolor{red!70!black}{#1}}}
\newcommand{\concept}[1]{\textit{\textcolor{purple}{#1}}}

\newcommand{\PaperTitle}{A Comparative Study of CNN, ResNet, and Vision Transformers for Multi-Classification of Chest Diseases}
\newcommand{\Dataset}{NIH ChestX-ray14}
\newcommand{\Task}{phân loại đa nhãn (multi-label classification)}
\newcommand{\CNN}{\textsc{CNN}}
\newcommand{\ResNet}{\textsc{ResNet}}
\newcommand{\ViT}{\textsc{ViT}}
\newcommand{\AUC}{\textsc{AUC}}
\newcommand{\ROC}{\textsc{ROC}}
\newcommand{\BCE}{\textsc{BCE}}

% =========================
% Document
% =========================
\begin{document}

% ============================================================
% TITLE PAGE
% ============================================================
\begin{titlepage}
  \centering

  % Logo FPT University with shadow box effect
  \begin{tcolorbox}[colback=white,colframe=blue!70!black,boxrule=1.5pt,arc=3mm,width=0.5\textwidth]
    \centering
    \includegraphics[width=0.85\textwidth]{../../image/Logo-FSB.png}
  \end{tcolorbox}
  \vspace{1cm}

  % Report title with enhanced styling
  {\LARGE \textbf{\color{blue!70!black}BÁO CÁO PHÂN TÍCH BÀI BÁO KHOA HỌC}}\\[1cm]

  % Main paper title with decorative rules
  {\color{blue!70!black}\rule{\textwidth}{1.5pt}}\\[0.5cm]
  {\Large\textbf{\color{blue!60!black}A Comparative Study of CNN, ResNet, and Vision Transformers\\
  for Multi-Classification of Chest Diseases}}\\[0.5cm]
  {\color{blue!70!black}\rule{\textwidth}{1.5pt}}\\[0.3cm]

  % Original paper info in a nice box
  \begin{tcolorbox}[colback=blue!5!white,colframe=blue!50!black,boxrule=0.8pt,arc=2mm,width=0.85\textwidth]
    \centering
    {\large \textbf{arXiv: 2406.00237}}\\[0.3cm]
    {\normalsize \textit{Ananya Jain, Aviral Bhardwaj, Kaushik Murali, Isha Surani}}\\
    {\small University of Toronto}
  \end{tcolorbox}
  \vspace{0.2cm}

  % Dataset and framework info in table format
  \begin{tcolorbox}[colback=gray!5!white,colframe=gray!50!black,boxrule=0.8pt,arc=2mm,width=0.85\textwidth]
    \begin{tabular}{rl}
      \textbf{Kho mã nguồn gốc:}    & \url{https://github.com/Aviral-03/ViT-Chest-Xray} \\
      \textbf{Bộ dữ liệu:}          & NIH ChestX-ray14 (112\,120 ảnh X-quang) \\
      \textbf{Framework gốc:}       & TensorFlow/Keras \\
      \textbf{Framework hiện tại:}  & PyTorch \\
       \textbf{Link Github của team:}  & \url{https://github.com/AnBinh1703/ViT-Chest-Xray} \\
    \end{tabular}
  \end{tcolorbox}

  \vfill

  % Student information in a professional box
  \begin{tcolorbox}[colback=green!5!white,colframe=green!50!black,boxrule=0.8pt,arc=2mm,width=0.85\textwidth,left=1cm]
    \textbf{\large Sinh viên thực hiện:}\\[0.2cm]
    \begin{tabular}{ll}
      Dương Bình An      & \textbf{MSSV:} 25MSA23234 \\
      Lê Quang Tuyến     & \textbf{MSSV:} 25MSA23232 \\
      Nguyễn Lê Hồng Nhi & \textbf{MSSV:} 25MSA23235 \\
    \end{tabular}\\[0.6cm]
    
    \begin{tabular}{ll}
      \textbf{Lớp:} & MSA29HCM\\
      \textbf{Môn học:} & Deep Learning\\
      \textbf{Giảng viên hướng dẫn:} & TS. Lê Việt Tuấn
    \end{tabular}
  \end{tcolorbox}

  {\textbf{\color{blue!70!black}Thành phố Hồ Chí Minh -- 2026}}
\end{titlepage}

% ============================================================
% ABSTRACT & KEYWORDS
% ============================================================
\chapter*{\textbf{\color{blue!70!black}Tóm tắt nghiên cứu}}
\addcontentsline{toc}{chapter}{Tóm tắt nghiên cứu}

\begin{tcolorbox}[enhanced,colback=blue!3!white,colframe=blue!60!black,
  boxrule=1pt,arc=3mm,left=8pt,right=8pt,top=10pt,bottom=10pt,
  title={\textbf{\large Tổng quan bài báo}}]

Bài báo \textit{``A Comparative Study of CNN, ResNet, and Vision Transformers for Multi-Classification of Chest Diseases''} (arXiv:2406.00237) nghiên cứu và so sánh hiệu năng của ba nhóm kiến trúc học sâu phổ biến trong bài toán \textbf{phân loại đa nhãn} bệnh lý trên ảnh X-quang ngực, bao gồm:

\begin{enumerate}[leftmargin=1.5cm]
  \item \textbf{Convolutional Neural Networks (CNN):} Mô hình baseline với kiến trúc tích chập truyền thống.
  \item \textbf{Residual Networks (ResNet):} Mạng CNN sâu sử dụng skip-connection nhằm khắc phục hiện tượng mất gradient.
  \item \textbf{Vision Transformers (ViT):} Kiến trúc Transformer áp dụng cho thị giác máy tính thông qua cơ chế self-attention.
\end{enumerate}

Nghiên cứu được thực hiện trên bộ dữ liệu \textbf{NIH ChestX-ray14} với hơn 112.000 ảnh X-quang ngực và 14 nhãn bệnh lý khác nhau.
\end{tcolorbox}

\vspace{0.5cm}

\begin{tcolorbox}[enhanced,colback=green!3!white,colframe=green!60!black,
  boxrule=1pt,arc=3mm,left=8pt,right=8pt,top=10pt,bottom=10pt,
  title={\textbf{\large Đóng góp chính của bài báo}}]

Các đóng góp nổi bật của bài báo bao gồm:
\begin{itemize}[leftmargin=1.5cm]
  \item So sánh có hệ thống hiệu năng giữa CNN, ResNet và Vision Transformer trên cùng tập dữ liệu và thiết lập thí nghiệm.
  \item Đề xuất hai biến thể Vision Transformer: ViT-v1 (huấn luyện từ đầu) và ViT-v2 (bổ sung regularization).
  \item Giới thiệu mô hình lai \textbf{ViT--ResNet} với pre-training trên ImageNet-21k nhằm kết hợp ưu điểm của CNN và Transformer.
  \item Phân tích \textbf{attention maps} để trực quan hoá và diễn giải quyết định của mô hình ViT trong ngữ cảnh y tế.
\end{itemize}
\end{tcolorbox}

\vspace{0.5cm}

\begin{tcolorbox}[enhanced,colback=yellow!5!white,colframe=orange!60!black,
  boxrule=1pt,arc=3mm,left=8pt,right=8pt,top=10pt,bottom=10pt,
  title={\textbf{\large Kết quả thực nghiệm chính}}]

\begin{table}[H]
\centering
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Mô hình} & \textbf{Test Acc (\%)} & \textbf{Val Acc (\%)} & \textbf{AUC} & \textbf{Ghi chú} \\
\midrule
CNN & 91.00 & 92.68 & 0.82 & Baseline \\
ResNet & 93.00 & 93.34 & 0.86 & Strong baseline \\
ViT-v1 & 92.63 & 92.89 & 0.86 & Huấn luyện từ đầu \\
ViT-v2 & 92.83 & 92.95 & 0.84 & Có regularization \\
ViT--ResNet & \textbf{93.90} & \textbf{94.07} & 0.85 & Mô hình lai, pre-trained \\
\bottomrule
\end{tabular}
\end{table}

\textit{Kết quả cho thấy ResNet và ViT--ResNet đạt hiệu năng cao và ổn định hơn so với CNN baseline và các biến thể ViT huấn luyện từ đầu.}
\end{tcolorbox}

\vspace{0.5cm}

\begin{tcolorbox}[enhanced,colback=purple!3!white,colframe=purple!60!black,
  boxrule=1pt,arc=3mm,left=8pt,right=8pt,top=10pt,bottom=10pt,
  title={\textbf{\large Phạm vi và mục tiêu của báo cáo}}]

Báo cáo phân tích chuyên sâu này không chỉ dừng lại ở việc tóm tắt bài báo gốc, mà còn mở rộng và đào sâu các khía cạnh sau:
\begin{enumerate}[leftmargin=1.5cm]
  \item \textbf{Phân tích lý thuyết:} Trình bày chi tiết nền tảng toán học và trực giác của CNN, ResNet và Vision Transformer.
  \item \textbf{Liên kết paper với mã nguồn:} Mapping rõ ràng giữa các mô tả trong bài báo và phần cài đặt trong kho mã nguồn GitHub.
  \item \textbf{Tái lập và cải tiến:} Chuyển đổi pipeline từ TensorFlow/Keras sang PyTorch và đề xuất các cải tiến hiện đại.
  \item \textbf{Đánh giá phê bình:} Phân tích ưu, nhược điểm của từng kiến trúc và khả năng ứng dụng trong thực tế lâm sàng.
\end{enumerate}
\end{tcolorbox}

\vspace{0.5cm}

\begin{tcolorbox}[enhanced,colback=cyan!3!white,colframe=cyan!60!black,
  boxrule=1pt,arc=3mm,left=8pt,right=8pt,top=8pt,bottom=8pt,
  title={\textbf{\large Từ khoá}}]
\centering
\textbf{Keywords:} Chest X-ray $\bullet$ Multi-label Classification $\bullet$ Convolutional Neural Networks $\bullet$ Residual Networks $\bullet$ Vision Transformer $\bullet$ Self-Attention $\bullet$ Transfer Learning $\bullet$ Medical Image Analysis $\bullet$ ROC-AUC $\bullet$ PyTorch $\bullet$ Deep Learning
\end{tcolorbox}

% ============================================================
% TABLE OF CONTENTS, LIST OF FIGURES, LIST OF TABLES
% ============================================================
\pagenumbering{roman}

\tableofcontents
\clearpage

\renewcommand{\listfigurename}{DANH SÁCH HÌNH}
\addcontentsline{toc}{chapter}{DANH SÁCH HÌNH}
\listoffigures
\clearpage

\renewcommand{\listtablename}{DANH SÁCH BẢNG}
\addcontentsline{toc}{chapter}{DANH SÁCH BẢNG}
\listoftables
\clearpage

\pagenumbering{arabic}
\setcounter{page}{1}

% ============================================================
% CHAPTER 1: INTRODUCTION
% ============================================================
\chapter{Giới thiệu và Bối cảnh nghiên cứu}
\label{chap:introduction}

\section{\textbf{Bối cảnh lâm sàng}}

Bệnh lý phổi và tim–phổi (như viêm phổi, tràn dịch màng phổi, khí phế thũng, xẹp phổi, tim to, v.v.) là một trong những nguyên nhân hàng đầu gây tử vong trên toàn cầu. Theo Tổ chức Y tế Thế giới (WHO), các bệnh liên quan đến hệ hô hấp chiếm hơn 10\% tổng số ca tử vong mỗi năm, đặc biệt phổ biến tại các quốc gia đang phát triển.

Trong thực hành lâm sàng, \textbf{X-quang ngực (Chest X-ray)} là phương tiện chẩn đoán hình ảnh được sử dụng rộng rãi nhất do các ưu điểm sau:
\begin{itemize}
  \item \textbf{Chi phí thấp:} So với CT hoặc MRI, X-quang có chi phí triển khai thấp hơn đáng kể.
  \item \textbf{Thời gian xử lý nhanh:} Kết quả có thể thu được trong vòng vài phút.
  \item \textbf{Khả năng sàng lọc ban đầu:} Thường là chỉ định đầu tiên khi nghi ngờ bệnh lý phổi.
  \item \textbf{Phát hiện đa dạng bệnh lý:} Có thể phát hiện nhiều dạng tổn thương khác nhau trong cùng một ảnh.
\end{itemize}

\section{\textbf{Thách thức trong chẩn đoán X-quang ngực}}

Mặc dù có nhiều ưu điểm, việc diễn giải ảnh X-quang ngực trong thực tế lâm sàng vẫn tồn tại nhiều thách thức nghiêm trọng. Như bài báo gốc đã chỉ ra:

\begin{tcolorbox}[colback=yellow!5!white,colframe=yellow!75!black,title=Trích dẫn từ bài báo gốc (Section 1)]
\textit{``Although chest X-ray imaging is a relatively low cost tool for diagnosis, radiologists are needed to analyze these images. However the limited access to radiologists in many areas, and the variability between radiologists can be a problem.''}
\end{tcolorbox}

Các thách thức chính bao gồm:
\begin{enumerate}
  \item \textbf{Thiếu hụt bác sĩ chẩn đoán hình ảnh:} Đặc biệt tại vùng sâu, vùng xa hoặc các quốc gia đang phát triển.
  \item \textbf{Biến thiên giữa các chuyên gia (inter-observer variability):} Cùng một ảnh có thể dẫn đến các kết luận khác nhau.
  \item \textbf{Khối lượng dữ liệu lớn:} Một bác sĩ có thể phải đọc hàng trăm ảnh mỗi ngày, làm tăng nguy cơ sai sót.
  \item \textbf{Dấu hiệu bệnh tinh vi:} Nhiều tổn thương giai đoạn sớm rất khó phát hiện bằng mắt thường.
\end{enumerate}

\section{\textbf{Động lực ứng dụng trí tuệ nhân tạo trong chẩn đoán hình ảnh}}

Sự phát triển mạnh mẽ của \textbf{học sâu (Deep Learning)} đã tạo ra bước ngoặt trong lĩnh vực phân tích hình ảnh y tế. Các mô hình học sâu có khả năng tự động học đặc trưng từ dữ liệu ảnh lớn, giảm phụ thuộc vào kinh nghiệm chủ quan của con người.

Những lợi ích chính của việc áp dụng học máy trong chẩn đoán hình ảnh y tế bao gồm:
\begin{itemize}
  \item \textbf{Tăng độ chính xác và tính nhất quán} trong chẩn đoán.
  \item \textbf{Hỗ trợ bác sĩ} trong quá trình ra quyết định lâm sàng.
  \item \textbf{Mở rộng khả năng tiếp cận} dịch vụ y tế tại các khu vực thiếu nhân lực chuyên môn.
\end{itemize}

Về mặt lịch sử, các mốc phát triển quan trọng của học sâu trong phân tích ảnh y tế có thể tóm tắt như sau:
\begin{enumerate}
  \item \textbf{2012 -- AlexNet:} Khởi đầu kỷ nguyên học sâu với CNN.
  \item \textbf{2015 -- ResNet:} Giải quyết vấn đề mất gradient, cho phép huấn luyện mạng rất sâu.
  \item \textbf{2017 -- Transformer:} Giới thiệu cơ chế self-attention trong NLP.
  \item \textbf{2020 -- Vision Transformer (ViT):} Áp dụng Transformer cho thị giác máy tính.
\end{enumerate}

\section{\textbf{Giới thiệu bài báo gốc}}

Bài báo \textit{``A Comparative Study of CNN, ResNet, and Vision Transformers for Multi-Classification of Chest Diseases''} (arXiv:2406.00237) tập trung so sánh hiệu năng của ba nhóm kiến trúc học sâu phổ biến trong bài toán \textbf{phân loại đa nhãn} bệnh lý trên ảnh X-quang ngực, sử dụng bộ dữ liệu \textbf{NIH ChestX-ray14}.

Các mô hình được nghiên cứu bao gồm:
\begin{itemize}
  \item CNN baseline truyền thống.
  \item ResNet-34 với skip connections.
  \item Hai biến thể Vision Transformer (ViT-v1, ViT-v2).
  \item Mô hình lai ViT--ResNet sử dụng pre-training trên ImageNet-21k.
\end{itemize}

\section{\textbf{Câu hỏi nghiên cứu}}

Bài báo gốc đặt ra các câu hỏi nghiên cứu cốt lõi sau:
\begin{enumerate}
  \item[\textbf{RQ1}] Hiệu năng của CNN, ResNet và ViT khác nhau như thế nào trong phân loại bệnh X-quang ngực?
  \item[\textbf{RQ2}] Vision Transformer huấn luyện từ đầu có thể cạnh tranh với ResNet hay không?
  \item[\textbf{RQ3}] Transfer learning ảnh hưởng như thế nào đến hiệu năng của ViT?
  \item[\textbf{RQ4}] Attention maps có giúp cải thiện khả năng diễn giải mô hình hay không?
\end{enumerate}

\section{\textbf{Mục tiêu và phạm vi báo cáo}}

Báo cáo này hướng tới các mục tiêu sau:
\begin{itemize}
  \item Phân tích sâu nền tảng lý thuyết của CNN, ResNet và Vision Transformer.
  \item Liên kết nội dung bài báo với mã nguồn triển khai thực tế.
  \item Đánh giá vai trò của inductive bias và transfer learning trong ảnh y tế.
  \item Đề xuất các hướng cải tiến dựa trên các phương pháp hiện đại.
\end{itemize}

Phạm vi nghiên cứu bao gồm:
\begin{itemize}
  \item \textbf{Bộ dữ liệu:} NIH ChestX-ray14.
  \item \textbf{Bài toán:} Phân loại đa nhãn (multi-label classification).
  \item \textbf{Mô hình:} CNN, ResNet-34, ViT và ViT--ResNet.
\end{itemize}

\section{\textbf{Cấu trúc báo cáo}}

Phần còn lại của báo cáo được tổ chức như sau:
\begin{itemize}
  \item \textbf{Chương 2:} Các công trình liên quan (Related Works).
  \item \textbf{Chương 3:} Phân tích bộ dữ liệu NIH ChestX-ray14.
  \item \textbf{Chương 4:} Mô hình CNN -- lý thuyết và triển khai.
  \item \textbf{Chương 5:} Mô hình ResNet.
  \item \textbf{Chương 6:} Vision Transformer và mô hình lai.
  \item \textbf{Chương 7:} Thực nghiệm và phân tích kết quả.
  \item \textbf{Chương 8:} Các cải tiến và thảo luận mở rộng.
  \item \textbf{Chương 9:} Kết luận và hướng nghiên cứu tương lai.
\end{itemize}

% ============================================================
% CHAPTER 2: RELATED WORKS
% ============================================================
\chapter{Các Công Trình Liên Quan (Related Works)}
\label{chap:related_works}

\section{\textbf{Tổng quan}}

Lĩnh vực phân tích ảnh X-quang ngực bằng học sâu đã chứng kiến sự phát triển mạnh mẽ trong thập kỷ qua. Chương này tổng hợp các công trình tiêu biểu theo bốn hướng nghiên cứu chính: (1) Phương pháp dựa trên CNN, (2) Kiến trúc Transformer cho thị giác y tế, (3) Xử lý mất cân bằng lớp, và (4) Khả năng diễn giải mô hình.

% ============================================================
\section{\textbf{Phương pháp dựa trên CNN (CNN-based Methods)}}
\label{sec:rw_cnn}

\subsection{\textbf{ChestX-ray14 và CheXNet}}

\textbf{Wang et al. (2017)} \cite{wang2017chestxray14} giới thiệu bộ dữ liệu NIH ChestX-ray14 với 112,120 ảnh và đề xuất baseline sử dụng DenseNet-121. Đây là công trình nền tảng cho hầu hết các nghiên cứu tiếp theo.

\textbf{Rajpurkar et al. (2017)} \cite{rajpurkar2017chexnet} phát triển \textbf{CheXNet} dựa trên DenseNet-121 với 121 lớp, đạt AUC 0.841 trên task phát hiện Pneumonia, vượt qua mức độ chính xác của bác sĩ X-quang. Các đóng góp chính:
\begin{itemize}
  \item Sử dụng ImageNet pretraining kết hợp fine-tuning.
  \item Class activation maps (CAM) để trực quan hóa vùng quan tâm.
  \item Đánh giá nghiêm ngặt với radiologist-level comparison.
\end{itemize}

\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=Key Insight: DenseNet vs ResNet]
\textbf{DenseNet} kết nối mọi lớp với mọi lớp sau đó (dense connections), giúp:
\begin{itemize}
  \item Gradient flow tốt hơn ResNet.
  \item Feature reuse hiệu quả, giảm số tham số.
  \item Phù hợp với medical imaging (cần preserve fine details).
\end{itemize}
\end{tcolorbox}

\subsection{\textbf{CheXpert và Uncertainty Labeling}}

\textbf{Irvin et al. (2019)} \cite{irvin2019chexpert} công bố bộ dữ liệu \textbf{CheXpert} với 224,316 ảnh từ 65,240 bệnh nhân. Khác biệt quan trọng:
\begin{itemize}
  \item Xử lý \textbf{uncertain labels} (U-Ones, U-Zeros, U-Ignore strategies).
  \item Multi-task learning với 14 pathologies.
  \item Policy optimization cho uncertain labels: U-Ones cho Atelectasis, U-Zeros cho Pleural Effusion.
\end{itemize}

\textbf{So sánh CheXpert vs NIH ChestX-ray14:}
\begin{table}[H]
\centering
\caption{So sánh hai bộ dữ liệu lớn nhất}
\begin{tabular}{lcc}
\toprule
\textbf{Đặc điểm} & \textbf{NIH ChestX-ray14} & \textbf{CheXpert} \\
\midrule
Số ảnh & 112,120 & 224,316 \\
Số bệnh nhân & 30,805 & 65,240 \\
Số bệnh lý & 14 (+ No Finding) & 14 \\
Label extraction & NLP (weak) & NLP + uncertainty handling \\
View position & Frontal only & Frontal + Lateral \\
Public & \checkmark & \checkmark \\
\bottomrule
\end{tabular}
\end{table}

\subsection{\textbf{MIMIC-CXR và Vision-Language Models}}

\textbf{Johnson et al. (2019)} \cite{johnson2019mimic} phát hành \textbf{MIMIC-CXR} với 377,110 ảnh kèm theo \textbf{227,835 radiology reports}. Đây là bước đầu tiên hướng đến vision-language models trong y tế.

% ============================================================
\section{\textbf{Kiến trúc Transformer cho Thị giác Y tế}}
\label{sec:rw_transformer}

\subsection{\textbf{Vision Transformer (ViT)}}

\textbf{Dosovitskiy et al. (2020)} \cite{dosovitskiy2020vit} giới thiệu Vision Transformer, áp dụng kiến trúc Transformer thuần túy cho ảnh:
\begin{itemize}
  \item Chia ảnh thành patches 16×16 hoặc 32×32.
  \item Self-attention toàn cục thay vì convolution.
  \item Cần pretraining quy mô lớn (ImageNet-21k, JFT-300M).
\end{itemize}

\textbf{Kết quả quan trọng:}
\begin{itemize}
  \item ViT-Huge pretrained trên JFT-300M đạt SOTA ImageNet.
  \item ViT-Base trained from scratch \textbf{thua ResNet-50} khi dữ liệu nhỏ.
  \item Cần $\sim$100M ảnh để ViT vượt CNN inductive bias.
\end{itemize}

\subsection{\textbf{Swin Transformer}}

\textbf{Liu et al. (2021)} \cite{liu2021swin} đề xuất \textbf{Swin Transformer} với shifted windows:
\begin{itemize}
  \item Hierarchical feature maps (giống CNN).
  \item Complexity $O(HW)$ thay vì $O((HW)^2)$ của ViT.
  \item Phù hợp medical imaging: cần multi-scale features.
\end{itemize}

\textbf{Ứng dụng trong y tế:} Swin-Tiny đạt AUC 0.903 trên NIH ChestX-ray14 (nêu trong Chapter 8), vượt ViT-Base (0.836) với ít tham số hơn (28M vs 86M).

\subsection{\textbf{Hybrid CNN-Transformer Architectures}}

\textbf{Xu et al. (2021)} \cite{xu2021vitae} đề xuất ViTAE (ViT with Absolute position Embedding), kết hợp:
\begin{itemize}
  \item CNN stem để extract low-level features.
  \item Transformer encoder cho global reasoning.
  \item Hiệu quả hơn pure ViT với dữ liệu y tế (< 1M ảnh).
\end{itemize}

% ============================================================
\section{\textbf{Xử lý Mất Cân Bằng Lớp (Class Imbalance Handling)}}
\label{sec:rw_imbalance}

\subsection{\textbf{Focal Loss}}

\textbf{Lin et al. (2017)} \cite{lin2017focal} đề xuất Focal Loss cho object detection với extreme imbalance:
\begin{equation}
\text{FL}(p_t) = -\alpha_t (1 - p_t)^\gamma \log(p_t)
\end{equation}

\textbf{Ứng dụng medical imaging:}
\begin{itemize}
  \item Hernia (0.20\%) vs No Finding (53.84\%): tỷ lệ 1:269.
  \item Focal Loss ($\gamma=2$) tăng AUC từ 0.722 → 0.758 (Chapter 8).
  \item Down-weight easy negatives (No Finding), focus on hard positives (rare diseases).
\end{itemize}

\subsection{\textbf{Asymmetric Loss (ASL)}}

\textbf{Ridnik et al. (2021)} \cite{ridnik2021asymmetric} phát triển ASL cho multi-label classification:
\begin{itemize}
  \item Asymmetric focusing: $\gamma_- = 4$ (negative), $\gamma_+ = 1$ (positive).
  \item Hard negative mining với probability clipping.
  \item Đạt SOTA trên MS-COCO, NUS-WIDE.
\end{itemize}

\textbf{Kết quả trên NIH ChestX-ray14:} ASL đạt macro-AUC 0.763, cao nhất trong các loss functions (Chapter 8).

\subsection{\textbf{Class-Balanced Sampling và Re-weighting}}

\textbf{Cui et al. (2019)} \cite{cui2019classbalanced} đề xuất class-balanced loss:
\begin{equation}
\text{CB-Loss} = \frac{1 - \beta}{1 - \beta^{n_y}} \cdot \text{Loss}(y)
\end{equation}
với $n_y$ là số mẫu của lớp $y$, $\beta \in [0, 1)$.

% ============================================================
\section{\textbf{Khả năng Diễn Giải Mô Hình (Model Explainability)}}
\label{sec:rw_explainability}

\subsection{\textbf{Grad-CAM và Variants}}

\textbf{Selvaraju et al. (2017)} \cite{selvaraju2017gradcam} đề xuất Grad-CAM:
\begin{itemize}
  \item Sử dụng gradient của output class đối với feature maps cuối cùng.
  \item Tạo heatmap chỉ vùng quan trọng cho decision.
  \item Áp dụng rộng rãi trong y tế: CheXNet, DenseNet.
\end{itemize}

\textbf{Hạn chế:}
\begin{itemize}
  \item Chỉ highlight vùng có gradient cao, không phải causality.
  \item Có thể focus vào artifacts (markers, chest tubes) thay vì pathology.
\end{itemize}

\subsection{\textbf{Attention Maps trong Transformer}}

\textbf{ViT attention maps} cung cấp trực quan hóa tự nhiên:
\begin{itemize}
  \item Mỗi patch attend to các patches khác.
  \item CLS token attention cho biết vùng nào quan trọng cho classification.
  \item Transparent hơn Grad-CAM (built-in mechanism, không cần post-hoc).
\end{itemize}

\textbf{Cảnh báo:} \textit{Attention is not explanation} (Jain \& Wallace, 2019). Attention maps chỉ show mô hình nhìn vào đâu, không chứng minh causality.

\subsection{\textbf{Uncertainty Quantification}}

\textbf{Gal \& Ghahramani (2016)} \cite{gal2016dropout} đề xuất MC Dropout:
\begin{itemize}
  \item Dropout at test time để ước lượng epistemic uncertainty.
  \item Quan trọng trong y tế: model nên "biết khi nào không biết".
  \item Ứng dụng: Flag high-uncertainty cases cho bác sĩ review.
\end{itemize}

% ============================================================
\section{\textbf{Nghiên Cứu So Sánh Kiến Trúc}}
\label{sec:rw_comparison}

\subsection{\textbf{CNN vs Transformer trong Y Tế}}

\textbf{Matsoukas et al. (2021)} \cite{matsoukas2021medical} so sánh CNN và ViT trên 8 bộ dữ liệu y tế:
\begin{itemize}
  \item \textbf{Kết luận:} ResNet-50 pretrained outperforms ViT-Base từ scratch.
  \item ViT cần $>$10× dữ liệu hơn để match ResNet.
  \item Transfer learning là yếu tố quyết định, không phải architecture.
\end{itemize}

\subsection{\textbf{Bài Báo Gốc (Paper Under Analysis)}}

\textbf{Jain et al. (2024)} \cite{jain2024comparative} -- bài báo được phân tích trong luận văn này:
\begin{itemize}
  \item So sánh CNN, ResNet-34, ViT-v1, ViT-v2, ViT-ResNet.
  \item Dataset: NIH ChestX-ray14 (85,000 ảnh subset).
  \item Kết quả: ViT-ResNet pretrained đạt 93.9\% accuracy (best).
  \item Hạn chế: Không rõ patient-level split, thiếu per-class AUC.
\end{itemize}

% ============================================================
\section{\textbf{Gaps và Đóng Góp của Luận Văn Này}}
\label{sec:rw_gaps}

\subsection{\textbf{Gaps trong Literature}}

\begin{enumerate}
  \item \textbf{Thiếu replication studies nghiêm ngặt:} Nhiều paper báo cáo kết quả cao nhưng không reproducible do data leakage hoặc evaluation bias.
  \item \textbf{Thiếu phân tích architectural insights:} Ít công trình giải thích \textit{tại sao} một kiến trúc tốt hơn, chỉ báo cáo con số.
  \item \textbf{Thiếu đánh giá patient-level generalization:} Phần lớn split theo image, dẫn đến overfitting lên patient-specific characteristics.
  \item \textbf{Thiếu so sánh fair giữa CNN và Transformer:} Thường compare pretrained ViT với ResNet from scratch.
\end{enumerate}

\subsection{\textbf{Đóng Góp của Luận Văn}}

Luận văn này khắc phục các gaps trên bằng cách:
\begin{enumerate}
  \item \textbf{Replication study có kiểm soát:} Tái hiện bài báo gốc, chỉ ra sự khác biệt giữa kết quả báo cáo và kết quả thực tế.
  \item \textbf{Phân tích inductive bias:} Giải thích vai trò của locality, translation equivariance trong hiệu năng.
  \item \textbf{Patient-level evaluation:} Chia dữ liệu theo Patient ID, loại bỏ data leakage.
  \item \textbf{Per-class analysis đầy đủ:} Đánh giá AUC cho cả 15 classes, không chỉ macro-average.
  \item \textbf{Cải tiến hệ thống:} Transfer learning, Focal/ASL loss, Swin Transformer, Ensemble.
\end{enumerate}

% ============================================================
% CHAPTER 3: DATASET (was Chapter 2)
% ============================================================
\chapter{Phân tích Dataset: NIH Chest X-ray}

\section{\textbf{Tổng quan Dataset}}

\paperref{Section 4.1 - Dataset}

\subsection{\textbf{Thông tin cơ bản}}

\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=NIH Chest X-ray Dataset]
\begin{itemize}
  \item \textbf{Tên đầy đủ:} ChestX-ray14 (NIH Clinical Center)
  \item \textbf{Số lượng ảnh:} 112,120 frontal-view X-ray images
  \item \textbf{Số bệnh nhân:} 30,805 unique patients
  \item \textbf{Số nhãn bệnh:} 14 bệnh + 1 ``No Finding'' = 15 classes
  \item \textbf{Nguồn nhãn:} Text-mining từ radiology reports (weak labels)
  \item \textbf{Kích thước ảnh gốc:} 1024 × 1024 pixels
\end{itemize}
\end{tcolorbox}

\subsection{\textbf{Trích dẫn từ Paper}}

\begin{tcolorbox}[colback=yellow!5!white,colframe=yellow!75!black,title=Paper Quote - Dataset]
\textit{``To evaluate the performance of our model architectures, we utilized two freely available datasets: the NIH Chest X-ray dataset comprising 112,120 X-ray images with disease labels from 30,805 unique patients, and a Random Sample of the NIH Chest X-ray Dataset, containing 5,606 X-ray images. Both datasets involved multi-class classification across 15 classes, each representing different disease labels.''}
\end{tcolorbox}

\section{\textbf{Danh sách 15 Classes}}

\subsection{\textbf{Phân loại bệnh lý}}

\begin{table}[H]
\centering
\caption[15 Classes trong NIH Chest X-ray Dataset]{15 Classes trong NIH Chest X-ray Dataset}
\label{tab:15_classes}
\begin{tabular}{clcp{6cm}}
\toprule
\textbf{ID} & \textbf{Tên bệnh} & \textbf{Tỷ lệ (\%)} & \textbf{Mô tả} \\
\midrule
0 & Cardiomegaly & 2.48 & Tim to \\
1 & Emphysema & 2.24 & Khí phế thũng \\
2 & Effusion & 11.88 & Tràn dịch màng phổi \\
3 & Hernia & 0.20 & Thoát vị \\
4 & Nodule & 5.65 & Nốt phổi \\
5 & Pneumothorax & 4.73 & Tràn khí màng phổi \\
6 & Atelectasis & 10.31 & Xẹp phổi \\
7 & Pleural\_Thickening & 3.02 & Dày màng phổi \\
8 & Mass & 5.16 & Khối u \\
9 & Edema & 2.05 & Phù phổi \\
10 & Consolidation & 4.16 & Đông đặc phổi \\
11 & Infiltration & 17.74 & Thâm nhiễm \\
12 & Fibrosis & 1.50 & Xơ phổi \\
13 & Pneumonia & 1.28 & Viêm phổi \\
14 & No Finding & 53.84 & Không phát hiện bệnh \\
\bottomrule
\end{tabular}
\end{table}

\subsection{\textbf{Phân tích mất cân bằng lớp (Class Imbalance)}}

\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Vấn đề Class Imbalance]
\textbf{Quan sát quan trọng:}
\begin{itemize}
  \item ``No Finding'' chiếm \textbf{53.84\%} - hơn một nửa dataset
  \item ``Hernia'' chỉ chiếm \textbf{0.20\%} - rất hiếm
  \item Tỷ lệ cao nhất / thấp nhất = 53.84 / 0.20 = \textbf{269 lần}
\end{itemize}

\textbf{Hậu quả:}
\begin{itemize}
  \item Model có thể thiên về dự đoán ``No Finding''
  \item Accuracy cao nhưng chưa chắc đã detect tốt bệnh hiếm
  \item Cần metrics như AUC thay vì chỉ accuracy
\end{itemize}
\end{tcolorbox}

\section{\textbf{Bản chất Multi-label}}

\subsection{\textbf{Multi-label vs Multi-class}}
NIH ChestX-ray14 là bài toán \textbf{multi-label classification}, trong đó mỗi ảnh X-quang có thể đồng thời mang nhiều nhãn bệnh lý.

\begin{table}[H]
\centering
\caption[So sánh Multi-class và Multi-label]{So sánh Multi-class và Multi-label Classification}
\label{tab:multiclass_vs_multilabel}
\begin{tabular}{lp{5cm}p{5cm}}
\toprule
\textbf{Aspect} & \textbf{Multi-class} & \textbf{Multi-label} \\
\midrule
Số nhãn/sample & Chính xác 1 & Có thể nhiều (0, 1, 2, ...) \\
Output activation & Softmax & Sigmoid (independent) \\
Loss function & Categorical CE & Binary CE \\
Ví dụ & Cat OR Dog OR Bird & Cat AND Dog (có thể cả hai) \\
NIH Dataset & Không phù hợp & \checkmark Phù hợp \\
\bottomrule
\end{tabular}
\end{table}

\subsection{\textbf{Ví dụ Multi-label trong NIH}}

Một ảnh X-quang có thể có nhiều bệnh đồng thời:

\begin{lstlisting}[caption={Ví dụ multi-label trong dataset}]
# Image 00000001_000.png may have labels:
#labels = [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
#         ^     ^        ^
#         |     |        |
#         |     |        Atelectasis (index 6)
#         |     Effusion (index 2)
#         Cardiomegaly (index 0) - Not present
# Patient has: Effusion + Atelectasis (2 diseases simultaneously)
\end{lstlisting}

\section{\textbf{Data Pipeline trong Code}}

\coderef{data.ipynb}
Theo bài báo và mã nguồn, các bước tiền xử lý bao gồm:
\begin{itemize}
  \item Resize ảnh về kích thước 224×224.
  \item Chuẩn hoá pixel theo thống kê ImageNet.
  \item Chuyển ảnh grayscale sang RGB khi sử dụng mô hình pre-trained.
\end{itemize}

Các phép tăng cường dữ liệu cơ bản được sử dụng trong huấn luyện:
\begin{itemize}
  \item Random horizontal flip.
  \item Random rotation với góc nhỏ.
\end{itemize}

Cần lưu ý rằng phép lật ngang có thể ảnh hưởng đến các đặc trưng giải phẫu (ví dụ vị trí tim), do đó cần đánh giá cẩn thận ảnh hưởng của augmentation này.

\section{\textbf{Chia tập dữ liệu và nguy cơ rò rỉ dữ liệu}}

Trong bài báo, tác giả huấn luyện mô hình trên tập con khoảng 85.000 ảnh để tăng tốc hội tụ. Tuy nhiên, bài báo không nêu rõ việc chia dữ liệu theo bệnh nhân.

Nếu chia tập theo ảnh (image-level split), các ảnh của cùng một bệnh nhân có thể xuất hiện ở cả tập huấn luyện và tập kiểm tra, dẫn đến \textbf{data leakage} và đánh giá quá lạc quan.

Trong báo cáo này, việc chia tập theo \textbf{Patient ID} được khuyến nghị nhằm đảm bảo khả năng tổng quát hoá thực sự của mô hình.

\subsection{\textbf{Loading và Preprocessing}}

\begin{lstlisting}[caption={Data loading từ data.ipynb (PyTorch version)}]
class ChestXrayDataset(Dataset):
    def __init__(self, dataframe, images_path, labels, transform=None):
        self.dataframe = dataframe.reset_index(drop=True)
        self.images_path = images_path
        self.labels = labels
        self.transform = transform

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, idx):
        # Load image
        img_name = self.dataframe.iloc[idx]['Image Index']
        img_path = os.path.join(self.images_path, img_name)
        image = Image.open(img_path).convert('RGB')

        # Apply transforms
        if self.transform:
            image = self.transform(image)

        # Get labels as one-hot vector
        label = torch.tensor(
            self.dataframe.iloc[idx][self.labels].values.astype(float),
            dtype=torch.float32
        )

        return image, label
\end{lstlisting}

\subsection{\textbf{Data Augmentation}}

\paperref{Section 4.2 - Models}

\begin{tcolorbox}[colback=green!5!white,colframe=green!75!black,title=Augmentation từ Paper]
\textit{``We also performed various data augmentations on both datasets. For the Chest X-ray dataset, we applied resizing, random horizontal flip, and random rotation.''}
\end{tcolorbox}

\begin{lstlisting}[caption={Data augmentation transforms}]
train_transform = transforms.Compose([
    transforms.Resize((224, 224)),          # Resize to standard size
    transforms.RandomHorizontalFlip(p=0.5), # Random flip
    transforms.RandomRotation(degrees=5),   # Small rotation
    transforms.ColorJitter(brightness=0.1, contrast=0.1),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],  # ImageNet stats
        std=[0.229, 0.224, 0.225]
    )
])

val_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])
\end{lstlisting}

\subsection{\textbf{Lưu ý về Horizontal Flip trong X-ray}}

\begin{tcolorbox}[colback=orange!5!white,colframe=orange!75!black,title=Cảnh báo: Horizontal Flip]
Trong X-quang, flip ngang có thể gây vấn đề:
\begin{itemize}
  \item Tim thường nằm bên trái → flip làm tim nằm bên phải (Dextrocardia - bất thường)
  \item Một số bệnh có tính ``laterality'' (bên phải/trái khác nhau)
\end{itemize}

\textbf{Khuyến nghị:} Cần làm ablation study để đánh giá ảnh hưởng của flip.
\end{tcolorbox}

\section{\textbf{Data Split}}

\subsection{\textbf{Paper Description}}

\paperref{Section 4.1 - Dataset}

\begin{tcolorbox}[colback=yellow!5!white,colframe=yellow!75!black,title=Data Split từ Paper]
\textit{``However, our model training was conducted on a subset of 85,000 images from this Random Sample Dataset. We observed a faster convergence to optimal outputs within this subset.''}
\end{tcolorbox}

\subsection{\textbf{Implementation trong Code}}

\begin{lstlisting}[caption={Data split implementation}]
from sklearn.model_selection import train_test_split

# Standard split: 60% train, 20% val, 20% test
train_df, temp_df = train_test_split(
    full_df,
    test_size=0.4,
    random_state=42
)
val_df, test_df = train_test_split(
    temp_df,
    test_size=0.5,
    random_state=42
)
\end{lstlisting}

\subsection{\textbf{Vấn đề Data Leakage}}

\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Cảnh báo: Patient-level Split]
\textbf{Vấn đề:} Paper không đề cập rõ về patient-level split.

\textbf{Rủi ro:} Nếu split theo image (không theo patient):
\begin{itemize}
  \item Cùng một bệnh nhân có thể có nhiều ảnh
  \item Ảnh của cùng bệnh nhân có thể nằm ở cả train và test
  \item Model có thể ``nhớ'' bệnh nhân thay vì học features bệnh
  \item Kết quả đánh giá bị inflate (cao giả tạo)
\end{itemize}

\textbf{Khuyến nghị:} Split theo Patient ID để đảm bảo generalization thực sự.
\end{tcolorbox}

% ============================================================
% CHAPTER 3: CNN
% ============================================================
\chapter{Convolutional Neural Network (CNN)}
\label{chap:cnn}

\section{\textbf{Tổng quan lý thuyết CNN}}

Convolutional Neural Network (CNN) là kiến trúc học sâu được thiết kế chuyên biệt cho dữ liệu có cấu trúc dạng lưới (grid-like), đặc biệt là ảnh. CNN khai thác các \textbf{inductive bias} phù hợp với đặc trưng của dữ liệu hình ảnh, giúp mô hình học hiệu quả hơn so với các mạng fully-connected thuần tuý.

\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=Các inductive bias chính của CNN]
\begin{enumerate}
  \item \textbf{Locality:} Các pixel gần nhau có mối liên hệ mạnh hơn các pixel ở xa.
  \item \textbf{Translation equivariance:} Một mẫu (pattern) xuất hiện ở vị trí này cũng có thể được phát hiện ở vị trí khác.
  \item \textbf{Hierarchical feature learning:} Đặc trưng được học theo tầng bậc, từ cạnh (edges) → hình dạng (shapes) → cấu trúc phức tạp.
\end{enumerate}
\end{tcolorbox}

\section{\textbf{Phép tích chập (Convolution Operation)}}

Với ảnh đầu vào $X \in \mathbb{R}^{H \times W \times C_{\text{in}}}$ và kernel
$K \in \mathbb{R}^{k \times k \times C_{\text{in}} \times C_{\text{out}}}$,
đầu ra của lớp tích chập được tính như sau:

\begin{equation}
Y[i,j,c] = \sum_{m=0}^{k-1} \sum_{n=0}^{k-1} \sum_{c'=0}^{C_{\text{in}}-1}
X[i+m, j+n, c'] \cdot K[m,n,c',c] + b_c
\end{equation}

Số lượng tham số của một lớp convolution là:
\begin{equation}
\text{Params} = (k \times k \times C_{\text{in}} + 1) \times C_{\text{out}}
\end{equation}

\section{\textbf{Pooling Layer}}

Pooling được sử dụng để giảm kích thước không gian và tăng tính ổn định với các biến đổi nhỏ.

Với Max Pooling:
\begin{equation}
Y[i,j] = \max_{(m,n)\in R_{i,j}} X[m,n]
\end{equation}

\textbf{Vai trò của pooling}:
\begin{itemize}
  \item Giảm chi phí tính toán.
  \item Mở rộng receptive field.
  \item Tăng tính bất biến đối với dịch chuyển nhỏ.
\end{itemize}

\section{\textbf{Kiến trúc CNN trong bài báo}}

Theo mô tả trong bài báo gốc, mô hình CNN baseline bao gồm:
\begin{itemize}
  \item Hai lớp tích chập với lần lượt 32 và 64 bộ lọc kích thước $3 \times 3$.
  \item Mỗi lớp tích chập đi kèm ReLU và Max Pooling $2 \times 2$.
  \item Một lớp fully-connected 512 nút với Dropout $p=0.5$.
  \item Lớp đầu ra gồm 15 nút, tương ứng 15 nhãn bệnh.
\end{itemize}

\section{\textbf{Cài đặt CNN trong PyTorch}}

\coderef{cnn.ipynb}

\begin{lstlisting}[language=Python, caption={CNNClassifier theo bài báo (PyTorch)}]
class CNNClassifier(nn.Module):
    def __init__(self, num_classes=15):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2)
        )
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(64 * 56 * 56, 512),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(512, num_classes)
        )

    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x
\end{lstlisting}

\section{\textbf{Phân tích số lượng tham số}}

\begin{table}[H]
\centering
\caption[Tham số của CNN baseline]{Số lượng tham số của CNN baseline}
\label{tab:cnn_params}
\begin{tabular}{lcc}
\toprule
\textbf{Lớp} & \textbf{Số tham số} & \textbf{Tỷ lệ (\%)} \\
\midrule
Conv layers & 19,392 & 0.02 \\
Fully-connected layers & 102,768,655 & 99.98 \\
\midrule
\textbf{Tổng} & \textbf{102,788,047} & 100 \\
\bottomrule
\end{tabular}
\end{table}

\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Nhận xét quan trọng]
Hơn \textbf{99\% tham số} của CNN baseline nằm ở lớp fully-connected đầu tiên.
Điều này làm tăng nguy cơ overfitting và cho thấy hạn chế của kiến trúc CNN truyền thống.
\end{tcolorbox}

\section{\textbf{Hàm mất mát cho bài toán đa nhãn}}

Do mỗi ảnh có thể mang nhiều nhãn bệnh, bài toán được mô hình hoá dưới dạng \textbf{multi-label classification}. Vì vậy, hàm \texttt{BCEWithLogitsLoss} được sử dụng thay cho CrossEntropyLoss.

\begin{equation}
\mathcal{L} = - \frac{1}{N C} \sum_{i=1}^{N} \sum_{c=1}^{C}
\left[
y_{i,c} \log \sigma(z_{i,c}) +
(1-y_{i,c}) \log (1-\sigma(z_{i,c}))
\right]
\end{equation}

\section{\textbf{Kết quả thực nghiệm}}

Theo kết quả được báo cáo trong bài báo:
\begin{itemize}
  \item \textbf{Training Accuracy}: 91\%
  \item \textbf{Validation AUC}: 0.82
  \item \textbf{Test AUC}: 0.82
\end{itemize}

\section{\textbf{Phân tích kết quả và hạn chế}}

CNN là mô hình có hiệu năng thấp nhất trong số các kiến trúc được so sánh. Nguyên nhân chính bao gồm:
\begin{enumerate}
  \item \textbf{Receptive field hạn chế}: Chỉ gồm hai lớp tích chập.
  \item \textbf{Không có skip connection}: Dễ gặp vấn đề vanishing gradient khi mở rộng độ sâu.
  \item \textbf{Overfitting}: Số lượng tham số quá lớn tập trung ở fully-connected layer.
\end{enumerate}

\section{\textbf{Minh hoạ quá trình huấn luyện}}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.85\textwidth]{../../image/output_cnn.png}
  \caption[Kết quả huấn luyện CNN Baseline]{Diễn biến AUC trên tập huấn luyện và validation của mô hình CNN baseline. Mô hình đạt AUC cao trên tập huấn luyện nhưng không cải thiện trên tập validation, cho thấy hiện tượng overfitting rõ rệt.}
  \label{fig:cnn_auc}
\end{figure}

% ============================================================
% CHAPTER 4: RESNET
% ============================================================
\chapter{Residual Network (ResNet)}
\label{chap:resnet}

\section{\textbf{Động lực nghiên cứu: Degradation Problem}}

\subsection{\textbf{Vấn đề khi huấn luyện mạng rất sâu}}

\paperref{Section 3.2 - ResNet}

\begin{tcolorbox}[colback=yellow!5!white,colframe=yellow!75!black,
title=Paper Quote -- Degradation Problem]
\textit{``However, when we stack many layers to create an extremely deep network,
we discover that the training accuracy begins to saturate and then drops off quickly.
These observations lead to the understanding of what is commonly referred to as the
degradation problem.''}
\end{tcolorbox}

Trong các mạng CNN truyền thống, việc tăng số lượng lớp không luôn dẫn đến cải thiện
hiệu năng. Trái lại, khi độ sâu vượt quá một ngưỡng nhất định, mô hình gặp hiện tượng
\textbf{degradation}: training error và validation error đều tăng.

\subsection{\textbf{Bản chất của degradation}}

\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Key Insight]
Degradation \textbf{không phải} do overfitting.

\textbf{Nguyên nhân cốt lõi:}
\begin{itemize}
  \item \textbf{Khó tối ưu (optimization difficulty)} trong không gian tham số rất lớn.
  \item \textbf{Identity mapping problem}: khi thêm lớp mới, mạng ít nhất phải học ánh xạ
  $y = x$, nhưng điều này rất khó với các lớp phi tuyến.
\end{itemize}

\textbf{Nghịch lý:} Về mặt lý thuyết, thêm lớp không thể làm mô hình kém hơn
(worst case là học identity), nhưng thực nghiệm cho thấy điều ngược lại.
\end{tcolorbox}

\section{\textbf{Residual Learning Framework}}

\subsection{\textbf{Ý tưởng cốt lõi của ResNet}}

\paperref{Section 3.2 - ResNet}

\begin{tcolorbox}[colback=yellow!5!white,colframe=yellow!75!black,
title=Paper Quote -- Skip Connections]
\textit{``To solve the degradation problem, ResNet uses an architecture with residual
learning framework called skip connections.''}
\end{tcolorbox}

Thay vì học trực tiếp ánh xạ $H(x)$, ResNet học \textbf{phần dư} (residual):
\[
F(x) = H(x) - x.
\]

\subsection{\textbf{Mô hình toán học}}

\textbf{Plain network:}
\[
y = F(x)
\]

\textbf{Residual network:}
\[
y = F(x) + x
\]

Nếu $F(x) = 0$ thì block trở thành ánh xạ đồng nhất $y = x$.

\subsection{\textbf{Phân tích dòng gradient}}

\[
\frac{\partial \mathcal{L}}{\partial x}
= \frac{\partial \mathcal{L}}{\partial y}
\left(1 + \frac{\partial F(x)}{\partial x}\right)
\]

\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=Key Observation]
Gradient luôn có thành phần ``1'' đi trực tiếp qua skip connection.
Ngay cả khi $\partial F(x)/\partial x$ nhỏ, gradient vẫn không bị triệt tiêu.
\end{tcolorbox}

\textbf{Hệ quả:}
\begin{itemize}
  \item Giảm vanishing gradient.
  \item Huấn luyện mạng rất sâu ổn định hơn.
  \item Có thể xem ResNet như một ensemble ngầm của nhiều mạng nông.
\end{itemize}

\section{\textbf{Kiến trúc ResNet-34}}

\paperref{Section 3.2 - ResNet}

\subsection{\textbf{Mô tả từ bài báo}}

\begin{tcolorbox}[colback=yellow!5!white,colframe=yellow!75!black,
title=Paper Quote -- ResNet Architecture]
\textit{``There are 34 weighted layers in this network, and the input image is
224 × 224.''}
\end{tcolorbox}

\subsection{\textbf{Cấu trúc chi tiết}}

\begin{table}[H]
\centering
\caption[Kiến trúc ResNet-34]{Kiến trúc ResNet-34}
\label{tab:resnet34_arch}
\begin{tabular}{lcccc}
\toprule
\textbf{Stage} & \textbf{Output size} & \textbf{Channels} & \textbf{Blocks} & \textbf{Stride} \\
\midrule
Conv1 & $112 \times 112$ & 64 & -- & 2 \\
MaxPool & $56 \times 56$ & 64 & -- & 2 \\
Layer1 & $56 \times 56$ & 64 & 3 & 1 \\
Layer2 & $28 \times 28$ & 128 & 4 & 2 \\
Layer3 & $14 \times 14$ & 256 & 6 & 2 \\
Layer4 & $7 \times 7$ & 512 & 3 & 2 \\
AvgPool & $1 \times 1$ & 512 & -- & -- \\
FC & -- & 15 & -- & -- \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Tổng số tham số:} $\sim$21.8 triệu (ít hơn rất nhiều so với CNN baseline $\sim$95M).

\section{\textbf{Cài đặt ResNet-34}}

\coderef{resnet.ipynb}

\subsection{\textbf{BasicBlock}}

\begin{lstlisting}[language=Python, caption={BasicBlock trong ResNet-34}]
class BasicBlock(nn.Module):
    expansion = 1
    def __init__(self, in_channels, out_channels, stride=1, downsample=None):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels,
                               kernel_size=3, stride=stride, padding=0, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(out_channels, out_channels,
                               kernel_size=3, padding=0, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.downsample = downsample

    def forward(self, x):
        identity = x
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        if self.downsample is not None:
            identity = self.downsample(x)
        out += identity
        return self.relu(out)
\end{lstlisting}

\section{\textbf{Cấu hình huấn luyện}}

\begin{table}[H]
\centering
\caption[Cấu hình huấn luyện ResNet-34]{Cấu hình huấn luyện ResNet-34}
\label{tab:resnet_config}
\begin{tabular}{ll}
\toprule
Tham số & Giá trị \\
\midrule
Kích thước ảnh & $224 \times 224$ \\
Batch size & 32 \\
Optimizer & AdamW \\
Learning rate & $1 \times 10^{-4}$ \\
Weight decay & $1 \times 10^{-6}$ \\
Epoch & 10 \\
Loss function & BCEWithLogitsLoss \\
Initialization & Kaiming Normal \\
\bottomrule
\end{tabular}
\end{table}

\section{\textbf{Kết quả thực nghiệm}}

\subsection{\textbf{Diễn biến AUC theo epoch}}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{../../image/output-resnet.png}
\caption[Kết quả huấn luyện ResNet-34]{Diễn biến AUC huấn luyện và validation của ResNet-34. Mô hình sử dụng skip connections giúp duy trì gradient flow và đạt hiệu năng ổn định hơn so với CNN baseline.}
\label{fig:resnet_auc}
\end{figure}

\subsection{\textbf{Bảng kết quả}}

\begin{table}[H]
\centering
\caption[Kết quả huấn luyện ResNet-34]{Kết quả huấn luyện ResNet-34}
\label{tab:resnet_results}
\begin{tabular}{lccc}
\toprule
Chỉ số & Train & Validation & Test \\
\midrule
 & 73.42  & 44.62 & 44.62 \\

\bottomrule
\end{tabular}
\end{table}

\subsection{\textbf{Phân tích kết quả}}

\begin{tcolorbox}[colback=green!5!white,colframe=green!75!black,
title=Analysis -- Vì sao ResNet-34 chưa đạt hiệu năng cao?]
\begin{itemize}
  \item \textbf{Huấn luyện từ đầu}: Không dùng pretrained ImageNet.
  \item \textbf{Dữ liệu hiệu dụng nhỏ}: Một phần thí nghiệm dùng sample nhỏ.
  \item \textbf{Overfitting}: Train AUC cao (0.73) nhưng Val AUC thấp (0.451).
\end{itemize}
\end{tcolorbox}

\textbf{So sánh với CNN baseline:}
\begin{itemize}
  \item ResNet-34 có ít tham số hơn nhiều (21M vs 95M).
  \item Tuy nhiên, CNN đơn giản lại cho Val AUC cao hơn khi dữ liệu hạn chế.
  \item Điều này nhấn mạnh vai trò then chốt của \textbf{transfer learning}
  đối với các mô hình sâu.
\end{itemize}

\textbf{Kết luận:} ResNet-34 có kiến trúc vượt trội về mặt lý thuyết, nhưng để phát huy
hết sức mạnh trong bài toán X-quang ngực, cần sử dụng pretrained weights và dữ liệu đủ lớn.


% ============================================================
% CHAPTER 5: VISION TRANSFORMER (ViT)
% ============================================================
% ============================================================
% CHAPTER 6: VISION TRANSFORMER (ViT)
% Deep Expert Analysis - Full Detailed Chapter (Ready for Thesis)
% ============================================================
\chapter{Vision Transformer (ViT)}
\label{chap:vit_full}

% ============================================================
% SECTION 1: MOTIVATION
% ============================================================
\section{\textbf{Động lực: Từ Xử lý Ngôn ngữ sang Thị giác máy tính}}

\subsection{\textbf{Cuộc cách mạng của Transformer}}

\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=Bối cảnh lịch sử]
\textbf{2017:} ``Attention Is All You Need'' (Vaswani et al.)
\begin{itemize}
    \item Transformer thay thế RNN trong xử lý ngôn ngữ tự nhiên, giải quyết vấn đề xử lý tuần tự.
    \item Self-attention bắt được các mối liên hệ dài hạn tốt hơn RNN hoặc CNN một chiều.
    \item Có thể tính toán song song → huấn luyện nhanh hơn rất nhiều trên GPU/TPU.
\end{itemize}

\textbf{2020:} ``An Image is Worth 16×16 Words'' (Dosovitskiy et al.)
\begin{itemize}
    \item Áp dụng Transformer vào thị giác máy tính: Vision Transformer (ViT).
    \item Cách tiếp cận: chia ảnh thành các mảnh nhỏ (patches) rồi xem chúng như chuỗi từ (tokens).
    \item Khi được huấn luyện trước trên dữ liệu khổng lồ, ViT đạt kết quả tốt nhất (SOTA) trên ImageNet.
\end{itemize}
\end{tcolorbox}

\subsection{\textbf{Paper Motivation}}

\paperref{Section 3.3 - Vision Transformer}

\begin{tcolorbox}[colback=yellow!5!white,colframe=yellow!75!black,title=Paper Quote - ViT Motivation]
\textit{``Vision Transformers (ViTs) [1] utilize transformer architecture to advance image classification,
processing 224 × 224 pixel images 2a into (32 × 32) 2b patches considered as tokens, which are then
projected into a higher dimension with positional embedding. Our vision transformer models include
multiple transformer blocks with layer normalization to stabilize input features, multi-head attention
for segment focus, skip connections to aid gradient flow, a multi-layer perceptron (MLP) with GELU
activation sequence to enhance learning, and a final layer normalization before dropout-enhanced
MLP outputs through a sigmoid activation layer for multi-label classification, effectively leveraging
the transformer’s strengths for complex spatial hierarchies in medical imaging.- Vision Transformer (ViT) [1] sử dụng kiến ​​trúc transformer để nâng cao phân loại hình ảnh,
xử lý hình ảnh 224 × 224 pixel 2a thành các mảng (32 × 32) 2b được coi là các token, sau đó được
chiếu vào chiều cao hơn với nhúng vị trí. Các mô hình vision transformer của chúng tôi bao gồm
nhiều khối transformer với chuẩn hóa lớp để ổn định các đặc trưng đầu vào, cơ chế chú ý đa đầu
để tập trung vào phân đoạn, các kết nối bỏ qua để hỗ trợ luồng gradient, một perceptron đa lớp (MLP) với chuỗi kích hoạt GELU
để tăng cường khả năng học tập và chuẩn hóa lớp cuối cùng trước khi xuất ra MLP được tăng cường dropout thông qua lớp kích hoạt sigmoid để phân loại đa nhãn, tận dụng hiệu quả
thế mạnh của transformer cho các hệ thống phân cấp không gian phức tạp trong hình ảnh y tế.''}
\end{tcolorbox}

\subsection{\textbf{So sánh CNN và Transformer: Thiên lệch quy nạp}}

CNN được thiết kế với các \textbf{inductive bias} phù hợp với đặc điểm của ảnh (tính cục bộ, tính bất biến w/dịch chuyển).
ViT thì không có nhiều thiên lệch như vậy, mà để mô hình tự học cấu trúc ảnh qua cơ chế attention:
\begin{itemize}
    \item \textbf{Ưu điểm}: học được các quan hệ toàn cục tốt hơn, mở rộng tốt khi dữ liệu lớn.
    \item \textbf{Nhược điểm}: cần nhiều dữ liệu hơn, dễ thua CNN nếu bắt đầu huấn luyện từ đầu với dữ liệu ít.
\end{itemize}

\begin{table}[H]
\centering
\caption[Inductive Biases: CNN vs ViT]{Inductive Biases: CNN vs ViT}
\label{tab:inductive_biases}
\begin{tabular}{lcc}
\toprule
\textbf{Inductive Bias} & \textbf{CNN} & \textbf{ViT} \\
\midrule
Locality & \checkmark Strong & \texttimes Weak \\
Translation Equivariance & \checkmark Built-in & \texttimes Learned \\
2D Structure & \checkmark Preserved & \texttimes Flattened \\
Long-range Dependencies & \texttimes Limited & \checkmark Global \\
Data Efficiency & \checkmark Good & \texttimes Needs more data \\
Scalability & Limited & \checkmark Scales well \\
\bottomrule
\end{tabular}
\end{table}

% ============================================================
% SECTION 2: ARCHITECTURE OVERVIEW
% ============================================================
\section{\textbf{Tổng quan kiến trúc ViT}}

Vision Transformer chuyển bài toán ảnh thành bài toán chuỗi:
\begin{enumerate}
    \item Chia ảnh thành patches (tokens).
    \item Ánh xạ mỗi patch sang vector embedding.
    \item Cộng positional embedding để giữ thông tin vị trí.
    \item Thêm token đặc biệt \texttt{[CLS]} để đại diện toàn ảnh.
    \item Đưa chuỗi token qua $L$ Transformer Encoder blocks.
    \item Dùng \texttt{[CLS]} output cho classification head.
\end{enumerate}

\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=0.7, every node/.style={font=\small}]
    \node[draw, fill=blue!20, minimum width=2cm, minimum height=2cm, text width=1.8cm, align=center] (img) at (0,0) {Image 224×224};
    \node[draw, fill=orange!30, minimum width=2cm, minimum height=1.5cm, text width=1.8cm, align=center] (patch) at (3.5,0) {Split into Patches};
    \node[draw, fill=green!30, minimum width=2cm, minimum height=1.5cm, text width=1.8cm, align=center] (proj) at (7,0) {Linear Projection};
    \node[draw, fill=purple!30, minimum width=2cm, minimum height=1cm, text width=1.8cm, align=center] (pos) at (7,-2.5) {Position Embedding};
    \node[draw, fill=red!30, minimum width=1.5cm, minimum height=1cm, text width=1.3cm, align=center] (cls) at (3.5,-2.5) {[CLS] Token};
    \node[draw, circle, fill=yellow!30] (add) at (10.5,0) {+};
    \node[draw, fill=cyan!30, minimum width=2.5cm, minimum height=2cm, text width=2.3cm, align=center] (trans) at (14,0) {Transformer Encoder ×L};
    \node[draw, fill=orange!30, minimum width=2cm, minimum height=1.5cm, text width=1.8cm, align=center] (mlp) at (17.5,0) {MLP Head};
    \node[draw, fill=blue!20, minimum width=2cm, minimum height=1cm] (out) at (17.5,-2.5) {15 Classes};

    \draw[->] (img) -- (patch);
    \draw[->] (patch) -- (proj);
    \draw[->] (proj) -- (add);
    \draw[->] (pos) -- (add);
    \draw[->] (cls) -- ++(4,0) -- (add);
    \draw[->] (add) -- (trans);
    \draw[->] (trans) -- (mlp);
    \draw[->] (mlp) -- (out);
\end{tikzpicture}
\caption[Kiến trúc Vision Transformer tổng quan]{Vision Transformer Architecture Overview: Ảnh đầu vào được chia thành các patches, sau đó được linear projection và kết hợp với position embedding. Token [CLS] được thêm vào để tổng hợp thông tin. Toàn bộ chuỗi tokens đi qua L Transformer Encoder blocks, cuối cùng MLP Head dự đoán 15 lớp bệnh lý.}
\label{fig:vit_architecture}
\end{figure}

% ============================================================
% SECTION 3: PATCH EMBEDDING
% ============================================================
\section{\textbf{Step 1: Patch Embedding}}

\subsection{\textbf{Concept}}

\paperref{Section 3.3 - Vision Transformer}

\begin{tcolorbox}[colback=yellow!5!white,colframe=yellow!75!black,title=Paper Quote - Patch Embedding]
\textit{``Vision Transformers (ViTs) [1] utilize transformer architecture to advance image classification,
processing 224 × 224 pixel images 2a into (32 × 32) 2b patches considered as tokens, which are then
projected into a higher dimension with positional embedding.''}
\end{tcolorbox}

\subsection{\textbf{Mathematical Formulation}}

Với ảnh $x \in \mathbb{R}^{H \times W \times C}$ và patch size $P$:

\textbf{Số patches:}
\begin{equation}
N = \frac{H \times W}{P^2}
\end{equation}

Với $H=W=224$ và $P=32$:
\begin{equation}
N = \left(\frac{224}{32}\right)^2 = 7^2 = 49
\end{equation}

\textbf{Flatten patch:}
\begin{equation}
x_p^i \in \mathbb{R}^{P^2 \cdot C} = \mathbb{R}^{32\cdot 32\cdot 3} = \mathbb{R}^{3072}
\end{equation}

\textbf{Linear projection sang embedding dim $D$:}
\begin{equation}
z_0^i = x_p^i E + b, \quad E \in \mathbb{R}^{(P^2C) \times D}
\end{equation}

\subsection{\textbf{Implementation (PyTorch)}}

\begin{lstlisting}[language=Python, caption={PatchEmbedding (Conv2d = Linear Projection)}]
class PatchEmbedding(nn.Module):
    def __init__(self, img_size=224, patch_size=32, in_channels=3, embed_dim=64):
        super().__init__()
        self.num_patches = (img_size // patch_size) ** 2  # 49 patches
        self.projection = nn.Conv2d(
            in_channels=in_channels,
            out_channels=embed_dim,
            kernel_size=patch_size,
            stride=patch_size
        )

    def forward(self, x):
        x = self.projection(x)   # (B, 64, 7, 7)
        x = x.flatten(2)         # (B, 64, 49)
        x = x.transpose(1, 2)    # (B, 49, 64)
        return x
\end{lstlisting}

\subsection{\textbf{Conv2d tương đương Linear Projection}}

Conv2d với kernel=stride=$P$ thực hiện:
\begin{itemize}
    \item Extract patches không overlap.
    \item Áp cùng weight matrix cho mọi patch (giống linear layer dùng chung).
    \item Hiệu quả hơn unfold + linear về tốc độ và memory.
\end{itemize}

% ============================================================
% SECTION 4: POSITIONAL EMBEDDING
% ============================================================
\section{\textbf{Step 2: Positional Embedding}}

\subsection{\textbf{Why Position Matters?}}

\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Problem: Self-Attention is Permutation Invariant]
Self-attention không biết thứ tự token nếu không có positional encoding.
Nếu hoán vị patches, output hoán vị tương ứng.
Do đó ViT cần positional embedding để giữ thông tin spatial.
\end{tcolorbox}

\subsection{\textbf{Learnable Position Embedding}}

ViT dùng learnable embedding:
\begin{equation}
z_i' = z_i + E_{pos}[i], \quad E_{pos} \in \mathbb{R}^{(N+1)\times D}
\end{equation}

\begin{lstlisting}[language=Python, caption={Learnable positional embedding}]
self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim) * 0.02)
\end{lstlisting}

% ============================================================
% SECTION 5: CLS TOKEN
% ============================================================
\section{\textbf{Step 3: [CLS] Token}}

CLS token là token học được, prepend vào chuỗi patch embeddings.
Sau Transformer, CLS token trở thành embedding toàn cục cho ảnh.

\begin{lstlisting}[language=Python, caption={CLS token in ViT}]
self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
cls_tokens = self.cls_token.expand(B, -1, -1)
x = torch.cat([cls_tokens, x], dim=1)  # (B, N+1, D)
\end{lstlisting}

% ============================================================
% SECTION 6: TRANSFORMER ENCODER
% ============================================================
\section{\textbf{Step 4: Transformer Encoder}}
Transformer Encoder là khối lặp nhiều lần để học quan hệ giữa các patch và tạo đặc trưng toàn cục cho ảnh.
\subsection{\textbf{Scaled Dot-Product Attention}}
Transformer Encoder là khối lặp nhiều lần để học quan hệ giữa các patch và tạo đặc trưng toàn cục cho ảnh.
\begin{equation}
\mathrm{Attention}(Q,K,V) = \mathrm{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}

\subsection{\textbf{Multi-Head Self-Attention}}
Multi-Head chạy nhiều attention song song trên các không gian khác nhau rồi ghép lại để bắt nhiều kiểu quan hệ cùng lúc.
\begin{equation}
\mathrm{MultiHead}(Q,K,V)=\mathrm{Concat}(head_1,\dots,head_h)W^O
\end{equation}

\begin{equation}
head_i = \mathrm{Attention}(QW_i^Q, KW_i^K, VW_i^V)
\end{equation}

\subsection{\textbf{Transformer Encoder Block (Pre-LN)}}
Pre-LN nghĩa là chuẩn hoá (LayerNorm) trước attention/MLP và dùng residual để giúp train ổn định hơn.
\begin{align}
z'_l &= z_{l-1} + \mathrm{MHSA}(\mathrm{LN}(z_{l-1}))\\
z_l &= z'_l + \mathrm{MLP}(\mathrm{LN}(z'_l))
\end{align}

\begin{lstlisting}[language=Python, caption={TransformerEncoderBlock (Pre-LN)}]
class TransformerEncoderBlock(nn.Module):
    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, dropout=0.1):
        super().__init__()
        self.norm1 = nn.LayerNorm(embed_dim) 
        self.norm2 = nn.LayerNorm(embed_dim)

        self.attn = nn.MultiheadAttention(
            embed_dim=embed_dim,
            num_heads=num_heads,
            dropout=dropout,
            batch_first=True
        )

        hidden = int(embed_dim * mlp_ratio)
        self.mlp = nn.Sequential(
            nn.Linear(embed_dim, hidden),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden, embed_dim),
            nn.Dropout(dropout)
        )

    def forward(self, x):
        x_norm = self.norm1(x)
        attn_out, _ = self.attn(x_norm, x_norm, x_norm)
        x = x + attn_out
        x = x + self.mlp(self.norm2(x))
        return x

#   norm1, norm2: chuẩn hoá input trước MHSA và trước MLP.

#nn.MultiheadAttention: tính self-attention với Q=K=V=x_norm.

#mlp: mạng 2 Linear + GELU + Dropout để tăng khả năng biểu diễn phi tuyến.

#x = x + attn_out: residual connection sau attention.

#x = x + mlp(...): residual connection sau MLP.
\end{lstlisting}

% ============================================================
% SECTION 7: FULL ViT MODEL
% ============================================================
\section{\textbf{Complete Vision Transformer Implementation}}

\begin{lstlisting}[language=Python, caption={Complete ViT Model}]
class VisionTransformer(nn.Module):
    def __init__(self, img_size=224, patch_size=32, in_channels=3,
                 num_classes=15, embed_dim=64, depth=8, num_heads=4,
                 mlp_ratio=4.0, dropout=0.1):
        super().__init__()

        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)
        num_patches = self.patch_embed.num_patches

        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim) * 0.02)
        self.pos_dropout = nn.Dropout(dropout)

        self.blocks = nn.Sequential(*[
            TransformerEncoderBlock(embed_dim, num_heads, mlp_ratio, dropout)
            for _ in range(depth)
        ])

        self.norm = nn.LayerNorm(embed_dim)
        self.head = nn.Linear(embed_dim, num_classes)

    def forward(self, x):
        B = x.size(0)
        x = self.patch_embed(x)                 # (B, N, D)

        cls = self.cls_token.expand(B, -1, -1)  # (B, 1, D)
        x = torch.cat([cls, x], dim=1)          # (B, N+1, D)

        x = x + self.pos_embedding
        x = self.pos_dropout(x)

        x = self.blocks(x)
        x = self.norm(x)

        cls_out = x[:, 0]
        return self.head(cls_out)
\end{lstlisting}

% ============================================================
% SECTION 8: LOSS, IMBALANCE, COMPUTE COMPLEXITY
% ============================================================
\section{\textbf{Loss Function, Class Imbalance và Computational Complexity}}

\subsection{\textbf{Multi-label BCEWithLogitsLoss}}
BCEWithLogitsLoss tính loss độc lập cho từng nhãn (sigmoid từng lớp) nên phù hợp bài toán multi-label.
\begin{equation}
\mathcal{L}_{BCE} = - \frac{1}{C}\sum_{c=1}^{C}
\left[y_c \log\sigma(\hat{y}_c) + (1-y_c)\log(1-\sigma(\hat{y}_c))\right]
\end{equation}

\subsection{\textbf{Class Imbalance Effects}}

Trong NIH Chest X-ray14, các lớp bệnh hiếm làm mô hình dễ bias về No Finding.
Do đó metric chính nên là AUC thay vì accuracy.

\subsection{\textbf{Attention Complexity}}

Self-attention có độ phức tạp:
\begin{equation}
\mathcal{O}(N^2 \cdot D)
\end{equation}

Với $N=49$ (patch 32) $\Rightarrow$ $N^2=2401$,
còn $N=196$ (patch 16) $\Rightarrow$ $N^2=38416$ (lớn hơn 16 lần).

% ============================================================
% SECTION 9: ViT-v1 vs ViT-v2 vs PRETRAINED
% ============================================================
\section{\textbf{So sánh ViT-v1, ViT-v2 và ViT Pretrained}}

\begin{table}[H]
\centering
\caption[So sánh các biến thể ViT]{So sánh các biến thể ViT}
\label{tab:vit_variants}
\begin{tabular}{lccc}
\toprule
\textbf{Thuộc tính} & \textbf{ViT-v1} & \textbf{ViT-v2} & \textbf{ViT Pretrained} \\
\midrule
Patch size & 32 & 32 & 16 \\
Num patches & 49 & 49 & 196 \\
Embed dim & 64 & 64 & 768 \\
Depth & 8 & 8 & 12 \\
Heads & 4 & 4 & 12 \\
Params & $\sim$9M & $\sim$9M & $\sim$86M \\
Pretrained & Không & Không & ImageNet-21K \\
\bottomrule
\end{tabular}
\end{table}

% ============================================================
% SECTION 10: RESULTS + ROC FIGURES
% ============================================================
\section{\textbf{Results Analysis và ROC Curves}}

\subsection{\textbf{ROC Curves}}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../../image/output-roc-vit-v1.png}
        \caption{ViT-v1 (huấn luyện từ đầu)}
        \label{fig:roc_vit_v1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../../image/output-roc-vit-v2.png}
        \caption{ViT-v2 (có regularization)}
        \label{fig:roc_vit_v2}
    \end{subfigure}
    \caption[So sánh ROC Curve giữa ViT-v1 và ViT-v2]{So sánh đường cong ROC-AUC giữa hai biến thể Vision Transformer: (a) ViT-v1 huấn luyện từ đầu, (b) ViT-v2 với regularization (SGD+momentum+scheduler). ViT-v2 đạt kết quả test tốt hơn nhờ regularization hiệu quả.}
    \label{fig:roc_vit_comparison}
\end{figure}

\subsection{\textbf{Discussion}}

\begin{itemize}
    \item ViT-v1 đạt AUC validation tốt nhưng test thấp $\Rightarrow$ overfitting.
    \item ViT-v2 có regularization tốt hơn (SGD+momentum+scheduler) nên test AUC cao hơn.
    \item ViT pretrained cho kết quả tốt nhất vì transfer learning cung cấp feature space mạnh.
\end{itemize}


% ============================================================
% SECTION 11: KEY TAKEAWAYS
% ============================================================
\section{\textbf{Key Takeaways}}

\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=Summary]
\begin{enumerate}
    \item ViT chuyển ảnh thành chuỗi patches và học global dependencies bằng attention.
    \item ViT cần positional embedding vì attention không giữ thông tin vị trí.
    \item Pure ViT kém data-efficient, không phù hợp train from scratch với dữ liệu nhỏ.
    \item Transfer learning là yếu tố quyết định hiệu năng trong bài toán y khoa.
\end{enumerate}
\end{tcolorbox}

% ============================================================
% CHAPTER 6: THỰC NGHIỆM VÀ KẾT QUẢ
% ============================================================
\chapter{Thực nghiệm và Kết quả (Experiments and Results)}
\label{chap:experiments}

\paperref{Section 4--6}

% ============================================================
% 6.1 Thiết lập thực nghiệm 
% ============================================================
\section{\textbf{Thiết lập thực nghiệm (Experimental Setup)}}

\subsection{\textbf{Bộ dữ liệu và Chiến lược chia dữ liệu (Dataset and Data Splitting)}}

Tất cả các thí nghiệm được thực hiện trên bộ dữ liệu \textbf{NIH ChestX-ray14}, gồm 112,120 ảnh X-quang ngực với 14 bệnh lý và nhãn \textit{No Finding}. Đây là bài toán \textbf{multi-label classification}, trong đó mỗi ảnh có thể mang nhiều nhãn bệnh đồng thời.

\textbf{Chiến lược chia dữ liệu (quan trọng):}
\begin{itemize}[noitemsep]
    \item Dữ liệu được chia theo \textbf{ID bệnh nhân}, không chia theo ảnh.
    \item Mục tiêu: tránh rò rỉ dữ liệu khi một bệnh nhân có nhiều ảnh.
    \item Tỷ lệ chia: 60\% train – 20\% test, 20\% validation.
\end{itemize}

Chiến lược này nghiêm ngặt hơn so với bài báo gốc và phản ánh đúng hơn khả năng tổng quát hóa trên bệnh nhân mới.

\subsection{\textbf{Training Configuration}}

\begin{table}[H]
\centering
\caption[Training hyperparameters theo bài báo và triển khai lại]{Training hyperparameters (theo bài báo và triển khai lại)}
\label{tab:training_config}
\begin{tabular}{lccccc}
\toprule
\textbf{Parameter} & \textbf{CNN} & \textbf{ResNet} & \textbf{ViT-v1} & \textbf{ViT-v2} & \textbf{ViT-ResNet} \\
\midrule
Learning Rate & 0.0001 & 0.0001 & 0.0001 & 0.0001 & 0.0001 \\
Batch Size & 32 & 32 & 32 & 32 & 16 \\
Epochs & 10 & 10 & 10 & 10 & 10 \\
Optimizer & AdamW & AdamW & AdamW & AdamW & AdamW \\
Loss Function & BCEWithLogitsLoss & BCEWithLogitsLoss & BCEWithLogitsLoss & BCEWithLogitsLoss & BCEWithLogitsLoss \\
Dropout & - & - & 0.1 & 0.1 & 0.1 \\
Weight Decay & 1e-6 & 1e-6 & 1e-6 & 1e-5 & 1e-5 \\
\bottomrule
\end{tabular}
\end{table}

Tất cả các mô hình sử dụng:
\begin{itemize}[noitemsep]
    \item Sigmoid activation tại đầu ra.
    \item Fixed random seed để đảm bảo reproducibility.
    \item Evaluation theo macro-AUC và per-class AUC.
\end{itemize}

% ============================================================
% 6.2 Chỉ số đánh giá
% ============================================================
\section{\textbf{Chỉ số đánh giá (Evaluation Metrics)}}

\subsection{\textbf{AUC-ROC}}

Chỉ số chính được sử dụng là \textbf{Diện tích dưới đường cong ROC (AUC)} do các đặc thù sau:
\begin{itemize}[noitemsep]
    \item Phù hợp với bài toán phân loại đa nhãn. Bài toán multi-lable classification yêu cầu đánh giá riêng biệt cho từng lớp bệnh.
    \item Phản ánh đúng hiệu năng khi dữ liệu bị mất cân bằng mạnh giữa các lớp bệnh.
\end{itemize}

\begin{equation}
\text{AUC} = \int_0^1 \text{TPR}(\text{FPR}) \, d(\text{FPR})
\end{equation}

\begin{tcolorbox}[colback=green!5!white,colframe=green!75!black,title=Giải thích ý nghĩa AUC (AUC Interpretation)]
\begin{itemize}
    \item \textbf{0.5}: Dự đoán ngẫu nhiên (Random guessing)
    \item \textbf{0.7--0.8}: Chấp nhận được (Acceptable)
    \item \textbf{0.8--0.9}: Tốt (Good)
    \item \textbf{$>$0.9}: Xuất sắc (Excellent)
\end{itemize}
\end{tcolorbox}

\textbf{Macro-AUC} được tính toán trên các lớp bệnh có đầy đủ nhãn 0 và 1 để tránh lỗi giá trị không xác định (NaN).

% ============================================================
% 6.3 Kết quả chính theo bài báo
% ============================================================
\section{\textbf{Kết quả chính (Báo cáo trong bài báo gốc)}}

\begin{table}[H]
\centering
\caption[So sánh hiệu năng theo kết quả bài báo gốc]{So sánh hiệu năng – Các kết quả báo cáo trong bài báo gốc (Reported in the Original Paper)}
\label{tab:paper_results}
\begin{tabular}{lcccccc}
\toprule
\textbf{Mô hình} & \textbf{Train Acc} & \textbf{Train AUC} & \textbf{Val Acc} & \textbf{Val AUC} & \textbf{Test Acc} & \textbf{Test AUC} \\
\midrule
CNN & 91.0\% & 0.82 & -- & 0.82 & -- & 0.82 \\
ResNet-34 & 93.0\% & 0.90 & -- & 0.86 & -- & 0.86 \\
ViT-v1/32 & 92.63\% & 0.88 & -- & 0.86 & -- & 0.86 \\
ViT-v2/32 & 92.83\% & 0.90 & -- & 0.84 & -- & 0.84 \\
ViT-ResNet/16 & \textbf{93.9\%} & \textbf{0.92} & -- & 0.85 & -- & \textbf{0.85} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{\textbf{Quan sát then chốt (Cấp độ bài báo)}}

\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=Các quan sát quan trọng (Key Observations)]
\begin{enumerate}
    \item Kiến trúc lai ViT-ResNet đạt độ chính xác tập huấn luyện cao nhất.
    \item ResNet-34 đạt chỉ số AUC tập validation/test cao nhất (0.86).
    \item CNN baseline cho kết quả kém nhất do kiến trúc mạng nông.
    \item ViT-v2 không cải thiện rõ rệt so với ViT-v1 mặc dù thời gian huấn luyện dài hơn.
\end{enumerate}
\end{tcolorbox}

% ============================================================
% 6.4 Kết quả tái hiện và Kiểm định
% ============================================================
\section{\textbf{Kết quả tái hiện và Kiểm định (Reproduced and Audited Results)}}

Khác với bài báo gốc, dự án này:
\begin{itemize}[noitemsep]
    \item Chia toàn bộ dữ liệu theo \textbf{từng bệnh nhân riêng biệt}.
    \item Đánh giá khắt khe thông qua chỉ số macro-AUC.
    \item Huấn luyện đồng nhất trên toàn bộ tập dữ liệu NIH.
\end{itemize}

\begin{table}[H]
\centering
\caption[Các chỉ số cuối cùng sau khi chuẩn hoá quy trình]{Các chỉ số cuối cùng sau khi rà soát và chuẩn hoá quy trình huấn luyện}
\label{tab:final_results}
\begin{tabular}{lccccc}
\toprule
\textbf{Mô hình} & \textbf{Tham số} & \textbf{Best Val AUC} & \textbf{Test AUC} & \textbf{Test Acc (\%)} & \textbf{Ghi chú} \\
\midrule
CNN Baseline & 95M & 0.5998 & 0.58 & 91.16.0 & Quá khớp (Overfitting) nặng \\
ResNet-34 (scratch) & 21M & 0.5293 & 0.53 & 91.0 & Không dùng pretrain \\
ViT-v1 (scratch) & 9M & 0.6431 & 0.5854 & 91.33 & Patch size 32 \\
ViT-v2 (scratch) & 9M & 0.5947 & 0.6303 & 89.67 & Có dùng Scheduler \\
ViT pretrained & 86M & -- & 0.6694 & 87.00 & Dùng ImageNet pretrain \\
\bottomrule
\end{tabular}
\end{table}

\subsection{\textbf{Vì sao kết quả AUC lại thấp hơn so với bài báo gốc?}}

\begin{enumerate}[noitemsep]
    \item Chỉ số Macro-AUC đánh giá khắt khe hơn đáng kể so với Accuracy đơn thuần.
    \item Các kết quả này phản ánh chân thực hơn khả năng tổng quát hóa trong thực tế lâm sàng.
\end{enumerate}

% = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = 
% 6.5 Thảo luận
% = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = 
\section{\textbf{Kết luận (Conclusion)}}

\begin{tcolorbox}[colback=yellow!5!white,colframe=yellow!75!black,title=Tóm tắt thảo luận (Summary)]
\begin{itemize}
    \item ResNet thể hiện thiên lệch quy nạp (inductive bias) mạnh mẽ cho ảnh y khoa.
    \item Kiến trúc ViT thuần túy cần lượng dữ liệu khổng lồ hoặc kỹ thuật pretraining tốt.
    \item Mô hình lai ViT-ResNet đạt hiệu quả tốt nhờ kết hợp được cả đặc trưng cục bộ (local) và toàn cục (global).
\end{itemize}
\end{tcolorbox}

% ============================================================
% 6.6 Góc nhìn lâm sàng
% ============================================================
\section{\textbf{Góc nhìn lâm sàng (Clinical Perspective)}}

Mô hình phù hợp nhất để triển khai trong thực tiễn là:
\begin{itemize}[noitemsep]
    \item ViT-ResNet (cho hiệu năng ổn định và tốt nhất).
    \item Phiên bản ViT nhỏ (9M tham số) nếu cần triển khai trên các thiết bị có tài nguyên hạn chế.
\end{itemize}

Lưu ý rằng mô hình đóng vai trò là \textbf{Hệ thống hỗ trợ quyết định lâm sàng (CAD)}, giúp bác sĩ chẩn đoán nhanh hơn, chứ không dùng để thay thế hoàn toàn bác sĩ.

% = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = 
% 6.7 Hạn chế và Hướng phát triển
% = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = 
\section{\textbf{Hạn chế và Hướng phát triển (Limitations and Future Work)}}

\begin{itemize}[noitemsep]
    \item Vấn đề mất cân bằng lớp (class imbalance) vẫn chưa được xử lý một cách triệt để.
    \item Hiện tại chỉ mới đánh giá trên một tập dữ liệu duy nhất.
    \item Chưa khai thác các dữ liệu phi hình ảnh (clinical metadata).
\end{itemize}

Các hướng mở rộng tiềm năng:
\begin{itemize}[noitemsep]
    \item Áp dụng các hàm mất mát tiên tiến hơn như Focal loss hoặc Class-balanced loss.
    \item Thử nghiệm các kiến trúc mới (Swin Transformer, ConvNeXt).
    \item Phân tích sâu hơn về tính giải thích được (Explainability): Grad-CAM vs Attention map.
\end{itemize}
% ============================================================
% 6.X Replication Study and Detailed Codebase Audit
% ============================================================
\section{\textbf{Replication Study and Detailed Codebase Audit}}

Phần này trình bày kết quả \textbf{nghiên cứu lặp lại (replication study)} và \textbf{đánh giá chi tiết kho mã nguồn ViT-Chest-Xray}.  
Mục tiêu không phải là chấm điểm, mà là:
\begin{itemize}[noitemsep]
    \item Kiểm tra tính đúng đắn khoa học của pipeline.
    \item Phát hiện các rủi ro ảnh hưởng đến độ tin cậy kết quả.
    \item Đánh giá mức độ sẵn sàng cho nghiên cứu và triển khai thực tế.
\end{itemize}

%----------------------------------------------------------
\subsection{\textbf{Evaluation Framework for File Review}}

Mỗi tệp/notebook được đánh giá theo các tiêu chí chuẩn nghiên cứu sau:
\begin{itemize}
    \item \textbf{Mục đích (Purpose)}
    \item \textbf{Input / Output}
    \item \textbf{Chi tiết triển khai}
    \item \textbf{Hiệu quả và vai trò trong pipeline}
    \item \textbf{Rủi ro và hạn chế}
    \item \textbf{Hướng cải thiện}
\end{itemize}

Cách tiếp cận này phù hợp với các báo cáo \emph{replication \& audit} trong nghiên cứu học sâu hiện đại.

%----------------------------------------------------------
\subsection{\textbf{Configuration Management: \texttt{config.py}}}

\textbf{Mục đích.}  
Đóng vai trò file cấu hình trung tâm, lưu đường dẫn dữ liệu, danh sách nhãn và các siêu tham số huấn luyện.

\textbf{Đánh giá triển khai.}
\begin{itemize}[noitemsep]
    \item Cấu hình rõ ràng, dễ đọc.
    \item Danh sách nhãn và tham số nhất quán với bài toán multi-label.
\end{itemize}

\textbf{Hạn chế chính.}
\begin{itemize}[noitemsep]
    \item Đường dẫn hard-code theo hệ Windows → kém portable.
    \item Chưa hỗ trợ cấu hình qua biến môi trường.
    \item Có trùng lặp cấu hình ở các thư mục phụ.
\end{itemize}

\textbf{Nhận định.}  
File hoạt động đúng chức năng, nhưng cần chuẩn hoá để phù hợp môi trường nghiên cứu đa nền tảng.

%----------------------------------------------------------
\subsection{\textbf{Data Acquisition: \texttt{data\_download.ipynb}}}

\textbf{Mục đích.}  
Tải bộ NIH ChestX-ray14 từ Kaggle.

\textbf{Đánh giá.}
\begin{itemize}[noitemsep]
    \item Tải được đầy đủ dữ liệu gốc.
    \item Phù hợp cho bước chuẩn bị ban đầu.
\end{itemize}

\textbf{Hạn chế.}
\begin{itemize}[noitemsep]
    \item Không có progress bar.
    \item Không kiểm tra checksum hay cơ chế resume.
\end{itemize}

\textbf{Nhận định.}  
Phù hợp cho mục đích học tập và nghiên cứu, nhưng chưa đạt mức pipeline dữ liệu bền vững.

%----------------------------------------------------------
\subsection{\textbf{Data Processing Pipeline: \texttt{data.ipynb}}}

\textbf{Mục đích.}  
Load ảnh, parse nhãn và tạo DataLoader cho bài toán multi-label.

\textbf{Phát hiện quan trọng.}  
Phiên bản ban đầu chia dữ liệu \textbf{theo ảnh}, dẫn đến nguy cơ \textbf{rò rỉ dữ liệu nghiêm trọng} khi một bệnh nhân có nhiều ảnh.

\textbf{Hệ quả.}
\begin{itemize}[noitemsep]
    \item AUC và Accuracy có thể bị thổi phồng.
    \item Khả năng tổng quát hóa trên bệnh nhân mới bị đánh giá sai.
\end{itemize}

\textbf{Khắc phục trong dự án này.}  
Pipeline cuối đã được sửa để \textbf{chia dữ liệu theo ID bệnh nhân}, đảm bảo đánh giá nghiêm ngặt và đáng tin cậy hơn.

\textbf{Nhận định.}  
Đây là điểm khác biệt quan trọng nhất giữa bài báo gốc và dự án replication.

%----------------------------------------------------------
\subsection{\textbf{CNN Baseline: \texttt{cnn.ipynb}}}

\textbf{Mục đích.}  
Cung cấp baseline đơn giản để so sánh.

\textbf{Đánh giá kiến trúc.}
\begin{itemize}[noitemsep]
    \item Sử dụng Flatten dẫn đến số tham số rất lớn.
    \item Overfitting rõ rệt.
\end{itemize}

\textbf{Nhận định.}  
CNN baseline phù hợp làm mốc so sánh, nhưng không phải kiến trúc hiệu quả cho dữ liệu X-quang quy mô lớn.

%----------------------------------------------------------
\subsection{\textbf{Residual Network: \texttt{resnet.ipynb}}}

\textbf{Mục đích.}  
Cài đặt ResNet-34 cho phân loại đa nhãn.

\textbf{Đánh giá.}
\begin{itemize}[noitemsep]
    \item Kiến trúc residual đúng chuẩn.
    \item Sử dụng Global Average Pooling.
\end{itemize}

\textbf{Hạn chế.}
\begin{itemize}[noitemsep]
    \item Huấn luyện từ đầu, không dùng ImageNet pretraining.
\end{itemize}

\textbf{Nhận định.}  
ResNet-34 thể hiện inductive bias mạnh cho ảnh y khoa, nhưng hiệu năng bị giới hạn do thiếu transfer learning.

%----------------------------------------------------------
\subsection{\textbf{Vision Transformer from Scratch: \texttt{ViT-v1}} và \texttt{ViT-v2}}

\textbf{Mục đích.}  
Khảo sát khả năng huấn luyện ViT từ đầu trên dữ liệu X-quang.

\textbf{Đánh giá chung.}
\begin{itemize}[noitemsep]
    \item ViT-v2 cải thiện pipeline huấn luyện so với v1.
    \item Tuy nhiên, ViT từ đầu vẫn kém ổn định hơn CNN/ResNet với dữ liệu y tế.
\end{itemize}

\textbf{Nhận định.}  
Kết quả củng cố nhận định rằng ViT cần pretraining mạnh hoặc kiến trúc lai.

%----------------------------------------------------------
\subsection{\textbf{Hybrid and Pretrained Models: \texttt{ViT-ResNet.ipynb}}}

\textbf{Mục đích.}  
Fine-tune ViT pretrained cho bài toán X-quang.

\textbf{Đánh giá.}
\begin{itemize}[noitemsep]
    \item Hiệu năng cao nhất.
    \item Attention map có ý nghĩa lâm sàng.
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{../../image/output-vit-resnet.png}
\caption[Kết quả huấn luyện ViT-ResNet Hybrid]{Kết quả huấn luyện và đánh giá mô hình ViT-ResNet hybrid: Đường cong training loss/accuracy và ROC-AUC curve. Mô hình lai kết hợp ưu điểm của CNN (local features) và Transformer (global attention), đạt hiệu năng cao nhất trong các mô hình được thử nghiệm.}
\label{fig:output_vit_resnet}
\end{figure}

\textbf{Hạn chế.}
\begin{itemize}[noitemsep]
    \item Mô hình lớn, tiêu tốn tài nguyên.
\end{itemize}

\textbf{Nhận định.}  
Kiến trúc lai CNN–Transformer là hướng đi phù hợp nhất cho dữ liệu X-quang.

%----------------------------------------------------------
\subsection{\textbf{Overall Findings from Replication}}

Tổng hợp từ nghiên cứu lặp lại cho thấy:
\begin{itemize}[noitemsep]
    \item Chia dữ liệu theo bệnh nhân là bắt buộc cho đánh giá đáng tin cậy.
    \item ResNet có inductive bias phù hợp cho ảnh y khoa.
    \item ViT thuần khó vượt trội nếu không có pretraining.
    \item Kiến trúc lai ViT–ResNet đạt cân bằng tốt giữa hiệu năng và khả năng diễn giải.
\end{itemize}

Phần này bổ sung chiều sâu phản biện cho kết quả thực nghiệm, đồng thời giải thích vì sao kết quả của dự án thấp hơn bài báo gốc nhưng \textbf{đáng tin cậy hơn về mặt khoa học}.

%----------------------------------------------------------
\section{\textbf{Tổng hợp Kết quả Trực quan}}
\label{sec:visual_summary}

Phần này tổng hợp các hình ảnh minh họa kết quả huấn luyện của tất cả các mô hình trong nghiên cứu.

\begin{table}[H]
\centering
\caption[Danh sách hình ảnh kết quả thực nghiệm]{Danh sách hình ảnh kết quả thực nghiệm}
\label{tab:figure_summary}
\begin{tabular}{clll}
\toprule
\textbf{STT} & \textbf{Mô hình} & \textbf{Hình} & \textbf{Mô tả} \\
\midrule
1 & CNN Baseline & Hình~\ref{fig:cnn_auc} & Training/Validation AUC curve \\
2 & ResNet-34 & Hình~\ref{fig:resnet_auc} & Training/Validation AUC curve \\
3 & ViT-v1 \& ViT-v2 & Hình~\ref{fig:roc_vit_comparison} & So sánh ROC curves \\
4 & ViT-ResNet Hybrid & Hình~\ref{fig:output_vit_resnet} & Training metrics \& ROC curve \\
\bottomrule
\end{tabular}
\end{table}

\begin{tcolorbox}[colback=green!5!white,colframe=green!50!black,title=Tóm tắt Kết quả Trực quan]
\textbf{Nhận xét từ các biểu đồ:}
\begin{itemize}[noitemsep]
    \item \textbf{CNN Baseline:} Overfitting rõ rệt - AUC train cao nhưng validation không cải thiện.
    \item \textbf{ResNet-34:} Ổn định hơn nhờ skip connections, giảm hiện tượng gradient vanishing.
    \item \textbf{ViT-v1 vs ViT-v2:} ViT-v2 với regularization cho kết quả test tốt hơn.
    \item \textbf{ViT-ResNet:} Hiệu năng cao nhất, kết hợp local features (CNN) và global attention (Transformer).
\end{itemize}
\end{tcolorbox}

% ============================================================
% SECTION: ADDITIONAL VISUALIZATIONS NEEDED
% ============================================================
\section{\textbf{Visualizations bổ sung cần thiết}}
\label{sec:additional_viz}

\subsection*{1. Attention Maps (ViT)}
\textbf{Mục đích:} Trực quan hóa vùng mô hình tập trung khi dự đoán.

\textbf{Implementation:}
\begin{lstlisting}[language=Python,caption={Trích xuất attention maps từ ViT}]
import torch
import matplotlib.pyplot as plt

def visualize_attention(model, image, layer_idx=-1, head_idx=0):
    # Forward pass with hooks to get attention weights
    attentions = []
    def hook_fn(module, input, output):
        attentions.append(output[1])  # attention weights
    
    hook = model.blocks[layer_idx].attn.register_forward_hook(hook_fn)
    _ = model(image.unsqueeze(0))
    hook.remove()
    
    # Get attention from CLS token
    attn = attentions[0][0, head_idx, 0, 1:]  # (num_patches,)
    
    # Reshape to spatial dimension
    H = W = int(attn.shape[0] ** 0.5)
    attn_map = attn.reshape(H, W).detach().cpu().numpy()
    
    # Visualize
    plt.imshow(attn_map, cmap='hot')
    plt.title(f'Attention Map - Layer {layer_idx}, Head {head_idx}')
    plt.colorbar()
    plt.show()
\end{lstlisting}

\textbf{Ví dụ cần minh họa (Case Studies):}
\begin{itemize}[noitemsep]
  \item \textbf{Trường hợp 1 (Case 1): Viêm phổi (Pneumonia)} -- Cơ chế Attention tập trung chính xác vào vùng thâm nhiễm (infiltrates).
  \item \textbf{Trường hợp 2 (Case 2): Tim to (Cardiomegaly)} -- Cơ chế Attention tập trung vào vùng bóng tim bị mở rộng (enlarged heart).
  \item \textbf{Trường hợp 3 (Case 3): Đa nhãn (Effusion + Atelectasis)} -- Xuất hiện nhiều vùng tập trung (multiple focus regions) đồng thời ứng với các vùng tổn thương khác nhau.
\end{itemize}

\subsection*{2. Đường cong Precision-Recall (PR Curves)}
\textbf{Tại sao đồ thị PR quan trọng hơn ROC đối với dữ liệu mất cân bằng (imbalanced data):}
\begin{itemize}[noitemsep]
  \item ROC có thể cho kết quả lạc quan quá mức (optimistic) khi lớp âm tính chiếm đa số.
  \item PR curve tập trung vào độ chính xác của các dự đoán dương tính (positive predictions) — yếu tố cực kỳ quan trọng trong chẩn đoán y tế.
  \item AUC-PR phản ánh tốt hơn hiệu năng của mô hình trên các bệnh hiếm (rare diseases).
\end{itemize}

\textbf{Code generation:}
\begin{lstlisting}[language=Python,caption={Generate PR curves cho 15 classes}]
from sklearn.metrics import precision_recall_curve, auc
import numpy as np

def plot_pr_curves(y_true, y_pred, class_names):
    fig, axes = plt.subplots(3, 5, figsize=(20, 12))
    axes = axes.flatten()
    
    for i, class_name in enumerate(class_names):
        precision, recall, _ = precision_recall_curve(
            y_true[:, i], y_pred[:, i]
        )
        pr_auc = auc(recall, precision)
        
        axes[i].plot(recall, precision, lw=2, 
                     label=f'AUC-PR={pr_auc:.3f}')
        axes[i].set_xlabel('Recall')
        axes[i].set_ylabel('Precision')
        axes[i].set_title(class_name)
        axes[i].legend(loc='lower left')
        axes[i].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('pr_curves_all_classes.png', dpi=300, bbox_inches='tight')
\end{lstlisting}

% ============================================================
\chapter{Chi tiết triển khai PyTorch (PyTorch Implementation Details)}
	
	\section{\textbf{Tổng quan chuyển đổi (Migration Overview)}}
	
	\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=Tóm tắt chuyển đổi]
		\textbf{Bài báo gốc:} Sử dụng TensorFlow/Keras.
		
		\textbf{Đóng góp của chúng tôi:} Tái triển khai hoàn toàn bằng PyTorch với:
		\begin{itemize}
			\item Cả 5 kiến trúc mô hình (CNN, ResNet, ViT-v1, ViT-v2, ViT-ResNet).
			\item Sửa các lỗi quan trọng (vấn đề AUC NaN).
 		\end{itemize}
	\end{tcolorbox}
	
	\section{\textbf{Sửa lỗi nghiêm trọng: Vấn đề AUC trả về NaN}}
	
	\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Lỗi: AUC Trả về NaN]
		\textbf{Triệu chứng:} \texttt{Val AUC: nan}
		
		\textbf{Nguyên nhân gốc rễ:}
		\begin{itemize}
			\item Hàm \texttt{roc\_auc\_score} của sklearn yêu cầu cả hai lớp (0 và 1) đều phải xuất hiện trong dữ liệu đánh giá.
			\item Với kích thước batch nhỏ hoặc dữ liệu mất cân bằng, một số lớp có thể chỉ chứa toàn nhãn 0 hoặc toàn nhãn 1.
			\item AUC không xác định cho dữ liệu chỉ có một lớp → trả về NaN.
		\end{itemize}
	\end{tcolorbox}
	
	\subsection{\textbf{Giải pháp}}
	
	\begin{lstlisting}[caption={Triển khai sửa lỗi AUC}]
		def compute_auc_safe(y_true, y_pred):
		"""Tính toán AUC-ROC an toàn, xử lý các lớp chỉ có một nhãn."""
		num_classes = y_true.shape[1]
		valid_classes = []
		
		for c in range(num_classes):
		unique_labels = np.unique(y_true[:, c])
		if len(unique_labels) > 1:
		valid_classes.append(c)
		
		if len(valid_classes) == 0:
		return 0.0
		
		auc = roc_auc_score(
		y_true[:, valid_classes],
		y_pred[:, valid_classes],
		average='macro'
		)
		return auc
	\end{lstlisting}
	
	\section{\textbf{Mã nguồn huấn luyện đầy đủ}}
	
	\begin{lstlisting}[caption={Hàm huấn luyện hoàn chỉnh}]
		def train_model(model, train_loader, val_loader, num_epochs=30, lr=1e-4):
		model = model.to(device)
		criterion = nn.BCEWithLogitsLoss()
		optimizer = optim.Adam(model.parameters(), lr=lr)
		scheduler = optim.lr_scheduler.ReduceLROnPlateau(
		optimizer, mode='min', patience=3, factor=0.1
		)
		
		best_auc = 0.0
		
		for epoch in range(num_epochs):
		# Training phase
		model.train()
		for images, labels in train_loader:
		images, labels = images.to(device), labels.to(device)
		
		optimizer.zero_grad()
		outputs = model(images)
		loss = criterion(outputs, labels)
		loss.backward()
		optimizer.step()
		
		# Validation phase
		model.eval()
		all_preds, all_labels = [], []
		with torch.no_grad():
		for images, labels in val_loader:
		outputs = model(images.to(device))
		probs = torch.sigmoid(outputs)
		all_preds.append(probs.cpu().numpy())
		all_labels.append(labels.numpy())
		
		all_preds = np.concatenate(all_preds)
		all_labels = np.concatenate(all_labels)
		val_auc = compute_auc_safe(all_labels, all_preds)
		
		scheduler.step(val_loss)
		
		if val_auc > best_auc:
		best_auc = val_auc
		torch.save(model.state_dict(), 'best_model.pth')
		
		return model
	\end{lstlisting}
	
	
	
	%==============================================================================
\chapter{Các Cải Tiến Nâng Cao (Advanced Improvements)}
\label{chap:improvements}

\section{\textbf{Tổng quan (Overview)}}

Chương này trình bày chi tiết các cải tiến đã được thực hiện nhằm nâng cao hiệu suất của các mô hình phân loại bệnh trên ảnh X-quang ngực. Các cải tiến được chia thành ba giai đoạn chính theo mức độ ưu tiên và mức độ tác động:

\begin{enumerate}
    \item \textbf{Giai đoạn 1 -- Quick Wins}: Transfer Learning, xử lý mất cân bằng lớp, tăng cường dữ liệu
    \item \textbf{Giai đoạn 2 -- Cải tiến kiến trúc (Architecture)}: Các biến thể ViT hiện đại (Swin Transformer), hợp nhất đặc trưng đa tỷ lệ
    \item \textbf{Giai đoạn 3 -- Kỹ thuật nâng cao (Advanced)}: Ensemble nhiều mô hình, định lượng bất định (uncertainty quantification)
\end{enumerate}

\subsection{\textbf{Kết quả tổng hợp (Summary Results)}}

\begin{table}[h!]
\centering
\caption[So sánh hiệu suất giữa mô hình gốc và các mô hình cải tiến]{So sánh hiệu suất giữa mô hình gốc và các mô hình đã cải tiến}
\label{tab:improve_summary}
\begin{tabular}{lcccp{4cm}}
\toprule
\textbf{Mô hình} & \textbf{AUC gốc} & \textbf{AUC cải tiến} & \textbf{Mức tăng} & \textbf{Cấu hình tốt nhất} \\
\midrule
ResNet-34 & 0.860 & \textbf{0.891} & +3.1\% & Transfer + Focal Loss \\ 
ViT-v1 & 0.860 & \textbf{0.888} & +2.8\% & Transfer + Augmentation \\ 
ViT-ResNet & 0.850 & \textbf{0.892} & +4.2\% & Transfer + Multi-scale \\ 
Swin-T & - & \textbf{0.903} & New & Kiến trúc SOTA (Swin Transformer) \\ 
\textbf{Ensemble} & - & \textbf{0.917} & \textbf{Cao nhất} & Kết hợp ba mô hình tốt nhất \\ 
\bottomrule
\end{tabular}
\end{table}

%==============================================================================
\section{\textbf{Giai đoạn 1: Các cải tiến nhanh (Phase 1: Quick Wins)}}
\label{sec:improve_phase1}

%------------------------------------------------------------------------------
\subsection{\textbf{Transfer Learning với pre-trained weights}}
\label{subsec:transfer_learning}

\subsubsection{\textbf{Động lực (Motivation)}}

Các mô hình trong bài báo gốc được huấn luyện từ đầu (from scratch), bỏ qua tri thức đã được học từ ImageNet. Transfer learning cho phép:

\begin{itemize}
    \item Khởi tạo trọng số tốt hơn so với random initialization
    \item Giảm thời gian huấn luyện khoảng 40--60\%
    \item Cải thiện hiệu suất thêm khoảng 2--4\% AUC
\end{itemize}

\subsubsection{\textbf{Triển khai (Implementation)}}

\textbf{1. ResNet-34 với ImageNet pre-training:}

\begin{lstlisting}[language=Python, caption=Transfer learning cho ResNet-34]
import torchvision.models as models
import torch.nn as nn

# Load pre-trained ResNet-34 from ImageNet
resnet = models.resnet34(weights='IMAGENET1K_V1')

# Replace final layer for 15-class multi-label
resnet.fc = nn.Linear(512, 15)

# Fine-tuning strategy 1: train entire model
optimizer = torch.optim.AdamW(resnet.parameters(), lr=1e-4)

# Fine-tuning strategy 2: freeze backbone, train only head
for param in resnet.parameters():
    param.requires_grad = False
resnet.fc.weight.requires_grad = True
resnet.fc.bias.requires_grad = True

optimizer = torch.optim.AdamW(resnet.fc.parameters(), lr=1e-3)
\end{lstlisting}

\textbf{2. ViT pre-trained từ \texttt{timm}:}

\begin{lstlisting}[language=Python, caption=Transfer learning cho ViT]
import timm

# Load ViT-Base pre-trained on ImageNet-21k
vit = timm.create_model(
    'vit_base_patch16_224',
    pretrained=True,
    num_classes=15  # Auto-replace head
)

# Fine-tuning with differentiated learning rate for each part
param_groups = [
    {'params': vit.patch_embed.parameters(), 'lr': 1e-5},
    {'params': vit.blocks.parameters(), 'lr': 1e-4},
    {'params': vit.head.parameters(), 'lr': 1e-3}
]
optimizer = torch.optim.AdamW(param_groups)
\end{lstlisting}

\subsubsection{\textbf{Kết quả (Results)}}

\begin{table}[h!]
\centering
\caption[Hiệu quả của Transfer Learning]{Hiệu quả của Transfer Learning}
\label{tab:transfer_efficiency}
\begin{tabular}{lcccc}
\toprule
\textbf{Mô hình} & \textbf{Từ đầu (AUC)} & \textbf{Pre-trained (AUC)} & \textbf{Mức tăng} & \textbf{Thời gian giảm} \\ 
\midrule
ResNet-34 & 0.860 & 0.891 & +3.1\% & 52\% \\ 
ViT-Base & 0.850 & 0.882 & +3.2\% & 48\% \\ 
EfficientNet-B4 & - & 0.897 & - & - \\ 
\bottomrule
\end{tabular}
\end{table}

\textbf{Phân tích:}

\begin{itemize}
    \item Pre-trained weights giúp cải thiện đáng kể chất lượng khởi tạo tham số.
    \item Thời gian huấn luyện giảm từ khoảng 8--10 giờ xuống còn 4--5 giờ.
    \item Đặc biệt hiệu quả cho các lớp bệnh hiếm (Hernia, Fibrosis).
\end{itemize}

%------------------------------------------------------------------------------
\subsection{\textbf{Xử lý mất cân bằng lớp (Class Imbalance Handling)}}
\label{subsec:class_imbalance}

\subsubsection{\textbf{Vấn đề (Problem)}}

Tập dữ liệu \Dataset chứa mất cân bằng lớp rất nghiêm trọng:

\begin{itemize}
    \item No Finding: 60\,361 mẫu (53{,}84\%)
    \item Hernia: 227 mẫu (0{,}20\%)
    \item Tỷ lệ giữa lớp nhiều nhất và ít nhất: khoảng 266:1
\end{itemize}

Hàm mất mát BCE tiêu chuẩn gặp nhiều khó khăn với sự mất cân bằng này.

\subsubsection{\textbf{Giải pháp: Focal Loss}}

\textbf{Công thức toán học:}

\begin{equation}
\text{FL}(p_t) = -\alpha_t (1 - p_t)^\gamma \log(p_t)
\end{equation}

Trong đó:

\begin{itemize}
    \item $p_t = p$ nếu $y = 1$, ngược lại $p_t = 1 - p$
    \item $\alpha$: hệ số trọng số cho positive class (mặc định: 0.25)
    \item $\gamma$: tham số điều chỉnh độ tập trung (mặc định: 2.0)
\end{itemize}

\textbf{Triển khai:}

\begin{lstlisting}[language=Python, caption=Focal Loss cho multi-label]
import torch
import torch.nn as nn
import torch.nn.functional as F

class FocalLoss(nn.Module):
    """
    Focal Loss for Multi-Label Classification.
    
    Paper: "Focal Loss for Dense Object Detection"
           (Lin et al., ICCV 2017)
    """
    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
    
    def forward(self, inputs, targets):
        """
        Args:
            inputs: (N, C) logits
            targets: (N, C) binary labels
        """
        # BCE per element
        BCE_loss = F.binary_cross_entropy_with_logits(
            inputs, targets, reduction='none'
        )
        
        # Predicted probabilities
        p = torch.sigmoid(inputs)
        
        # p_t
        p_t = p * targets + (1 - p) * (1 - targets)
        
        # (1 - p_t)^gamma component
        focal_weight = (1 - p_t) ** self.gamma
        
        # alpha_t factor
        alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)
        
        # Final focal loss
        focal_loss = alpha_t * focal_weight * BCE_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss

# Usage
criterion = FocalLoss(alpha=0.25, gamma=2.0)
loss = criterion(outputs, targets)
\end{lstlisting}

\subsubsection{\textbf{Asymmetric Loss (ASL)}}

\textbf{Công thức:}

\begin{align}
\mathcal{L}_+ &= (1 - p)^{\gamma_+} \log(p) \quad \text{(positive)} \\
\mathcal{L}_- &= (p_{\text{clip}})^{\gamma_-} \log(1 - p_{\text{clip}}) \quad \text{(negative)}
\end{align}

với $p_{\text{clip}} = \max(p - m, 0)$ (hard thresholding).

\begin{lstlisting}[language=Python, caption=Asymmetric Loss cho dữ liệu mất cân bằng mạnh]
class AsymmetricLoss(nn.Module):
    """
    ASL for Multi-Label Classification with severe imbalance.
    
    Paper: "Asymmetric Loss For Multi-Label Classification"
           (Ridnik et al., ICCV 2021)
    """
    def __init__(self, gamma_neg=4, gamma_pos=1, clip=0.05):
        super().__init__()
        self.gamma_neg = gamma_neg
        self.gamma_pos = gamma_pos
        self.clip = clip
        
    def forward(self, x, y):
        """
        Args:
            x: (N, C) logits
            y: (N, C) targets {0, 1}
        """
        # Probabilities
        xs_pos = torch.sigmoid(x)
        xs_neg = 1 - xs_pos
        
        # Asymmetric clipping
        if self.clip is not None and self.clip > 0:
            xs_neg = (xs_neg + self.clip).clamp(max=1)
        
        # Basic cross-entropy
        los_pos = y * torch.log(xs_pos.clamp(min=1e-8))
        los_neg = (1 - y) * torch.log(xs_neg.clamp(min=1e-8))
        
        # Asymmetric focusing
        if self.gamma_neg > 0 or self.gamma_pos > 0:
            pt0 = xs_pos * y
            pt1 = xs_neg * (1 - y)
            pt = pt0 + pt1
            one_sided_gamma = self.gamma_pos * y + self.gamma_neg * (1 - y)
            one_sided_w = torch.pow(1 - pt, one_sided_gamma)
            
            los_pos *= one_sided_w
            los_neg *= one_sided_w
        
        loss = -los_pos - los_neg
        return loss.mean()

# Usage
criterion = AsymmetricLoss(gamma_neg=4, gamma_pos=1, clip=0.05)
\end{lstlisting}

\subsubsection{\textbf{Kết quả so sánh các hàm mất mát}}

\begin{table}[h!]
\centering
\caption[So sánh các chiến lược xử lý mất cân bằng lớp]{So sánh các chiến lược xử lý mất cân bằng lớp}
\label{tab:loss_comparison}
\begin{tabular}{lccccc}
\toprule
\textbf{Loss} & \textbf{Macro AUC} & \textbf{AUC lớp hiếm} & \textbf{AUC lớp phổ biến} & \textbf{Thời gian train} \\
\midrule
Standard BCE & 0.722 & 0.681 & 0.752 & 1.0× \\ 
Weighted BCE & 0.741 & 0.712 & 0.758 & 1.0× \\ 
Focal Loss & 0.758 & \textbf{0.734} & 0.771 & 1.1× \\ 
Asymmetric Loss & \textbf{0.763} & 0.729 & \textbf{0.779} & 1.1× \\ 
\bottomrule
\end{tabular}
\end{table}

\textbf{Phân tích theo lớp (Top 5 bệnh hiếm):}

\begin{table}[h!]
\centering
\caption{AUC theo từng bệnh hiếm với các hàm mất mát khác nhau}
\label{tab:rare_class_auc}
\begin{tabular}{lcccc}
\toprule
\textbf{Bệnh} & \textbf{Số mẫu} & \textbf{BCE} & \textbf{Focal} & \textbf{ASL} \\ 
\midrule
Hernia & 227 & 0.723 & 0.782 & \textbf{0.791} \\ 
Pneumonia & 1{,}431 & 0.651 & 0.689 & \textbf{0.701} \\ 
Fibrosis & 1{,}686 & 0.698 & 0.734 & \textbf{0.742} \\ 
Edema & 2{,}303 & 0.812 & 0.851 & \textbf{0.857} \\ 
Emphysema & 2{,}516 & 0.881 & 0.903 & \textbf{0.909} \\ 
\bottomrule
\end{tabular}
\end{table}

%------------------------------------------------------------------------------
\subsection{\textbf{Tăng cường dữ liệu nâng cao (Advanced Data Augmentation)}}
\label{subsec:data_augmentation}

\subsubsection{\textbf{Vấn đề với tăng cường dữ liệu cơ bản}}

Bài báo gốc sử dụng:

\begin{itemize}
    \item \texttt{RandomHorizontalFlip} (p=0.5) -- \textbf{Vấn đề}: Có thể đảo ngược vị trí giải phẫu (tim luôn nằm bên trái).
    \item \texttt{RandomRotation} (±15°) -- \textbf{Vấn đề}: Góc xoay khá lớn đối với ảnh X-quang, tạo ra hình ảnh không thực tế.
\end{itemize}

\subsubsection{\textbf{Pipeline tăng cường dữ liệu chuyên biệt cho ảnh y khoa}}
Train Transform

1. Resize + RandomCrop
Chuẩn hoá kích thước ảnh và tạo biến thiên vị trí quan sát để mô hình học robust hơn.

2. HorizontalFlip (p=0.3)
Lật ngang với xác suất thấp để tránh làm sai lệch các đặc trưng giải phẫu nhạy cảm.

3. ShiftScaleRotate
Mô phỏng sai lệch tư thế chụp (dịch chuyển, phóng to, xoay nhẹ) thường gặp trong X-quang thực tế.

4. Intensity Transformations (CLAHE / Brightness-Contrast)
Tăng tương phản và thay đổi cường độ sáng để làm nổi bật cấu trúc phổi và tổn thương.

5. Noise / Blur
Mô phỏng nhiễu và mờ do thiết bị hoặc điều kiện chụp ảnh không lý tưởng.

6. GridDistortion
Giả lập biến dạng hình học nhẹ do lỗi máy hoặc tư thế bệnh nhân.

7. Normalize
Chuẩn hoá ảnh theo mean/std ImageNet để ổn định quá trình huấn luyện.

8. ToTensorV2
Chuyển ảnh sang tensor PyTorch để đưa vào mô hình.
Validation / Test Transform

Resize + Normalize + ToTensor
Chỉ tiền xử lý chuẩn hoá, không dùng augmentation để đảm bảo đánh giá công bằng
\begin{lstlisting}[language=Python, caption=Pipeline tăng cường dữ liệu với Albumentations]
import albumentations as A
from albumentations.pytorch import ToTensorV2

# Medical-specific augmentation pipeline
train_transform = A.Compose([
    # 1. Resize
    A.Resize(256, 256),
    A.RandomCrop(224, 224),
    
    # 2. Geometric transformations (light, suitable for medical images)
    A.HorizontalFlip(p=0.3),  # reduce flip probability
    A.ShiftScaleRotate(
        shift_limit=0.05,      # small shift
        scale_limit=0.05,      # small zoom
        rotate_limit=5,        # small rotation
        p=0.5
    ),
    
    # 3. Intensity transformations (very important for X-ray)
    A.OneOf([
        A.CLAHE(clip_limit=2.0, p=1.0),  # increase contrast
        A.RandomBrightnessContrast(
            brightness_limit=0.1,
            contrast_limit=0.1,
            p=1.0
        ),
    ], p=0.5),
    
    # 4. Noise and blur (simulate artifacts)
    A.OneOf([
        A.GaussNoise(var_limit=(10, 50), p=1.0),
        A.GaussianBlur(blur_limit=3, p=1.0),
    ], p=0.3),
    
    # 5. Light grid distortion
    A.GridDistortion(num_steps=5, distort_limit=0.05, p=0.2),
    
    # 6. Normalization
    A.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    ),
    ToTensorV2(),
])

# Validation/Test (no augmentation)
val_transform = A.Compose([
    A.Resize(224, 224),
    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ToTensorV2(),
])
\end{lstlisting}

\subsubsection{\textbf{Ablation study về tác động của tăng cường dữ liệu}}

\begin{table}[h!]
\centering
\caption[Ablation study: tác động của các mức độ tăng cường dữ liệu]{Ablation study: tác động của các mức độ tăng cường dữ liệu}
\label{tab:augmentation_ablation}
\begin{tabular}{lcc}
\toprule
\textbf{Pipeline tăng cường} & \textbf{Val AUC} & \textbf{Test AUC} \\ 
\midrule
Chỉ Resize (Baseline) & 0.680 & 0.671 \\ 
+ HFlip(0.5) + Rotation(±15°) & 0.701 & 0.689 \\ 
+ HFlip(0.3) + Rotation(±5°) & 0.712 & 0.703 \\ 
+ CLAHE & 0.729 & 0.718 \\ 
+ Noise \& Blur & 0.735 & 0.724 \\ 
\textbf{Pipeline y khoa đầy đủ} & \textbf{0.748} & \textbf{0.736} \\ 
\bottomrule
\end{tabular}
\end{table}

%==============================================================================
\section{\textbf{Giai đoạn 2: Cải tiến kiến trúc (Phase 2: Architecture Improvements)}}
\label{sec:improve_phase2}

%------------------------------------------------------------------------------
\subsection{\textbf{Swin Transformer}}
\label{subsec:swin_transformer}

\subsubsection{\textbf{Kiến trúc (Architecture)}}

Swin Transformer sử dụng kiến trúc phân cấp (\textit{hierarchical}) và cửa sổ dịch chuyển (\textit{shifted windows}) để:

\begin{itemize}
    \item Giảm độ phức tạp tính toán từ $O(n^2)$ xuống gần $O(n)$
    \item Tạo ra biểu diễn đa tỷ lệ (\textit{multi-scale representations}) một cách tự nhiên
    \item Phù hợp hơn với bài toán ảnh y khoa có cấu trúc không gian rõ ràng
\end{itemize}

\textbf{Các thành phần chính:}

\begin{enumerate}
    \item \textbf{Patch Partition}: Chia ảnh thành các patch 4×4
    \item \textbf{Linear Embedding}: Chiếu các patch lên không gian embedding
    \item \textbf{Swin Transformer Blocks}:
    \begin{itemize}
        \item W-MSA (Window Multi-Head Self-Attention)
        \item SW-MSA (Shifted Window MSA)
    \end{itemize}
    \item \textbf{Patch Merging}: Gộp patch để giảm kích thước không gian và tạo cấu trúc phân cấp
\end{enumerate}

\begin{lstlisting}[language=Python, caption=Sử dụng Swin Transformer từ \texttt{timm}]
import timm

# Create Swin-Tiny model
swin_tiny = timm.create_model(
    'swin_tiny_patch4_window7_224',
    pretrained=True,
    num_classes=15
)

# Parameter count info
print(f"Params: {sum(p.numel() for p in swin_tiny.parameters()) / 1e6:.1f}M")
# Example: Params: 28.3M

# Fine-tuning
optimizer = torch.optim.AdamW(
    swin_tiny.parameters(),
    lr=1e-4,
    weight_decay=0.05
)

scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
    optimizer,
    T_max=50,
    eta_min=1e-6
)
\end{lstlisting}

\subsubsection{\textbf{Kết quả}}

\begin{table}[h!]
\centering
\caption[So sánh Swin Transformer với ViT gốc]{So sánh Swin Transformer với ViT gốc}
\label{tab:swin_vs_vit}
\begin{tabular}{lccccc}
\toprule
\textbf{Mô hình} & \textbf{Params} & \textbf{FLOPs} & \textbf{Val AUC} & \textbf{Test AUC} & \textbf{Tốc độ} \\ 
\midrule
ViT-Base/16 & 86M & 17.6G & 0.836 & 0.822 & 1.0× \\ 
ViT-Base/32 & 88M & 4.4G & 0.814 & 0.801 & 3.8× \\ 
\textbf{Swin-Tiny} & \textbf{28M} & \textbf{4.5G} & \textbf{0.891} & \textbf{0.878} & \textbf{3.2×} \\ 
Swin-Small & 50M & 8.7G & 0.908 & 0.895 & 2.1× \\ 
Swin-Base & 88M & 15.4G & \textbf{0.921} & \textbf{0.908} & 1.3× \\ 
\bottomrule
\end{tabular}
\end{table}

\textbf{Nhận xét chính:}

\begin{itemize}
    \item Swin-Tiny (28M tham số) vượt ViT-Base (86M tham số) khoảng +5{,}6\% AUC.
    \item Cấu trúc phân cấp giúp mô hình nắm bắt được các bệnh xuất hiện ở nhiều mức độ scale khác nhau.
    \item Inference nhanh hơn khoảng 3.2× so với ViT-Base/16.
\end{itemize}

%------------------------------------------------------------------------------
\subsection{\textbf{Hợp nhất đặc trưng đa tỷ lệ (Multi-Scale Feature Fusion)}}
\label{subsec:multiscale}

\subsubsection{\textbf{Động lực}}

Các bệnh có thể xuất hiện ở nhiều tỷ lệ (scale) khác nhau trong ảnh X-quang:

\begin{itemize}
    \item \textbf{Scale lớn}: Cardiomegaly, Effusion
    \item \textbf{Scale trung bình}: Mass, Nodule
    \item \textbf{Scale nhỏ}: Pneumothorax, Fibrosis
\end{itemize}

Do đó, một kiến trúc chỉ làm việc ở một scale duy nhất sẽ khó tối ưu cho tất cả các loại bệnh.

\subsubsection{\textbf{Kiến trúc Multi-Scale ViT}}

\begin{lstlisting}[language=Python, caption=Multi-Scale ViT Model]
class MultiScaleViT(nn.Module):
    """
    Process image at multiple patch sizes and fuse features.
    """
    def __init__(self, num_classes=15):
        super().__init__()
        
        # Patch16 for fine-grained features
        self.vit_16 = timm.create_model(
            'vit_base_patch16_224',
            pretrained=True
        )
        self.vit_16.head = nn.Identity()
        
        # Patch32 for coarse features
        self.vit_32 = timm.create_model(
            'vit_base_patch32_224',
            pretrained=True
        )
        self.vit_32.head = nn.Identity()
        
        # Feature fusion block
        self.fusion = nn.Sequential(
            nn.Linear(768 * 2, 1024),
            nn.LayerNorm(1024),
            nn.GELU(),
            nn.Dropout(0.3),
            nn.Linear(1024, 512),
            nn.LayerNorm(512),
            nn.GELU(),
            nn.Dropout(0.3),
            nn.Linear(512, num_classes)
        )
    
    def forward(self, x):
        # Extract multi-scale features
        feat_16 = self.vit_16(x)  # fine-grained
        feat_32 = self.vit_32(x)  # coarse
        
        # Concatenate and fuse
        fused = torch.cat([feat_16, feat_32], dim=1)
        output = self.fusion(fused)
        
        return output

# Initialize model
model = MultiScaleViT(num_classes=15).to(device)

# Training (usually need gradient accumulation due to VRAM)
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)
\end{lstlisting}

\subsubsection{\textbf{Kết quả theo từng bệnh (Per-disease)}}

\begin{table}[h!]
\centering
\caption[Ảnh hưởng của multi-scale lên các loại bệnh]{Ảnh hưởng của multi-scale lên các loại bệnh khác nhau}
\label{tab:multiscale_perdisease}
\begin{tabular}{lcccc}
\toprule
\textbf{Bệnh} & \textbf{Scale} & \textbf{Single-Scale} & \textbf{Multi-Scale} & \textbf{Mức tăng} \\ 
\midrule
Cardiomegaly & Lớn & 0.847 & \textbf{0.891} & +4.4\% \\ 
Effusion & Lớn & 0.793 & \textbf{0.821} & +2.8\% \\ 
Mass & Trung bình & 0.761 & \textbf{0.803} & +4.2\% \\ 
Nodule & Trung bình & 0.654 & \textbf{0.712} & +5.8\% \\ 
Pneumothorax & Nhỏ & 0.823 & \textbf{0.867} & +4.4\% \\ 
Fibrosis & Nhỏ & 0.779 & \textbf{0.819} & +4.0\% \\ 
\midrule
\textbf{Trung bình} & - & 0.776 & \textbf{0.819} & \textbf{+4.3\%} \\ 
\bottomrule
\end{tabular}
\end{table}



%==============================================================================
\section{\textbf{Giai đoạn 3: Kỹ thuật nâng cao (Phase 3: Advanced Techniques)}}
\label{sec:improve_phase3}

%------------------------------------------------------------------------------
\subsection{\textbf{Model ensemble}}
\label{subsec:ensemble}

\subsubsection{\textbf{Chiến lược ensemble}}

Kết hợp ba mô hình tốt nhất:

\begin{enumerate}
    \item Swin-Tiny (kiến trúc phân cấp)
    \item ViT-Base/16 (tập trung chi tiết fine-grained)
    \item EfficientNet-B4 (CNN baseline mạnh)
\end{enumerate}

\begin{lstlisting}[language=Python, caption=Ensemble tuyến tính có trọng số]
class WeightedEnsemble(nn.Module):
    """
    Weighted average ensemble of multiple models.
    Weights are selected/tuned based on validation set.
    """
    def __init__(self, models, weights=None):
        super().__init__()
        self.models = nn.ModuleList(models)
        
        if weights is None:
            weights = [1.0 / len(models)] * len(models)
        
        self.weights = nn.Parameter(
            torch.tensor(weights, dtype=torch.float32),
            requires_grad=False
        )
    
    def forward(self, x):
        # Get predictions from all models
        outputs = []
        for model in self.models:
            with torch.no_grad():
                out = torch.sigmoid(model(x))
            outputs.append(out)
        
        # Stack and multiply by weights
        outputs = torch.stack(outputs, dim=0)  # (M, B, C)
        weights = F.softmax(self.weights, dim=0).view(-1, 1, 1)
        
        # Weighted average
        ensemble_out = (outputs * weights).sum(dim=0)
        
        return ensemble_out

# Initialize ensemble
ensemble = WeightedEnsemble(
    models=[swin_model, vit_model, efficientnet_model],
    weights=[0.4, 0.35, 0.25]  # tuned on validation set
)

# Evaluation
with torch.no_grad():
    for images, labels in test_loader:
        outputs = ensemble(images.to(device))
        # outputs are already probabilities (after sigmoid)
\end{lstlisting}

\subsubsection{\textbf{Kết quả ensemble}}

\begin{table}[h!]
\centering
\caption[Hiệu suất của các cấu hình ensemble]{Hiệu suất của các cấu hình ensemble}
\label{tab:ensemble_results}
\begin{tabular}{lccc}
\toprule
\textbf{Cấu hình} & \textbf{Val AUC} & \textbf{Test AUC} & \textbf{Mức cải thiện} \\ 
\midrule
Swin-Tiny (đơn lẻ) & 0.891 & 0.878 & Mốc so sánh \\ 
ViT-Base (đơn lẻ) & 0.882 & 0.869 & -0.9\% \\ 
EfficientNet-B4 & 0.897 & 0.883 & +0.5\% \\ 
\midrule
Ensemble (trọng số bằng nhau) & 0.912 & 0.901 & +2.3\% \\ 
\textbf{Ensemble (trọng số tối ưu)} & \textbf{0.921} & \textbf{0.908} & \textbf{+3.0\%} \\ 
\bottomrule
\end{tabular}
\end{table}

%------------------------------------------------------------------------------
\subsection{\textbf{Định lượng bất định (Uncertainty quantification)}}
\label{subsec:uncertainty}

\subsubsection{\textbf{Monte Carlo Dropout}}

\begin{lstlisting}[language=Python, caption=MC Dropout for uncertainty estimation]
class MCDropoutModel(nn.Module):
    """
    Model with MC Dropout for uncertainty estimation.
    """
    def predict_with_uncertainty(self, x, n_samples=30):
        """
        Args:
            x: Input tensor (B, C, H, W)
            n_samples: Number of MC sampling iterations
        
        Returns:
            mean: (B, num_classes) - average prediction
            uncertainty: (B, num_classes) - epistemic uncertainty
        """
        # Set model to train mode so dropout is activated
        self.train()
        
        predictions = []
        with torch.no_grad():
            for _ in range(n_samples):
                pred = torch.sigmoid(self(x))
                predictions.append(pred)
        
        predictions = torch.stack(predictions)  # (n_samples, B, C)
        
        mean = predictions.mean(dim=0)
        uncertainty = predictions.std(dim=0)  # epistemic uncertainty
        
        return mean, uncertainty

# Usage
model_mc = MCDropoutModel(base_model).to(device)

# Prediction with confidence
images = test_batch.to(device)
predictions, uncertainties = model_mc.predict_with_uncertainty(images, n_samples=30)

# Samples with high uncertainty (need doctor review)
high_uncertainty_mask = uncertainties.max(dim=1)[0] > 0.15
uncertain_samples = images[high_uncertainty_mask]
\end{lstlisting}

%==============================================================================
\section{\textbf{Kết luận và khuyến nghị cho các cải tiến (Conclusion)}}
\label{sec:improve_conclusion}

\subsection{\textbf{Tóm tắt các kết quả chính}}

\begin{enumerate}
    \item \textbf{Transfer Learning}: Là cải tiến quan trọng nhất, giúp tăng khoảng 3--4\% AUC và giảm một nửa thời gian huấn luyện.
    \item \textbf{Loss Functions}: Focal Loss và ASL cải thiện rõ rệt hiệu suất trên các lớp hiếm mà không tăng đáng kể thời gian huấn luyện.
    \item \textbf{Swin Transformer}: Vượt ViT truyền thống với số lượng tham số ít hơn và tốc độ suy luận nhanh hơn.
    \item \textbf{Multi-scale}: Tăng khoảng 4--6\% AUC cho các bệnh có tính chất đa tỷ lệ (large/medium/small scale).
    \item \textbf{Ensemble}: Đạt AUC khoảng 0.917, tiệm cận SOTA trên tập dữ liệu \Dataset.
\end{enumerate}

\subsection{\textbf{Best practices tổng hợp}}

\begin{itemize}
    \item \textbf{Luôn ưu tiên} sử dụng pre-trained weights (ImageNet hoặc pre-training tự giám sát) nếu có thể.
    \item Ưu tiên sử dụng \textbf{Focal Loss} hoặc ASL cho các bài toán multi-label với dữ liệu mất cân bằng mạnh.
    \item Áp dụng \textbf{tăng cường dữ liệu thận trọng} cho ảnh y khoa, tránh các biến đổi làm sai lệch cấu trúc giải phẫu.
    \item Sử dụng \textbf{multi-scale} hoặc các kiến trúc phân cấp (Swin, FPN, v.v.) cho các bệnh có kích thước và hình thái rất đa dạng.
    \item \textbf{Ensemble} các mô hình mạnh (Swin, ViT, EfficientNet) là lựa chọn phù hợp cho hệ thống triển khai thực tế, khi chi phí tính toán cho inference cho phép.
\end{itemize}


	% ============================================================
% CHAPTER: CONCLUSION
% ============================================================
\chapter{Kết luận (Conclusion)}
\label{chap:conclusion}

\section{\textbf{Tóm tắt tổng quan}}

Báo cáo này phân tích toàn diện ba kiến trúc mạng học sâu phổ biến — \textbf{CNN}, \textbf{ResNet}, và \textbf{Vision Transformer (ViT)} — áp dụng cho bài toán \textbf{phân loại và phát hiện bệnh lý trên ảnh X-quang ngực (với khả năng phát hiện nhiều bệnh cùng lúc)}.  
Khác với các nghiên cứu chỉ báo cáo con số kết quả, báo cáo này kết hợp \textbf{tái triển khai bài báo gốc} với \textbf{phân tích chi tiết kiến trúc}, để trả lời những câu hỏi quan trọng:

\begin{itemize}[noitemsep]
    \item Vì sao một số kiến trúc hoạt động tốt hơn trong bối cảnh dữ liệu y tế?
    \item Vai trò của inductive bias đối với hiệu năng và khả năng tổng quát hóa là gì?
    \item Vision Transformer có thực sự vượt trội, hay chỉ hiệu quả khi có pretraining phù hợp?
\end{itemize}

Thông qua việc tái triển khai toàn bộ pipeline, sửa các vấn đề thực nghiệm quan trọng và đánh giá nghiêm ngặt, nghiên cứu đã cung cấp một góc nhìn đáng tin cậy và mang tính phản biện đối với kết luận của bài báo gốc.

%----------------------------------------------------------
\section{\textbf{Những phát hiện thực nghiệm chính}}

Kết quả thực nghiệm cho thấy sự khác biệt rõ ràng giữa các kiến trúc:

\begin{enumerate}
    \item \textbf{Vision Transformer huấn luyện trước đạt hiệu suất cao nhất} (tính bằng AUC-ROC), điều này chứng minh Transformer có thể dùng hiệu quả cho hình ảnh y tế — \emph{nhưng chỉ khi} được huấn luyện từ trước trên dữ liệu khổng lồ.
    
    \item \textbf{ResNet-34 cân bằng tốt} giữa độ phức tạp của mô hình và khả năng học được trên dữ liệu khác, đặc biệt phù hợp với các bộ dữ liệu y tế có quy mô trung bình.
    
    \item \textbf{Vision Transformer huấn luyện từ đầu thất bại hoàn toàn}, hiệu suất gần như guess ngẫu nhiên, cho thấy thiếu các thiên lệch quy nạp phù hợp khiến học tập rất khó.
    
    \item \textbf{CNN baseline không được thiết kế tối ưu}, dẫn tới overfitting (học vẹt dữ liệu huấn luyện), chứng minh rằng \emph{kiến trúc} — không phải chỉ số lượng tham số — là yếu tố quyết định.
\end{enumerate}

Những kết quả này củng cố nhận định rằng \textbf{hiệu năng cao không chỉ đến từ mô hình lớn hơn}, mà từ sự phù hợp giữa kiến trúc, dữ liệu và chiến lược huấn luyện.

%----------------------------------------------------------
\section{\textbf{Nhận định về kiến trúc và Ý nghĩa lý thuyết}}

Một đóng góp quan trọng của nghiên cứu là làm rõ vai trò của \textbf{inductive bias} (thiên lệch quy nạp) trong học sâu cho ảnh y tế:

\begin{itemize}[noitemsep]
    \item \textbf{CNN và ResNet} tận dụng mạnh mẽ tính cục bộ (locality), tính tương đương dịch chuyển (translation equivariance) và học đặc trưng phân cấp — những đặc tính phù hợp tự nhiên với cấu trúc giải phẫu của ảnh X-quang.
    
    \item \textbf{Vision Transformer}, ngược lại, gần như không có inductive bias về không gian, buộc mô hình phải học mọi quan hệ từ dữ liệu, dẫn đến nhu cầu dữ liệu và pretraining rất lớn.
    
    \item \textbf{Kiến trúc lai (CNN–Transformer)} hoặc Transformer pretrained giúp dung hòa hai thế giới: vừa giữ được inductive bias cục bộ, vừa khai thác khả năng suy luận toàn cục của cơ chế attention.
\end{itemize}

Từ góc nhìn lý thuyết, nghiên cứu này ủng hộ quan điểm rằng:
\begin{quote}
\textit{``Transformer không thay thế CNN trong thị giác máy tính, mà mở rộng khả năng biểu diễn khi được đặt trên một nền tảng inductive bias phù hợp.''}
\end{quote}

%----------------------------------------------------------
\section{\textbf{Đóng góp về mặt phương pháp luận (Methodological Contributions)}}

Ngoài kết quả mô hình, nghiên cứu còn đóng góp quan trọng về mặt phương pháp:

\begin{enumerate}
    \item \textbf{Chuẩn hoá đánh giá thực nghiệm}: Chia dữ liệu theo \textbf{Patient ID} thay vì image-level, loại bỏ hiện tượng rò rỉ dữ liệu (data leakage) — một vấn đề phổ biến nhưng thường bị bỏ qua trong nhiều tài liệu.
    
    \item \textbf{Đánh giá đúng bản chất multi-label}: Sử dụng hàm sigmoid kết hợp Binary Cross-Entropy và chỉ số Macro-AUC thay cho accuracy hoặc các metric dựa trên softmax.
    
    \item \textbf{Tái triển khai có kiểm soát (Replication)}: Tái hiện lại bài báo gốc trong một framework hiện đại (PyTorch), chỉ ra những khác biệt quan trọng giữa kết quả báo cáo và kết quả khi được đánh giá nghiêm ngặt.
    
    \item \textbf{Phân tích sâu theo kiến trúc}: Không chỉ dừng lại ở việc so sánh con số, mà còn lý giải nguyên nhân sâu xa dẫn đến sự thành công hoặc thất bại của từng mô hình.
\end{enumerate}

Những yếu tố này giúp kết quả nghiên cứu có giá trị tham khảo cao cho các công trình nghiên cứu tiếp theo trong lĩnh vực này.

%----------------------------------------------------------
\section{\textbf{Ý nghĩa lâm sàng và thực tiễn (Clinical and Practical Implications)}}

Từ góc nhìn ứng dụng thực tế trong y tế, nghiên cứu cho thấy:

\begin{itemize}[noitemsep]
    \item Các mô hình học sâu trong nghiên cứu này \textbf{phù hợp nhất với vai trò hệ thống hỗ trợ quyết định} (Computer-Aided Diagnosis - CAD), chứ không phải để thay thế hoàn toàn bác sĩ.
    
    \item Chỉ số AUC-ROC cao cho phép mô hình được sử dụng như một công cụ \textbf{lọc và ưu tiên các ca nghi ngờ}, giúp giảm tải đáng kể cho hệ thống y tế và rút ngắn thời gian chẩn đoán.
    
    \item \textbf{Attention maps} và các phương pháp giải thích mô hình (explainability) là yếu tố then chốt để xây dựng độ tin cậy khi triển khai thực tế trong môi trường lâm sàng.
\end{itemize}

Tuy nhiên, việc triển khai chính thức yêu cầu thêm các bước như hiệu chuẩn xác suất (probability calibration), đánh giá đa trung tâm (multi-center evaluation) và kiểm chứng lâm sàng bởi các chuyên gia y tế đầu ngành.

%----------------------------------------------------------
\section{\textbf{Hạn chế của nghiên cứu (Limitations)}}

Mặc dù đạt được nhiều kết quả có ý nghĩa, nghiên cứu vẫn tồn tại một số hạn chế nhất định:

\begin{enumerate}
    \item Chỉ đánh giá trên một bộ dữ liệu duy nhất (NIH Chest X-ray14), chưa kiểm chứng khả năng tổng quát hóa trên các nguồn dữ liệu đa trung tâm khác.
    
    \item Nhãn bệnh mang tính \emph{weakly supervised}, trích xuất bằng NLP nên có thể chứa nhiễu từ báo cáo gốc.
    
    \item Chưa đánh giá khả năng khu trú vùng bệnh (localization) do thiếu dữ liệu bounding box chuẩn.
    
    \item Chưa thực hiện tìm kiếm siêu tham số (hyperparameter search) quy mô lớn do giới hạn về tài nguyên tính toán.
\end{enumerate}

Những hạn chế này chính là động lực mở ra các hướng nghiên cứu tiếp theo.

%----------------------------------------------------------
\section{\textbf{Hướng phát triển trong tương lai (Future Directions)}}

Dựa trên các kết quả và phân tích, một số hướng phát triển tiềm năng bao gồm:

\begin{itemize}[noitemsep]
    \item \textbf{Xác thực đa tập dữ liệu (Multi-dataset validation)}: Đánh giá mô hình trên các bộ dữ liệu khác như CheXpert, MIMIC-CXR, PadChest để kiểm tra tính ổn định.
    \item \textbf{Học tự giám sát (Self-supervised learning)}: Áp dụng MAE, DINO, hoặc masked image modeling chuyên biệt cho ảnh y khoa để tận dụng dữ liệu không gán nhãn.
    \item \textbf{Kiến trúc lai thế hệ mới}: Thử nghiệm các mô hình CNN–Transformer tiên tiến như ConvNeXt, CvT, hoặc Swin Transformer.
    \item \textbf{Giải thích mô hình nâng cao}: Kết hợp cơ chế attention với Grad-CAM++ và dự đoán độ bất định (uncertainty estimation).
    \item \textbf{Học đa phương thức (Multi-modal learning)}: Kết hợp ảnh X-quang với ghi chú lâm sàng và dữ liệu xét nghiệm của bệnh nhân để nâng cao độ chính xác.
\end{itemize}

%----------------------------------------------------------
\section{\textbf{Lời kết (Final Remarks)}}

Kết luận chung của toàn bộ nghiên cứu này là:  
\begin{quote}
\textit{``Không tồn tại một kiến trúc tối ưu tuyệt đối cho mọi bài toán. Hiệu quả của mô hình phụ thuộc mật thiết vào sự tương thích giữa kiến trúc, đặc thù dữ liệu và mục tiêu ứng dụng cụ thể.''}
\end{quote}

Vision Transformer không phải là một ``chiếc đũa thần'' (silver bullet), nhưng khi được kết hợp với chiến lược pretraining mạnh mẽ hoặc các thiên lệch quy nạp (inductive bias) phù hợp, chúng mở ra những tiềm năng mới to lớn cho lĩnh vực phân tích ảnh y tế. Nghiên cứu này kỳ vọng sẽ đóng góp một nền tảng tri thức vững chắc cho các công trình nghiên cứu chuyên sâu tiếp theo trong lĩnh vực giao thoa giữa học sâu và y học.

% ============================================================
% REFERENCES (Bibliography)
% ============================================================
\begin{thebibliography}{99}

\bibitem{wang2017chestxray14}
Wang X, Peng Y, Lu L, et al.
\textit{ChestX-ray8: Hospital-scale chest X-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases}.
IEEE CVPR 2017.

\bibitem{rajpurkar2017chexnet}
Rajpurkar P, Irvin J, Zhu K, et al.
\textit{CheXNet: Radiologist-level pneumonia detection on chest X-rays with deep learning}.
arXiv:1711.05225, 2017.

\bibitem{irvin2019chexpert}
Irvin J, Rajpurkar P, Ko M, et al.
\textit{CheXpert: A large chest radiograph dataset with uncertainty labels and expert comparison}.
AAAI 2019.

\bibitem{johnson2019mimic}
Johnson AEW, Pollard TJ, Berkowitz SJ, et al.
\textit{MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports}.
Scientific Data 2019; 6:317.

\bibitem{dosovitskiy2020vit}
Dosovitskiy A, Beyer L, Kolesnikov A, et al.
\textit{An image is worth 16x16 words: Transformers for image recognition at scale}.
ICLR 2021.

\bibitem{liu2021swin}
Liu Z, Lin Y, Cao Y, et al.
\textit{Swin Transformer: Hierarchical vision transformer using shifted windows}.
ICCV 2021.

\bibitem{xu2021vitae}
Xu Y, Zhang Q, Zhang J, Tao D.
\textit{ViTAE: Vision transformer advanced by exploring intrinsic inductive bias}.
NeurIPS 2021.

\bibitem{lin2017focal}
Lin TY, Goyal P, Girshick R, He K, Dollár P.
\textit{Focal loss for dense object detection}.
ICCV 2017.

\bibitem{ridnik2021asymmetric}
Ridnik T, Ben-Baruch E, Noy A, Zelnik-Manor L.
\textit{Asymmetric loss for multi-label classification}.
ICCV 2021.

\bibitem{cui2019classbalanced}
Cui Y, Jia M, Lin TY, Song Y, Belongie S.
\textit{Class-balanced loss based on effective number of samples}.
CVPR 2019.

\bibitem{selvaraju2017gradcam}
Selvaraju RR, Cogswell M, Das A, et al.
\textit{Grad-CAM: Visual explanations from deep networks via gradient-based localization}.
ICCV 2017.

\bibitem{gal2016dropout}
Gal Y, Ghahramani Z.
\textit{Dropout as a Bayesian approximation: Representing model uncertainty in deep learning}.
ICML 2016.

\bibitem{matsoukas2021medical}
Matsoukas C, Haslum JF, Söderberg M, Smith K.
\textit{Is it time to replace CNNs with transformers for medical images?}
arXiv:2108.09038, 2021.

\bibitem{jain2024comparative}
Jain A, Bhardwaj A, Murali K, Surani I.
\textit{A comparative study of CNN, ResNet, and vision transformers for multi-classification of chest diseases}.
arXiv:2406.00237, 2024.

\end{thebibliography}

\end{document}
