# PhÃ¢n TÃ­ch Chi Tiáº¿t: Vision Transformer V1

## ğŸ“‹ Tá»•ng Quan

File `ViT-v1.ipynb` triá»ƒn khai **Vision Transformer (ViT)** tá»« Ä‘áº§u - má»™t kiáº¿n trÃºc Ä‘á»™t phÃ¡ Ã¡p dá»¥ng Transformer (vá»‘n dÃ¹ng cho NLP) vÃ o Computer Vision. ViT chia áº£nh thÃ nh cÃ¡c patches vÃ  xá»­ lÃ½ chÃºng nhÆ° sequence tokens.

---

## ğŸ¯ Vision Transformer Concept

### Ã tÆ°á»Ÿng chÃ­nh:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VISION TRANSFORMER                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  "An image is worth 16x16 words"                               â”‚
â”‚                                                                 â”‚
â”‚  Thay vÃ¬ convolution, ViT:                                     â”‚
â”‚  1. Chia áº£nh thÃ nh patches (16Ã—16 hoáº·c 32Ã—32)                  â”‚
â”‚  2. Flatten má»—i patch thÃ nh vector                             â”‚
â”‚  3. Ãp dá»¥ng Transformer encoder                                â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Cell 1-4: Setup

```python
# Standard imports + data loading + device config
%run data.ipynb
```

---

## ğŸ”§ Cell 5: MLP Block

```python
def mlp(x, hidden_units, dropout_rate):
    for units in hidden_units:
        x = Dense(units, activation=tf.nn.gelu)(x)
        x = Dropout(dropout_rate)(x)
    return x
```

### GELU Activation:

$$\text{GELU}(x) = x \cdot \Phi(x) = x \cdot \frac{1}{2}[1 + \text{erf}(x/\sqrt{2})]$$

```
     GELU vs ReLU
     â”‚
  1  â”‚        ___GELU___
     â”‚      /
     â”‚     /    ___ReLU
     â”‚    /   /
  0  â”œâ”€â”€â”€/â”€â”€â”€/â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     â”‚  /
     â”‚ /
 -1  â”‚/
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       -2  -1   0   1   2
```

**Táº¡i sao GELU?**
- Smooth approximation cá»§a ReLU
- Non-zero gradient cho negative inputs
- ÄÆ°á»£c dÃ¹ng trong BERT, GPT, ViT

---

## ğŸ”§ Cell 6: Patches Layer

```python
class Patches(layers.Layer):
    def __init__(self, patch_size):
        super(Patches, self).__init__()
        self.patch_size = patch_size

    def call(self, images):
        batch_size = tf.shape(images)[0]
        patches = tf.image.extract_patches(
            images=images,
            sizes=[1, self.patch_size, self.patch_size, 1],
            strides=[1, self.patch_size, self.patch_size, 1],
            rates=[1, 1, 1, 1],
            padding='VALID',
        )
        patch_dims = patches.shape[-1]
        patches = tf.reshape(patches, [batch_size, -1, patch_dims])
        return patches
```

### Patch Extraction Visualization:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INPUT IMAGE (224 Ã— 224 Ã— 3)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”                  â”‚
â”‚   â”‚  1  â”‚  2  â”‚  3  â”‚  4  â”‚  5  â”‚  6  â”‚  7  â”‚                  â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤                  â”‚
â”‚   â”‚  8  â”‚  9  â”‚ 10  â”‚ 11  â”‚ 12  â”‚ 13  â”‚ 14  â”‚                  â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤                  â”‚
â”‚   â”‚ 15  â”‚ 16  â”‚ 17  â”‚ 18  â”‚ 19  â”‚ 20  â”‚ 21  â”‚   patch_size=32  â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤   â†’ 7Ã—7 = 49     â”‚
â”‚   â”‚ 22  â”‚ 23  â”‚ 24  â”‚ 25  â”‚ 26  â”‚ 27  â”‚ 28  â”‚     patches      â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤                  â”‚
â”‚   â”‚ 29  â”‚ 30  â”‚ 31  â”‚ 32  â”‚ 33  â”‚ 34  â”‚ 35  â”‚                  â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤                  â”‚
â”‚   â”‚ 36  â”‚ 37  â”‚ 38  â”‚ 39  â”‚ 40  â”‚ 41  â”‚ 42  â”‚                  â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤                  â”‚
â”‚   â”‚ 43  â”‚ 44  â”‚ 45  â”‚ 46  â”‚ 47  â”‚ 48  â”‚ 49  â”‚                  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                                                 â”‚
â”‚   Má»—i patch: 32 Ã— 32 Ã— 3 = 3,072 dimensions                    â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### TÃ­nh toÃ¡n:

```python
image_size = 224
patch_size = 32
num_patches = (224 // 32) ** 2 = 7 Ã— 7 = 49 patches
patch_dims = 32 Ã— 32 Ã— 3 = 3,072
```

---

## ğŸ”§ Cell 7: Patch Visualization

```python
for image_batch, label_batch in train_generator:
    image = image_batch[0] 
    break 

patches = Patches(patch_size)(tf.expand_dims(image, 0))

# Visualize patches in grid
n = int(np.sqrt(num_patches))  # 7
for i in range(n * n):
    patch_img = patches_numpy[0, i].reshape(patch_size, patch_size, 3)
    plt.subplot(n, n, i + 1)
    plt.imshow(patch_img)
```

### Output:

```
Original Image â†’ 7Ã—7 Grid of Patches
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
â”‚             â”‚    â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚
â”‚    Chest    â”‚ â†’  â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚    X-ray    â”‚    â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚
â”‚             â”‚    â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚   â”‚ â¬›â”‚ â¬›â”‚   â”‚   â”‚   â”‚ â† Lung regions
                   â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜
```

---

## ğŸ”§ Cell 8: Patch Encoder

```python
class PatchEncoder(layers.Layer):
    def __init__(self, num_patches, projection_dim):
        super().__init__()
        self.num_patches = num_patches
        self.projection = layers.Dense(units=projection_dim)
        self.position_embedding = layers.Embedding(
            input_dim=num_patches, output_dim=projection_dim
        )

    def call(self, patch):
        positions = ops.expand_dims(
            ops.arange(start=0, stop=self.num_patches, step=1), axis=0
        )
        projected_patches = self.projection(patch)
        encoded = projected_patches + self.position_embedding(positions)
        return encoded
```

### Patch Encoding Process:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     PATCH ENCODER                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Input: Flattened patches (batch, 49, 3072)                    â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚           Linear Projection (Dense)                  â”‚       â”‚
â”‚  â”‚           3072 â†’ projection_dim (64)                â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                             â”‚                                   â”‚
â”‚                             â–¼                                   â”‚
â”‚         Projected patches: (batch, 49, 64)                      â”‚
â”‚                             â”‚                                   â”‚
â”‚                             â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚                             â”‚     â”‚ Position Embedding â”‚       â”‚
â”‚                             â”‚     â”‚ (49, 64)           â”‚       â”‚
â”‚                             â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                             â”‚               â”‚                   â”‚
â”‚                             â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                     â”‚                           â”‚
â”‚                                     â–¼                           â”‚
â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚                              â”‚   ADD   â”‚                        â”‚
â”‚                              â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                        â”‚
â”‚                                   â”‚                             â”‚
â”‚                                   â–¼                             â”‚
â”‚              Output: (batch, 49, 64) with positional info       â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Position Embedding:

```
Patch   Position    Embedding Vector (64-dim)
  1        0        [0.12, -0.34, 0.56, ...]
  2        1        [0.23, -0.45, 0.67, ...]
  3        2        [0.34, -0.56, 0.78, ...]
  ...
  49       48       [0.89, -0.12, 0.34, ...]
```

**Táº¡i sao cáº§n Position Embedding?**
- Transformer khÃ´ng cÃ³ khÃ¡i niá»‡m vá» thá»© tá»±
- Cáº§n encode vá»‹ trÃ­ cá»§a má»—i patch trong áº£nh

---

## ğŸ”§ Cell 9: Hyperparameters

```python
input_shape = (224, 224, 3)  
patch_size = 32             
num_patches = (224 // 32) ** 2  = 49
projection_dim = 64          
num_heads = 4                
transformer_units = [128, 64]  # projection_dim * 2, projection_dim
transformer_layers = 8       
mlp_head_units = [2048, 1024]  
num_classes = 15
```

### Giáº£i thÃ­ch:

| Parameter | Value | Meaning |
|-----------|-------|---------|
| `patch_size` | 32 | KÃ­ch thÆ°á»›c má»—i patch |
| `num_patches` | 49 | Sá»‘ patches = sequence length |
| `projection_dim` | 64 | Embedding dimension |
| `num_heads` | 4 | Multi-head attention heads |
| `transformer_layers` | 8 | Sá»‘ Transformer blocks |
| `mlp_head_units` | [2048, 1024] | Classification head |

### So sÃ¡nh vá»›i ViT gá»‘c:

| Config | ViT-Base | This Model |
|--------|----------|------------|
| patch_size | 16 | 32 |
| num_patches | 196 | 49 |
| projection_dim | 768 | 64 |
| num_heads | 12 | 4 |
| transformer_layers | 12 | 8 |
| Parameters | 86M | ~3M |

â†’ ÄÃ¢y lÃ  **ViT-Tiny** version!

---

## ğŸ”§ Cell 10: ViT Classifier

```python
def create_vit_classifier():
    inputs = Input(shape=input_shape)
    
    # Patch extraction + encoding
    patches = Patches(patch_size)(inputs)
    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)

    # Transformer blocks
    for _ in range(transformer_layers):
        # Layer Normalization 1
        x1 = LayerNormalization(epsilon=1e-6)(encoded_patches)
        
        # Multi-Head Self-Attention
        attention_output = MultiHeadAttention(
            num_heads=num_heads, key_dim=projection_dim, dropout=0.1
        )(x1, x1)
        
        # Skip connection 1
        x2 = Add()([attention_output, encoded_patches])
        
        # Layer Normalization 2
        x3 = LayerNormalization(epsilon=1e-6)(x2)
        
        # MLP
        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)
        
        # Skip connection 2
        encoded_patches = Add()([x3, x2])

    # Classification head
    representation = LayerNormalization(epsilon=1e-6)(encoded_patches)
    representation = Flatten()(representation)
    representation = Dropout(0.5)(representation)
    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)
    logits = Dense(num_classes, activation='sigmoid')(features)
    
    model = Model(inputs=inputs, outputs=logits)
    return model
```

### Kiáº¿n trÃºc Ä‘áº§y Ä‘á»§:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VISION TRANSFORMER                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  INPUT: (224, 224, 3)                                          â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ PATCH EMBEDDING                                          â”‚   â”‚
â”‚  â”‚   Patches(32) â†’ (49, 3072)                              â”‚   â”‚
â”‚  â”‚   PatchEncoder â†’ (49, 64)                               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                             â”‚                                   â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚         â”‚      TRANSFORMER ENCODER Ã— 8          â”‚              â”‚
â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚              â”‚
â”‚         â”‚  â”‚ LayerNorm                       â”‚  â”‚              â”‚
â”‚         â”‚  â”‚         â”‚                       â”‚  â”‚              â”‚
â”‚         â”‚  â”‚         â–¼                       â”‚  â”‚              â”‚
â”‚         â”‚  â”‚ Multi-Head Self-Attention       â”‚  â”‚              â”‚
â”‚         â”‚  â”‚ (4 heads, key_dim=64)           â”‚  â”‚              â”‚
â”‚         â”‚  â”‚         â”‚                       â”‚  â”‚              â”‚
â”‚         â”‚  â”‚         â–¼                       â”‚  â”‚              â”‚
â”‚         â”‚  â”‚    Add (Skip Connection)        â”‚  â”‚              â”‚
â”‚         â”‚  â”‚         â”‚                       â”‚  â”‚              â”‚
â”‚         â”‚  â”‚         â–¼                       â”‚  â”‚              â”‚
â”‚         â”‚  â”‚ LayerNorm                       â”‚  â”‚              â”‚
â”‚         â”‚  â”‚         â”‚                       â”‚  â”‚              â”‚
â”‚         â”‚  â”‚         â–¼                       â”‚  â”‚              â”‚
â”‚         â”‚  â”‚ MLP [128 â†’ 64]                  â”‚  â”‚              â”‚
â”‚         â”‚  â”‚         â”‚                       â”‚  â”‚              â”‚
â”‚         â”‚  â”‚         â–¼                       â”‚  â”‚              â”‚
â”‚         â”‚  â”‚    Add (Skip Connection)        â”‚  â”‚              â”‚
â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚              â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                             â”‚                                   â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚         â”‚      CLASSIFICATION HEAD               â”‚              â”‚
â”‚         â”‚  LayerNorm â†’ Flatten â†’ Dropout(0.5)   â”‚              â”‚
â”‚         â”‚  MLP [2048 â†’ 1024] â†’ Dense(15)        â”‚              â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                             â”‚                                   â”‚
â”‚                             â–¼                                   â”‚
â”‚                    OUTPUT: (15,) probabilities                  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Multi-Head Self-Attention:

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               MULTI-HEAD SELF-ATTENTION                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Input X: (49, 64)                                             â”‚
â”‚      â”‚                                                          â”‚
â”‚      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚      â”‚          â”‚          â”‚          â”‚                        â”‚
â”‚      â–¼          â–¼          â–¼          â–¼                        â”‚
â”‚   Head 1     Head 2     Head 3     Head 4                      â”‚
â”‚   (49,16)    (49,16)    (49,16)    (49,16)                    â”‚
â”‚      â”‚          â”‚          â”‚          â”‚                        â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                     â”‚                                           â”‚
â”‚                     â–¼                                           â”‚
â”‚              Concatenate â†’ (49, 64)                            â”‚
â”‚                     â”‚                                           â”‚
â”‚                     â–¼                                           â”‚
â”‚              Linear projection                                  â”‚
â”‚                     â”‚                                           â”‚
â”‚                     â–¼                                           â”‚
â”‚              Output: (49, 64)                                   â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Cell 11-14: Training & Visualization

```python
batch_size = 32
learning_rate = 1e-4
weight_decay = 1e-6
num_epochs = 10

vit_classifier = create_vit_classifier()
history = run_experiment(vit_classifier)
plot_combined_history(history)
```

---

## ğŸ”§ Cell 15: Evaluation vá»›i ROC Curves

```python
# Predict on test set
predictions = vit_classifier.predict(test_generator)

# Evaluate
loss, test_accuracy, test_auc = vit_classifier.evaluate(test_generator)

# Plot ROC curves for each class
def plot_roc_curves(y_true, y_pred, num_classes, class_labels):
    for i in range(num_classes):
        fpr[i], tpr[i], _ = roc_curve(y_true[:, i], y_pred[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])
    # ... plot
```

### ROC Curve Interpretation:

```
       True Positive Rate (Sensitivity)
  1.0  â”‚         ___________
       â”‚        /           
       â”‚       /  AUC = 0.85 (Good)
       â”‚      /              
  0.5  â”‚     /    
       â”‚    /     
       â”‚   /      AUC = 0.5 (Random)
       â”‚  /      /
  0.0  â”‚â”€/â”€â”€â”€â”€â”€â”€/â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       0.0     0.5         1.0
            False Positive Rate
```

---

## ğŸ“Š ÄÃ¡nh GiÃ¡ Tá»•ng Thá»ƒ

### âœ… Äiá»ƒm máº¡nh:
1. âœ… Triá»ƒn khai ViT Ä‘áº§y Ä‘á»§ tá»« Ä‘áº§u
2. âœ… Patches extraction Ä‘Ãºng chuáº©n
3. âœ… Position embedding
4. âœ… Multi-head attention
5. âœ… Pre-norm architecture (LayerNorm trÆ°á»›c attention)
6. âœ… ROC curve evaluation

### âŒ Äiá»ƒm yáº¿u:

| Váº¥n Ä‘á» | Má»©c Ä‘á»™ | Giáº£i phÃ¡p |
|--------|--------|-----------|
| Thiáº¿u CLS token | ğŸŸ  High | Add learnable CLS token |
| Model quÃ¡ nhá» | ğŸŸ  High | Increase dimensions |
| DÃ¹ng Flatten thay vÃ¬ CLS | ğŸŸ¡ Medium | Use CLS for classification |
| KhÃ´ng cÃ³ pretrained weights | ğŸŸ  High | Use ViT pretrained on ImageNet |
| Missing `ops` import | ğŸ”´ Critical | Use `tf` hoáº·c `keras.ops` |

### âš ï¸ Bug: `ops` not defined

```python
positions = ops.expand_dims(...)  # âŒ ops khÃ´ng Ä‘Æ°á»£c import
```

**Fix:**
```python
from tensorflow import keras
positions = keras.ops.expand_dims(...)  # Keras 3
# hoáº·c
positions = tf.expand_dims(...)  # TensorFlow
```

---

## ğŸ’¡ Improved ViT vá»›i CLS Token:

```python
class ViTWithCLS(layers.Layer):
    def __init__(self, num_patches, projection_dim, **kwargs):
        super().__init__(**kwargs)
        self.cls_token = self.add_weight(
            shape=(1, 1, projection_dim),
            initializer='zeros',
            trainable=True,
            name='cls_token'
        )
        
    def call(self, patches):
        batch_size = tf.shape(patches)[0]
        cls_tokens = tf.broadcast_to(
            self.cls_token, 
            [batch_size, 1, self.projection_dim]
        )
        return tf.concat([cls_tokens, patches], axis=1)
```

---

## ğŸ“š ViT vs CNN Comparison

| Aspect | CNN | ViT |
|--------|-----|-----|
| **Inductive bias** | Strong (locality, translation equivariance) | Weak |
| **Data efficiency** | Better with small data | Needs large data |
| **Scalability** | Limited | Excellent |
| **Interpretability** | Feature maps | Attention maps |
| **Training cost** | Lower | Higher |

---

## ğŸ“š References

1. Dosovitskiy et al., "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", ICLR 2021
2. Vaswani et al., "Attention is All You Need", NeurIPS 2017
3. Original ViT implementation: https://github.com/google-research/vision_transformer
