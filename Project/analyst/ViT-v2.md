# PhÃ¢n TÃ­ch Chi Tiáº¿t: Vision Transformer V2 (Improved)

## ğŸ“‹ Tá»•ng Quan

File `ViT-v2.ipynb` lÃ  **phiÃªn báº£n cáº£i tiáº¿n** cá»§a ViT-v1 vá»›i cÃ¡c ká»¹ thuáº­t regularization vÃ  training nÃ¢ng cao hÆ¡n, bao gá»“m:
- L2 Regularization
- Early Stopping
- Learning Rate Scheduling
- Multiple Optimizer Options

---

## ğŸ”„ So sÃ¡nh V1 vs V2

| Feature | ViT-v1 | ViT-v2 |
|---------|--------|--------|
| L2 Regularization | âŒ | âœ… |
| Early Stopping | âŒ | âœ… |
| LR Scheduler | âŒ | âœ… |
| Multiple Optimizers | âŒ | âœ… |
| Dropout in MLP | 0.1 | 0.1 + L2 |
| Weight Decay | 1e-6 | 1e-5 |

---

## ğŸ”§ Cell 5: Improved MLP with L2 Regularization

```python
def mlp(x, hidden_units, dropout_rate, regularizer_rate=0.01):
    for units in hidden_units:
        x = Dense(units, activation=tf.nn.gelu, 
                  kernel_regularizer=l2(regularizer_rate))(x)
        x = Dropout(dropout_rate)(x)
    return x
```

### L2 Regularization:

$$\mathcal{L}_{total} = \mathcal{L}_{BCE} + \lambda \sum_{i} w_i^2$$

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    L2 REGULARIZATION EFFECT                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  KhÃ´ng cÃ³ L2:                  CÃ³ L2 (Î»=0.01):                 â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ Large weights   â”‚          â”‚ Small weights   â”‚              â”‚
â”‚  â”‚ w = [5, -8, 12] â”‚    â†’     â”‚ w = [0.5, -0.8] â”‚              â”‚
â”‚  â”‚ Overfitting!    â”‚          â”‚ Generalize!     â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                                 â”‚
â”‚  Effect: Penalize large weights â†’ simpler model                â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### âš ï¸ Váº¥n Ä‘á»:
- `l2` chÆ°a Ä‘Æ°á»£c import!

**Fix:**
```python
from tensorflow.keras.regularizers import l2
```

---

## ğŸ”§ Cell 9: Hyperparameters

```python
batch_size = 32
learning_rate = 1e-4
weight_decay = 1e-5  # TÄƒng tá»« 1e-6
num_epochs = 10
```

| Parameter | V1 | V2 | LÃ½ do |
|-----------|-----|-----|-------|
| `weight_decay` | 1e-6 | 1e-5 | Stronger regularization |

---

## ğŸ”§ Cell 10: Advanced Training Configuration

### Early Stopping:

```python
early_stopping_callback = EarlyStopping(
    monitor="val_accuracy",
    patience=3,
    restore_best_weights=False,  # âš ï¸ NÃªn lÃ  True!
)
```

### Early Stopping Visualization:

```
     Validation Accuracy
  â”‚
  â”‚                    ___
  â”‚              _____|   |
  â”‚         ____|         |
  â”‚    ____|              |
  â”‚___|                   |
  â”‚                       â””â”€ Stop here! (patience=3)
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    1  2  3  4  5  6  7  8  9  10  Epochs
                        â”‚
                        â””â”€ Best model at epoch 7
                           but returns epoch 10 weights!
```

### âš ï¸ Bug: `restore_best_weights=False`

**Váº¥n Ä‘á»**: KhÃ´ng restore weights tá»‘t nháº¥t sau khi stop!

**Fix:**
```python
early_stopping_callback = EarlyStopping(
    monitor="val_loss",  # Thay vÃ¬ val_accuracy
    patience=5,          # TÄƒng patience
    restore_best_weights=True,  # âœ… Quan trá»ng!
)
```

---

## ğŸ”§ Multiple Optimizers:

```python
optimizers = {
    "adam": keras.optimizers.Adam(learning_rate=learning_rate),
    "adamw": keras.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay),
    "sgd": keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True, weight_decay=weight_decay),
    "sgd_momentum": keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9),
    "adagrad": keras.optimizers.Adagrad(learning_rate=learning_rate),
    "rmsprop": keras.optimizers.RMSprop(learning_rate=learning_rate)
}

optimizer = optimizers["sgd"]  # Chá»n SGD
```

### So sÃ¡nh Optimizers:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    OPTIMIZER COMPARISON                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Optimizer     â”‚   Characteristics                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Adam          â”‚ Adaptive LR, fast convergence                 â”‚
â”‚                 â”‚ Good default choice                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   AdamW         â”‚ Adam + decoupled weight decay                 â”‚
â”‚                 â”‚ Better for Transformers                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   SGD           â”‚ Simple, good generalization                   â”‚
â”‚                 â”‚ Slower convergence                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   SGD+Momentum  â”‚ Accelerated SGD                               â”‚
â”‚                 â”‚ Reduces oscillation                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   SGD+Nesterov  â”‚ Look-ahead momentum                           â”‚
â”‚                 â”‚ Better for convex optimization                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   RMSprop       â”‚ Adaptive, good for RNNs                       â”‚
â”‚                 â”‚ Less used for vision                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### âš ï¸ Nháº­n xÃ©t:
- **Chá»n SGD** thay vÃ¬ AdamW lÃ  **khÃ´ng tá»‘i Æ°u** cho ViT
- ViT papers thÆ°á»ng dÃ¹ng **AdamW** vá»›i warmup

---

## ğŸ”§ Learning Rate Scheduler:

```python
lr_scheduler = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.1,
    patience=5,
    min_lr=1e-6,
    verbose=1
)
```

### LR Scheduling Visualization:

```
  Learning Rate
  â”‚
  1e-4 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                 â”‚
       â”‚                 â”‚ patience=5
       â”‚                 â”‚
  1e-5 â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                                   â”‚
       â”‚                                   â”‚
  1e-6 â”‚                                   â””â”€â”€â”€â”€â”€â”€ min_lr
       â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         0     5    10    15    20    25    Epochs
                    â”‚           â”‚
              val_loss     val_loss
              stagnates    stagnates
              (reduce!)    (reduce!)
```

### Parameters:

| Param | Value | Meaning |
|-------|-------|---------|
| `monitor` | 'val_loss' | Metric to track |
| `factor` | 0.1 | LR *= 0.1 when triggered |
| `patience` | 5 | Epochs to wait before reducing |
| `min_lr` | 1e-6 | Minimum learning rate |

---

## ğŸ”§ Cell 11: ViT Classifier with Regularization

```python
def create_vit_classifier():
    # ... (same as V1)
    for _ in range(transformer_layers):
        # ...
        x3 = mlp(x3, transformer_units, dropout_rate=0.1, regularizer_rate=0.01)
        # ...
    
    features = mlp(representation, mlp_head_units, dropout_rate=0.5, regularizer_rate=0.01)
    # ...
```

### Regularization Points:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  REGULARIZATION IN VIT-V2                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  TRANSFORMER BLOCK:                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚  â”‚  MultiHeadAttention(dropout=0.1)     â”‚ â† Dropout            â”‚
â”‚  â”‚           â”‚                          â”‚                      â”‚
â”‚  â”‚  MLP (L2=0.01, dropout=0.1)          â”‚ â† L2 + Dropout       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                                                                 â”‚
â”‚  CLASSIFICATION HEAD:                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚  â”‚  Flatten                             â”‚                      â”‚
â”‚  â”‚  Dropout(0.5)                        â”‚ â† Strong Dropout     â”‚
â”‚  â”‚  MLP (L2=0.01, dropout=0.5)          â”‚ â† L2 + Dropout       â”‚
â”‚  â”‚  Dense (sigmoid)                     â”‚                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Cell 12: Training Function

```python
def run_experiment(model):
    model.compile(
        optimizer=optimizer,  # SGD
        loss='binary_crossentropy',
        metrics=[
            keras.metrics.BinaryAccuracy(name="accuracy"),
            keras.metrics.AUC(name="auc"),
        ]
    )

    history = model.fit(
        train_generator,
        epochs=num_epochs,
        validation_data=validation_generator,
        callbacks=[
            ModelCheckpoint(...),
            early_stopping_callback,
            lr_scheduler
        ],
    )
    return history
```

### Callback Execution Order:

```
Each Epoch:
  1. Train on batches
  2. Validate
  3. ModelCheckpoint: Save if val_loss improved
  4. ReduceLROnPlateau: Check if should reduce LR
  5. EarlyStopping: Check if should stop
```

---

## ğŸ”§ Cell 13-17: Evaluation

```python
# Training
vit_classifier = create_vit_classifier()
history = run_experiment(vit_classifier)

# Visualization
plot_combined_history(history)

# Test evaluation
predictions = vit_classifier.predict(test_generator)
loss, test_accuracy, test_auc = vit_classifier.evaluate(test_generator)

# ROC Curves
plot_roc_curves(y_true, predictions, num_classes, class_labels)
```

---

## ğŸ“Š So sÃ¡nh Training Behavior

### V1 (No regularization):

```
Epoch   Train Loss   Val Loss   Status
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  1       0.50        0.48      OK
  2       0.35        0.45      OK
  3       0.20        0.50      Overfitting starts
  4       0.10        0.60      Overfitting
  5       0.05        0.75      Severe overfitting
```

### V2 (With regularization):

```
Epoch   Train Loss   Val Loss   LR       Status
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  1       0.50        0.48     1e-4      OK
  2       0.40        0.44     1e-4      OK
  3       0.35        0.42     1e-4      OK
  4       0.32        0.41     1e-4      OK
  5       0.30        0.40     1e-4      OK
  6       0.28        0.40     1e-5      LR reduced
  7       0.27        0.39     1e-5      OK
  8       0.26        0.39     1e-5      OK
  9       0.26        0.39     1e-5      Early stop
```

---

## ğŸ“Š ÄÃ¡nh GiÃ¡ Tá»•ng Thá»ƒ

### âœ… Äiá»ƒm máº¡nh so vá»›i V1:
1. âœ… L2 Regularization chá»‘ng overfitting
2. âœ… Early Stopping tiáº¿t kiá»‡m thá»i gian
3. âœ… LR Scheduler adaptive learning
4. âœ… Multiple optimizer options
5. âœ… ROC curve visualization

### âŒ Äiá»ƒm yáº¿u:

| Váº¥n Ä‘á» | Má»©c Ä‘á»™ | Giáº£i phÃ¡p |
|--------|--------|-----------|
| `l2` not imported | ğŸ”´ Critical | Add import |
| `restore_best_weights=False` | ğŸ”´ Critical | Set to True |
| SGD thay vÃ¬ AdamW | ğŸŸ  High | Use AdamW for ViT |
| `ops` not defined | ğŸ”´ Critical | Use tf/keras.ops |
| Missing warmup | ğŸŸ¡ Medium | Add LR warmup |

---

## ğŸ’¡ Optimal Training Configuration:

```python
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint

# Warmup + Cosine decay
def lr_schedule(epoch, lr):
    warmup_epochs = 5
    if epoch < warmup_epochs:
        return lr * (epoch + 1) / warmup_epochs
    return lr * 0.5 * (1 + np.cos(np.pi * (epoch - warmup_epochs) / (num_epochs - warmup_epochs)))

# Callbacks
callbacks = [
    EarlyStopping(
        monitor='val_loss',
        patience=10,
        restore_best_weights=True
    ),
    ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=5,
        min_lr=1e-7
    ),
    ModelCheckpoint(
        'best_model.keras',
        monitor='val_auc',
        mode='max',
        save_best_only=True
    ),
    keras.callbacks.LearningRateScheduler(lr_schedule)
]

# Use AdamW for ViT
optimizer = keras.optimizers.AdamW(
    learning_rate=1e-4,
    weight_decay=0.01,
    beta_1=0.9,
    beta_2=0.999
)
```

---

## ğŸ“Š Expected Performance Improvement

| Metric | V1 | V2 (expected) |
|--------|-----|---------------|
| Train Accuracy | 95% | 88% (less overfit) |
| Val Accuracy | 75% | 82% |
| Test AUC | 0.65 | 0.72 |
| Generalization Gap | 20% | 6% |

---

## ğŸ“š Best Practices for ViT Training

1. **Optimizer**: AdamW vá»›i weight decay 0.01-0.1
2. **LR Schedule**: Warmup + Cosine decay
3. **Regularization**: Dropout + Label smoothing + MixUp
4. **Data Augmentation**: RandAugment, AutoAugment
5. **Pretrained**: Khá»Ÿi táº¡o tá»« ImageNet-21k weights

---

## ğŸ“š References

1. "Training data-efficient image transformers" (DeiT), Touvron et al., 2021
2. "How to train your ViT?", Steiner et al., 2021
3. "An Image is Worth 16x16 Words", Dosovitskiy et al., 2020
