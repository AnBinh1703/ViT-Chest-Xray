% ============================================================
% CHAPTER 9: CONCLUSION
% ============================================================
\chapter{Conclusion}

\section{Summary of Findings}

\subsection{Paper Contributions}

\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=Original Paper Contributions]
\begin{enumerate}
    \item \textbf{Comprehensive Comparison:} So sánh 5 architectures (CNN, ResNet, ViT-v1, ViT-v2, ViT-ResNet) trên cùng một dataset và setup
    
    \item \textbf{Hybrid Architecture:} Đề xuất ViT-ResNet kết hợp ResNet backbone với Transformer encoder, đạt kết quả tốt nhất (93.9\% accuracy)
    
    \item \textbf{Medical Imaging Application:} Áp dụng Vision Transformer cho bài toán phân loại bệnh phổi từ X-quang
    
    \item \textbf{Multi-label Classification:} Xử lý bài toán multi-label với 15 classes bao gồm 14 bệnh và ``No Finding''
\end{enumerate}
\end{tcolorbox}

\subsection{Key Results}

\begin{table}[H]
\centering
\caption{Final Results Summary}
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Accuracy} & \textbf{AUC} & \textbf{Key Insight} \\
\midrule
CNN & 91\% & 0.82 & Baseline, limited capacity \\
ResNet-34 & 93\% & 0.86 & Skip connections help \\
ViT-v1/32 & 92.63\% & 0.86 & Transformers viable for medical \\
ViT-v2/32 & 92.83\% & 0.84 & More epochs, risk of overfit \\
\textbf{ViT-ResNet/16} & \textbf{93.9\%} & \textbf{0.85} & \textbf{Hybrid is best} \\
\bottomrule
\end{tabular}
\end{table}

\section{Technical Insights}

\subsection{Architecture Comparison}

\begin{tcolorbox}[colback=green!5!white,colframe=green!75!black,title=Architecture Insights]
\textbf{CNN (Baseline):}
\begin{itemize}
    \item Strong locality inductive bias
    \item Limited receptive field
    \item Simple but effective for basic features
    \item Best for: Resource-constrained deployment
\end{itemize}

\textbf{ResNet:}
\begin{itemize}
    \item Skip connections solve degradation problem
    \item Enables deeper networks (34+ layers)
    \item Good balance of performance and complexity
    \item Best for: General-purpose medical imaging
\end{itemize}

\textbf{Vision Transformer:}
\begin{itemize}
    \item Global attention captures long-range dependencies
    \item Minimal inductive bias, needs more data
    \item Highly scalable with data and compute
    \item Best for: Large-scale datasets with compute resources
\end{itemize}

\textbf{ViT-ResNet (Hybrid):}
\begin{itemize}
    \item Combines CNN's locality with Transformer's globality
    \item ResNet extracts local features, Transformer reasons globally
    \item Best of both worlds
    \item Best for: Medical imaging with limited data
\end{itemize}
\end{tcolorbox}

\subsection{Key Technical Lessons}

\begin{enumerate}
    \item \textbf{Inductive Bias Matters:}
    \begin{itemize}
        \item Medical images benefit from locality bias (pathologies are local)
        \item But global context helps (correlating regions)
        \item Hybrid approaches leverage both
    \end{itemize}
    
    \item \textbf{Skip Connections are Crucial:}
    \begin{itemize}
        \item ResNet's +2\% over CNN from skip connections alone
        \item Enable gradient flow in deep networks
        \item Allow model to preserve important features
    \end{itemize}
    
    \item \textbf{Attention Mechanisms Work for Vision:}
    \begin{itemize}
        \item Self-attention can learn spatial relationships
        \item Multi-head attention captures diverse patterns
        \item Positional embeddings recover spatial structure
    \end{itemize}
    
    \item \textbf{Multi-label Requires Careful Handling:}
    \begin{itemize}
        \item BCEWithLogitsLoss for independent class predictions
        \item AUC more informative than accuracy
        \item Class imbalance significantly affects results
    \end{itemize}
\end{enumerate}

\section{Our Contributions}

\subsection{PyTorch Reimplementation}

\begin{tcolorbox}[colback=purple!5!white,colframe=purple!75!black,title=Our Technical Contributions]
\begin{enumerate}
    \item \textbf{Complete PyTorch Migration:}
    \begin{itemize}
        \item All 5 models reimplemented from TensorFlow/Keras
        \item Consistent code style and documentation
        \item Modular, reusable components
    \end{itemize}
    
    \item \textbf{Bug Fixes:}
    \begin{itemize}
        \item AUC NaN issue: Handle classes with single label
        \item ReduceLROnPlateau verbose deprecation
        \item Data loading edge cases
    \end{itemize}
    
    \item \textbf{Documentation:}
    \begin{itemize}
        \item Comprehensive LaTeX report (this document)
        \item Code comments and docstrings
        \item Mapping between paper descriptions and code
    \end{itemize}
    
    \item \textbf{Analysis:}
    \begin{itemize}
        \item Deep expert-level analysis of each architecture
        \item Mathematical foundations explained
        \item Visual diagrams for understanding
    \end{itemize}
\end{enumerate}
\end{tcolorbox}

\section{Limitations and Future Work}

\subsection{Current Limitations}

\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Limitations]
\begin{enumerate}
    \item \textbf{Dataset Issues:}
    \begin{itemize}
        \item Weak labels from NLP extraction
        \item Severe class imbalance
        \item No patient-level split verification
    \end{itemize}
    
    \item \textbf{Experimental Scope:}
    \begin{itemize}
        \item Single dataset (NIH Chest X-ray)
        \item No external validation
        \item Limited hyperparameter search
    \end{itemize}
    
    \item \textbf{Clinical Applicability:}
    \begin{itemize}
        \item Not validated by radiologists
        \item No interpretability analysis
        \item Single modality (frontal X-ray only)
    \end{itemize}
\end{enumerate}
\end{tcolorbox}

\subsection{Future Directions}

\begin{tcolorbox}[colback=green!5!white,colframe=green!75!black,title=Future Work Suggestions]
\begin{enumerate}
    \item \textbf{Architecture Improvements:}
    \begin{itemize}
        \item Swin Transformer (hierarchical, efficient)
        \item DeiT (data-efficient training)
        \item ConvNeXt (modernized CNN)
    \end{itemize}
    
    \item \textbf{Training Improvements:}
    \begin{itemize}
        \item Self-supervised pretraining on chest X-rays
        \item Multi-task learning with segmentation
        \item Knowledge distillation from larger models
    \end{itemize}
    
    \item \textbf{Clinical Integration:}
    \begin{itemize}
        \item Attention visualization for interpretability
        \item Uncertainty quantification
        \item Integration with clinical workflows
    \end{itemize}
    
    \item \textbf{Dataset Expansion:}
    \begin{itemize}
        \item CheXpert, MIMIC-CXR for validation
        \item Multi-institutional evaluation
        \item Prospective clinical studies
    \end{itemize}
\end{enumerate}
\end{tcolorbox}

\section{Final Remarks}

\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=Conclusion]
Paper ``A Comparative Study of CNN, ResNet, and Vision Transformers for Multi-Classification of Chest Diseases'' đã:

\begin{enumerate}
    \item \textbf{Chứng minh} Vision Transformers có thể áp dụng hiệu quả cho medical imaging
    
    \item \textbf{Cho thấy} hybrid approaches (ViT-ResNet) kết hợp ưu điểm của cả CNN và Transformer
    
    \item \textbf{Cung cấp} benchmark cho 5 architectures trên NIH Chest X-ray dataset
\end{enumerate}

\textbf{Đóng góp của chúng tôi:}
\begin{itemize}
    \item PyTorch reimplementation hoàn chỉnh
    \item Bug fixes và code improvements
    \item Expert-level documentation và analysis
\end{itemize}

\textbf{Takeaway message:} Cho bài toán medical image classification, hybrid architectures kết hợp CNN và Transformer là hướng đi promising, tận dụng cả local feature extraction và global reasoning capabilities.
\end{tcolorbox}

\section{Acknowledgments}

\begin{itemize}
    \item Original paper authors: Ananya Jain, Aviral Bhardwaj, Kaushik Murali, Isha Surani (University of Toronto)
    \item NIH Clinical Center for the Chest X-ray dataset
    \item PyTorch và open-source community
\end{itemize}
