{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6008655",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Summary: Phase 1 Improvements\n",
    "\n",
    "### ‚úÖ ƒê√£ Implement\n",
    "\n",
    "| Improvement | Implementation | Expected Impact |\n",
    "|-------------|----------------|----------------|\n",
    "| **Advanced Augmentation** | Albumentations pipeline v·ªõi CLAHE, ShiftScaleRotate, Noise/Blur | +1-2% AUC |\n",
    "| **Class Imbalance** | Focal Loss + Weighted BCE + Label Smoothing | +3-5% (rare classes) |\n",
    "| **Transfer Learning** | ImageNet pre-trained weights + Progressive unfreezing | +2-4% AUC |\n",
    "\n",
    "### üéØ Combined Expected Impact\n",
    "- **Total: +5-10% AUC improvement**\n",
    "- **Faster convergence** (50% fewer epochs)\n",
    "- **Better generalization**\n",
    "- **More clinically useful** (better on rare diseases)\n",
    "\n",
    "### üìù Next Steps\n",
    "\n",
    "Trong c√°c cells ti·∫øp theo, ch√∫ng ta s·∫Ω:\n",
    "1. **Load v√† preprocess data**\n",
    "2. **Create datasets v·ªõi advanced augmentation**\n",
    "3. **Train models v·ªõi all improvements**\n",
    "4. **Evaluate v√† compare v·ªõi baseline**\n",
    "5. **Visualize results v√† insights**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f40dd9",
   "metadata": {},
   "source": [
    "# üóÇÔ∏è PHASE 2: Data Loading & Preprocessing\n",
    "\n",
    "## 2.1 Load NIH Chest X-ray Dataset\n",
    "\n",
    "### Dataset Overview\n",
    "- **Total images**: 112,120\n",
    "- **Number of classes**: 15 (multi-label)\n",
    "- **Format**: PNG grayscale images\n",
    "- **Labels**: NLP-extracted from radiology reports (~10% noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2cbc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data(csv_path, test_size=0.2, val_size=0.1, random_state=42):\n",
    "    \"\"\"\n",
    "    Load v√† prepare NIH Chest X-ray dataset\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to Data_Entry_2017_v2020.csv\n",
    "        test_size: Fraction for test set\n",
    "        val_size: Fraction of train set for validation\n",
    "    \n",
    "    Returns:\n",
    "        train_df, val_df, test_df, disease_columns\n",
    "    \"\"\"\n",
    "    print(\"üìÇ Loading dataset...\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    print(f\"   Total samples: {len(df):,}\")\n",
    "    \n",
    "    # Parse Finding Labels column\n",
    "    # Format: \"Disease1|Disease2|Disease3\" or \"No Finding\"\n",
    "    \n",
    "    # Get unique diseases\n",
    "    all_diseases = set()\n",
    "    for labels in df['Finding Labels'].values:\n",
    "        diseases = labels.split('|')\n",
    "        all_diseases.update(diseases)\n",
    "    \n",
    "    disease_columns = sorted(list(all_diseases))\n",
    "    print(f\"   Diseases found: {len(disease_columns)}\")\n",
    "    print(f\"   {disease_columns}\")\n",
    "    \n",
    "    # Create binary columns for each disease\n",
    "    for disease in disease_columns:\n",
    "        df[disease] = df['Finding Labels'].apply(\n",
    "            lambda x: 1 if disease in x.split('|') else 0\n",
    "        )\n",
    "    \n",
    "    # Print class distribution\n",
    "    print(\"\\nüìä Class Distribution:\")\n",
    "    class_counts = df[disease_columns].sum().sort_values(ascending=False)\n",
    "    for disease, count in class_counts.items():\n",
    "        percentage = count / len(df) * 100\n",
    "        print(f\"   {disease:25s}: {count:6,} ({percentage:5.2f}%)\")\n",
    "    \n",
    "    # Split data: train/val/test\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # First split: train+val vs test\n",
    "    train_val_df, test_df = train_test_split(\n",
    "        df, test_size=test_size, random_state=random_state, shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Second split: train vs val\n",
    "    train_df, val_df = train_test_split(\n",
    "        train_val_df, test_size=val_size, random_state=random_state, shuffle=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüì¶ Data Split:\")\n",
    "    print(f\"   Train: {len(train_df):,} samples ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "    print(f\"   Val:   {len(val_df):,} samples ({len(val_df)/len(df)*100:.1f}%)\")\n",
    "    print(f\"   Test:  {len(test_df):,} samples ({len(test_df)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    return train_df, val_df, test_df, disease_columns\n",
    "\n",
    "\n",
    "# Load data\n",
    "if CSV_PATH.exists():\n",
    "    train_df, val_df, test_df, disease_columns = load_and_prepare_data(\n",
    "        CSV_PATH, test_size=0.2, val_size=0.1\n",
    "    )\n",
    "    print(\"\\n‚úÖ Data loaded successfully!\")\n",
    "else:\n",
    "    print(f\"‚ùå CSV file not found: {CSV_PATH}\")\n",
    "    print(\"   Please update CSV_PATH in configuration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f14e71c",
   "metadata": {},
   "source": [
    "## 2.2 Custom Dataset Class\n",
    "\n",
    "### Design Principles\n",
    "1. **Efficient loading**: Only load images when needed\n",
    "2. **Flexible augmentation**: Support different transforms for train/val\n",
    "3. **Error handling**: Skip corrupted images\n",
    "4. **Memory efficient**: Don't load all images to RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480789e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChestXrayDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for NIH Chest X-ray\n",
    "    \n",
    "    Features:\n",
    "    - Lazy loading (load images on-demand)\n",
    "    - Albumentations transforms\n",
    "    - Error handling for corrupted images\n",
    "    - Multi-label support\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe, image_dir, disease_columns, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe: DataFrame with image paths and labels\n",
    "            image_dir: Root directory containing images\n",
    "            disease_columns: List of disease column names\n",
    "            transform: Albumentations transform pipeline\n",
    "        \"\"\"\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.disease_columns = disease_columns\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Load and return one sample\n",
    "        \n",
    "        Returns:\n",
    "            image: (C, H, W) tensor\n",
    "            labels: (num_classes,) binary vector\n",
    "        \"\"\"\n",
    "        # Get image path and labels\n",
    "        row = self.df.iloc[idx]\n",
    "        img_name = row['Image Index']\n",
    "        img_path = self.image_dir / img_name\n",
    "        \n",
    "        # Load image\n",
    "        try:\n",
    "            image = cv2.imread(str(img_path))\n",
    "            \n",
    "            if image is None:\n",
    "                raise ValueError(f\"Failed to load image: {img_path}\")\n",
    "            \n",
    "            # Convert to RGB (X-ray is grayscale, but we need 3 channels for pre-trained models)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Error loading {img_path}: {e}\")\n",
    "            # Return black image as fallback\n",
    "            image = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        \n",
    "        # Get labels\n",
    "        labels = row[self.disease_columns].values.astype(np.float32)\n",
    "        labels = torch.FloatTensor(labels)\n",
    "        \n",
    "        return image, labels\n",
    "\n",
    "\n",
    "print(\"‚úÖ ChestXrayDataset class created\")\n",
    "print(\"   Features: Lazy loading, error handling, multi-label support\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207bcede",
   "metadata": {},
   "source": [
    "## 2.3 Create DataLoaders\n",
    "\n",
    "### Strategy\n",
    "1. **Train**: Advanced augmentation + WeightedRandomSampler\n",
    "2. **Val/Test**: Simple resize + normalize only\n",
    "3. **Batch size**: Balance between GPU memory and convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b5293b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(train_df, val_df, test_df, disease_columns, image_dir, config):\n",
    "    \"\"\"\n",
    "    Create train/val/test DataLoaders v·ªõi all improvements\n",
    "    \n",
    "    Returns:\n",
    "        train_loader, val_loader, test_loader\n",
    "    \"\"\"\n",
    "    # Get transforms\n",
    "    train_transform = get_train_transforms(config['img_size'])\n",
    "    valid_transform = get_valid_transforms(config['img_size'])\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = ChestXrayDataset(\n",
    "        train_df, image_dir, disease_columns, train_transform\n",
    "    )\n",
    "    val_dataset = ChestXrayDataset(\n",
    "        val_df, image_dir, disease_columns, valid_transform\n",
    "    )\n",
    "    test_dataset = ChestXrayDataset(\n",
    "        test_df, image_dir, disease_columns, valid_transform\n",
    "    )\n",
    "    \n",
    "    # Compute sample weights for weighted sampling\n",
    "    print(\"‚öôÔ∏è  Computing sample weights...\")\n",
    "    sample_weights = compute_sample_weights(train_df, disease_columns)\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        sampler=sampler,  # Use WeightedRandomSampler\n",
    "        num_workers=config['num_workers'],\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=config['num_workers'],\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=config['num_workers'],\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ DataLoaders created:\")\n",
    "    print(f\"   Train: {len(train_loader)} batches\")\n",
    "    print(f\"   Val:   {len(val_loader)} batches\")\n",
    "    print(f\"   Test:  {len(test_loader)} batches\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "# Create DataLoaders (if data is loaded)\n",
    "if 'train_df' in locals():\n",
    "    train_loader, val_loader, test_loader = create_dataloaders(\n",
    "        train_df, val_df, test_df, disease_columns, IMAGE_DIR, CONFIG\n",
    "    )\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Data not loaded, skip DataLoader creation\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
