# PhÃ¢n TÃ­ch Chi Tiáº¿t: ResNet Model

## ğŸ“‹ Tá»•ng Quan

File `resnet.ipynb` triá»ƒn khai mÃ´ hÃ¬nh **Residual Network (ResNet)** tá»« Ä‘áº§u Ä‘á»ƒ phÃ¢n loáº¡i bá»‡nh lÃ½ tá»« X-quang ngá»±c. ResNet giáº£i quyáº¿t váº¥n Ä‘á» **vanishing gradient** trong deep networks thÃ´ng qua **skip connections**.

---

## ğŸ”§ Cell 1-3: Setup (Giá»‘ng CNN)

```python
# Import libraries
# %run data.ipynb
# Device configuration
```

*(TÆ°Æ¡ng tá»± nhÆ° trong cnn.ipynb)*

---

## ğŸ”§ Cell 4: Hyperparameters

```python
batch_size = 32
learning_rate = 1e-4
weight_decay = 1e-6
num_epochs = 10
input_shape = (224, 224, 3) 
num_classes = 15
```

| Parameter | GiÃ¡ trá»‹ | ÄÃ¡nh giÃ¡ |
|-----------|---------|----------|
| `batch_size` | 32 | âœ… Standard |
| `learning_rate` | 1e-4 | âœ… PhÃ¹ há»£p |
| `weight_decay` | 1e-6 | âš ï¸ CÃ³ thá»ƒ tÄƒng |
| `num_epochs` | 10 | âš ï¸ Ãt cho ResNet |
| `input_shape` | (224,224,3) | âœ… Standard ImageNet size |

---

## ğŸ”§ Cell 5: Residual Block

```python
def block(x, filters, strides=1):
    identity = x
    x = Conv2D(filters, 3, strides=strides, padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Conv2D(filters, 3, strides=1, padding='same')(x)
    x = BatchNormalization()(x)

    if strides != 1 or identity.shape[-1] != filters:
        identity = Conv2D(filters, 1, strides=strides, padding='same')(identity)
        identity = BatchNormalization()(identity)
    
    x += identity
    x = Activation('relu')(x)
    return x
```

### Kiáº¿n trÃºc Residual Block:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     RESIDUAL BLOCK                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚    Input (x)                                                    â”‚
â”‚       â”‚                                                         â”‚
â”‚       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚       â”‚                            â”‚                            â”‚
â”‚       â–¼                            â”‚ (Identity/Shortcut)        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚                            â”‚
â”‚  â”‚ Conv2D 3Ã—3  â”‚                   â”‚                            â”‚
â”‚  â”‚ (filters)   â”‚                   â”‚                            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                   â”‚                            â”‚
â”‚         â”‚                          â”‚                            â”‚
â”‚         â–¼                          â”‚                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚                            â”‚
â”‚  â”‚ BatchNorm   â”‚                   â”‚                            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                   â”‚                            â”‚
â”‚         â”‚                          â”‚                            â”‚
â”‚         â–¼                          â”‚                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚                            â”‚
â”‚  â”‚    ReLU     â”‚                   â”‚                            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                   â”‚                            â”‚
â”‚         â”‚                          â”‚                            â”‚
â”‚         â–¼                          â”‚                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚                            â”‚
â”‚  â”‚ Conv2D 3Ã—3  â”‚                   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ (filters)   â”‚                   â”‚  â”‚ 1Ã—1 Conv        â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                   â”‚  â”‚ (if dimension   â”‚       â”‚
â”‚         â”‚                          â”‚  â”‚  mismatch)      â”‚       â”‚
â”‚         â–¼                          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚           â”‚                â”‚
â”‚  â”‚ BatchNorm   â”‚                   â”‚           â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                   â”‚           â”‚                â”‚
â”‚         â”‚                          â”‚           â”‚                â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                        â”‚                                        â”‚
â”‚                        â–¼                                        â”‚
â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚                   â”‚   ADD   â”‚  â† Skip Connection                â”‚
â”‚                   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                                   â”‚
â”‚                        â”‚                                        â”‚
â”‚                        â–¼                                        â”‚
â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚                   â”‚  ReLU   â”‚                                   â”‚
â”‚                   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                                   â”‚
â”‚                        â”‚                                        â”‚
â”‚                    Output                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### CÃ´ng thá»©c toÃ¡n há»c:

$$\mathbf{y} = \mathcal{F}(\mathbf{x}, \{W_i\}) + \mathbf{x}$$

Trong Ä‘Ã³:
- $\mathbf{x}$: input
- $\mathcal{F}(\mathbf{x}, \{W_i\})$: residual mapping (2 conv layers)
- $\mathbf{y}$: output

### Identity Shortcut Projection:

```python
if strides != 1 or identity.shape[-1] != filters:
    identity = Conv2D(filters, 1, strides=strides, padding='same')(identity)
    identity = BatchNormalization()(identity)
```

**Khi nÃ o cáº§n projection?**
1. `strides != 1`: Khi downsample (giáº£m spatial dimension)
2. `identity.shape[-1] != filters`: Khi sá»‘ channels khÃ¡c nhau

---

## ğŸ”§ Cell 6: Full ResNet Architecture

```python
def create_resnet():
    inputs = Input(shape=input_shape)
    x = Conv2D(64, (7, 7), strides=2, padding='same')(inputs)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = MaxPooling2D(pool_size=3, strides=2, padding='same')(x)
    
    # Stage 1: 64 filters
    x = block(x, 64)
    x = block(x, 64)
    x = block(x, 64)

    # Stage 2: 128 filters  
    x = block(x, 128, 2)  # Downsample
    x = block(x, 128)
    x = block(x, 128)
    x = block(x, 128)

    # Stage 3: 256 filters
    x = block(x, 256, 2)  # Downsample
    x = block(x, 256)
    x = block(x, 256)
    x = block(x, 256)
    x = block(x, 256)
    x = block(x, 256)

    # Stage 4: 512 filters
    x = block(x, 512, 2)  # Downsample
    x = block(x, 512)
    x = block(x, 512)
    
    x = GlobalAveragePooling2D()(x)
    outputs = Dense(num_classes, activation='sigmoid')(x)
    model = Model(inputs, outputs)
    return model
```

### Kiáº¿n trÃºc Ä‘áº§y Ä‘á»§:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ResNet-34 Style                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  INPUT: (224, 224, 3)                                          â”‚
â”‚         â”‚                                                       â”‚
â”‚         â–¼                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ STEM: Conv 7Ã—7, 64 filters, stride 2                    â”‚   â”‚
â”‚  â”‚       BatchNorm â†’ ReLU â†’ MaxPool 3Ã—3, stride 2          â”‚   â”‚
â”‚  â”‚       Output: (56, 56, 64)                              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                             â”‚                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ STAGE 1: 3 blocks Ã— 64 filters                          â”‚   â”‚
â”‚  â”‚          Output: (56, 56, 64)                           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                             â”‚                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ STAGE 2: 4 blocks Ã— 128 filters (first block stride=2)  â”‚   â”‚
â”‚  â”‚          Output: (28, 28, 128)                          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                             â”‚                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ STAGE 3: 6 blocks Ã— 256 filters (first block stride=2)  â”‚   â”‚
â”‚  â”‚          Output: (14, 14, 256)                          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                             â”‚                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ STAGE 4: 3 blocks Ã— 512 filters (first block stride=2)  â”‚   â”‚
â”‚  â”‚          Output: (7, 7, 512)                            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                             â”‚                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ GlobalAveragePooling2D                                   â”‚   â”‚
â”‚  â”‚          Output: (512,)                                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                             â”‚                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Dense(15, sigmoid)                                       â”‚   â”‚
â”‚  â”‚          Output: (15,) - Multi-label probabilities      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Sá»‘ blocks theo stage:

| Stage | Filters | Blocks | Total Layers |
|-------|---------|--------|--------------|
| 1 | 64 | 3 | 6 conv |
| 2 | 128 | 4 | 8 conv |
| 3 | 256 | 6 | 12 conv |
| 4 | 512 | 3 | 6 conv |
| **Total** | - | **16 blocks** | **32 conv + 2 = 34 layers** |

â†’ ÄÃ¢y lÃ  kiáº¿n trÃºc **ResNet-34** style!

### So sÃ¡nh vá»›i ResNet gá»‘c:

| Variant | Blocks (per stage) | Total Layers |
|---------|-------------------|--------------|
| ResNet-18 | [2, 2, 2, 2] | 18 |
| **This model** | **[3, 4, 6, 3]** | **~34** |
| ResNet-34 | [3, 4, 6, 3] | 34 |
| ResNet-50 | [3, 4, 6, 3] + Bottleneck | 50 |

### TÃ­nh sá»‘ Parameters:

```
STEM:
  Conv 7Ã—7Ã—3Ã—64 + bias = 9,472
  
STAGE 1 (64 filters, 3 blocks):
  Each block: 2 Ã— (3Ã—3Ã—64Ã—64) = 73,728
  Total: 3 Ã— 73,728 = 221,184
  
STAGE 2 (128 filters, 4 blocks):
  First block with projection: ~200K
  Other blocks: 3 Ã— (2 Ã— 3Ã—3Ã—128Ã—128) = 884,736
  
... (tÆ°Æ¡ng tá»± cho stage 3, 4)

Dense: 512 Ã— 15 = 7,680

TOTAL: ~21.3 million parameters
```

---

## ğŸ”§ So sÃ¡nh ResNet vá»›i CNN Ä‘Æ¡n giáº£n

| Aspect | Simple CNN | ResNet |
|--------|-----------|--------|
| **Depth** | 2 conv layers | 34 layers |
| **Parameters** | ~95M (bottleneck á»Ÿ Dense) | ~21M |
| **Skip connections** | âŒ | âœ… |
| **BatchNorm** | âŒ | âœ… |
| **Gradient flow** | Vanishing | Healthy |
| **Expected AUC** | 0.55-0.65 | 0.70-0.80 |

---

## ğŸ”§ Training Function

```python
def run_experiment(model):
    optimizer = keras.optimizers.AdamW(
        learning_rate=learning_rate, weight_decay=weight_decay
    )
    model.compile(
        optimizer=optimizer,
        loss='binary_crossentropy',
        metrics=[
            keras.metrics.BinaryAccuracy(name="accuracy"),
            keras.metrics.AUC(name="auc"),
        ]
    )
    history = model.fit(
        train_generator,
        epochs=num_epochs,
        validation_data=validation_generator,
        callbacks=[ModelCheckpoint(...)]
    )
    return history
```

### âš ï¸ Missing imports:

Code sá»­ dá»¥ng nhÆ°ng khÃ´ng import:
- `GlobalAveragePooling2D`
- `Dense`
- `Model`
- `keras`

### ğŸ’¡ Fix:
```python
from tensorflow.keras.layers import (
    Input, Conv2D, BatchNormalization, Activation, 
    MaxPooling2D, GlobalAveragePooling2D, Dense, Add
)
from tensorflow.keras.models import Model
from tensorflow import keras
```

---

## ğŸ“Š Vanishing Gradient Problem & Solution

### Váº¥n Ä‘á» vá»›i Deep Networks:

```
Layer 1 â†’ Layer 2 â†’ ... â†’ Layer 30
   â”‚         â”‚              â”‚
   â–¼         â–¼              â–¼
âˆ‚L/âˆ‚Wâ‚    âˆ‚L/âˆ‚Wâ‚‚    ...   âˆ‚L/âˆ‚Wâ‚ƒâ‚€

Gradient chain rule:
âˆ‚L/âˆ‚Wâ‚ = âˆ‚L/âˆ‚Wâ‚ƒâ‚€ Ã— âˆ‚Wâ‚ƒâ‚€/âˆ‚Wâ‚‚â‚‰ Ã— ... Ã— âˆ‚Wâ‚‚/âˆ‚Wâ‚

Náº¿u má»—i term < 1 â†’ gradient tiáº¿n vá» 0
Náº¿u má»—i term > 1 â†’ gradient explodes
```

### ResNet Solution:

```python
x += identity  # Skip connection

# Gradient cá»§a skip connection:
âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚y Ã— (âˆ‚F/âˆ‚x + 1)  # LuÃ´n cÃ³ +1!
```

**Káº¿t quáº£**: Gradient luÃ´n cÃ³ Ã­t nháº¥t lÃ  1, khÃ´ng bao giá» vanish!

---

## ğŸ“Š ÄÃ¡nh GiÃ¡ Tá»•ng Thá»ƒ

### âœ… Äiá»ƒm máº¡nh:
1. âœ… Kiáº¿n trÃºc ResNet Ä‘Ãºng chuáº©n
2. âœ… Skip connections cho better gradient flow
3. âœ… BatchNormalization á»Ÿ má»i layer
4. âœ… GlobalAveragePooling thay vÃ¬ Flatten
5. âœ… Sigmoid activation cho multi-label

### âŒ Äiá»ƒm yáº¿u:

| Váº¥n Ä‘á» | Má»©c Ä‘á»™ | Giáº£i phÃ¡p |
|--------|--------|-----------|
| Missing imports | ğŸ”´ Critical | Add proper imports |
| KhÃ´ng dÃ¹ng pretrained weights | ğŸŸ  High | Use ImageNet weights |
| Thiáº¿u Dropout | ğŸŸ¡ Medium | Add after GAP |
| weight_decay quÃ¡ nhá» | ğŸŸ¡ Medium | Increase to 1e-4 |

### ğŸ’¡ Improved Version:

```python
def create_resnet_improved():
    # Use pretrained ResNet50
    base_model = tf.keras.applications.ResNet50(
        include_top=False,
        weights='imagenet',
        input_shape=(224, 224, 3)
    )
    
    # Fine-tune last few layers
    for layer in base_model.layers[:-20]:
        layer.trainable = False
    
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    x = Dropout(0.5)(x)
    x = Dense(256, activation='relu')(x)
    x = Dropout(0.3)(x)
    outputs = Dense(num_classes, activation='sigmoid')(x)
    
    model = Model(inputs=base_model.input, outputs=outputs)
    return model
```

---

## ğŸ“š ResNet Family

| Model | Params | Top-1 Acc (ImageNet) |
|-------|--------|---------------------|
| ResNet-18 | 11.7M | 69.8% |
| ResNet-34 | 21.8M | 73.3% |
| ResNet-50 | 25.6M | 76.1% |
| ResNet-101 | 44.5M | 77.4% |
| ResNet-152 | 60.2M | 78.3% |

---

## ğŸ“š References

1. He et al., "Deep Residual Learning for Image Recognition", CVPR 2016
2. He et al., "Identity Mappings in Deep Residual Networks", ECCV 2016
3. TensorFlow ResNet Documentation
