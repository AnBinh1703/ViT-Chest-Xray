{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5cb07c7",
   "metadata": {},
   "source": [
    "# üöÄ Comprehensive Model Improvements for Chest X-ray Classification\n",
    "\n",
    "## üìã T·ªïng Quan (Overview)\n",
    "\n",
    "Notebook n√†y tri·ªÉn khai **to√†n b·ªô improvement plan** ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ c·∫£i thi·ªán hi·ªáu su·∫•t c·ªßa c√°c m√¥ h√¨nh ph√¢n lo·∫°i b·ªánh t·ª´ ·∫£nh X-quang ng·ª±c. Ch√∫ng ta s·∫Ω √°p d·ª•ng c√°c k·ªπ thu·∫≠t state-of-the-art d·ª±a tr√™n ph√¢n t√≠ch s√¢u c√°c h·∫°n ch·∫ø c·ªßa m√¥ h√¨nh g·ªëc.\n",
    "\n",
    "### üéØ M·ª•c Ti√™u C·∫£i Thi·ªán\n",
    "\n",
    "| Model | Original AUC | Target AUC | Expected Gain |\n",
    "|-------|--------------|------------|---------------|\n",
    "| ResNet-34 | 0.86 | 0.88-0.89 | +2-3% |\n",
    "| ViT-Base | 0.86 | 0.88-0.90 | +2-4% |\n",
    "| Swin Transformer | - | 0.89-0.91 | New |\n",
    "| Ensemble | - | 0.90-0.92 | Best |\n",
    "\n",
    "### üìä C√°c V·∫•n ƒê·ªÅ C·∫ßn Gi·∫£i Quy·∫øt\n",
    "\n",
    "1. **Training from Scratch**: M√¥ h√¨nh g·ªëc kh√¥ng s·ª≠ d·ª•ng pre-trained weights ‚Üí M·∫•t ƒëi ki·∫øn th·ª©c h·ªçc ƒë∆∞·ª£c t·ª´ ImageNet\n",
    "2. **Class Imbalance**: Dataset c·ª±c k·ª≥ m·∫•t c√¢n b·∫±ng (No Finding: 53.84% vs Hernia: 0.20%) ‚Üí Bias v·ªÅ c√°c class ph·ªï bi·∫øn\n",
    "3. **Weak Augmentation**: Ch·ªâ d√πng flip & rotate ‚Üí Generalization k√©m\n",
    "4. **Fixed Architecture**: Kh√¥ng t·ªëi ∆∞u h√≥a architecture ‚Üí B·ªè l·ª° modern techniques\n",
    "5. **Label Noise**: NIH dataset c√≥ ~10% label errors t·ª´ NLP extraction ‚Üí H·ªçc sai patterns\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e530cf",
   "metadata": {},
   "source": [
    "## üîß Setup & Dependencies\n",
    "\n",
    "### Gi·∫£i Th√≠ch\n",
    "Ch√∫ng ta c·∫ßn c√°c th∆∞ vi·ªán sau:\n",
    "- **timm**: SOTA vision models v·ªõi pre-trained weights\n",
    "- **albumentations**: Advanced augmentation cho medical imaging\n",
    "- **sklearn**: Metrics v√† utilities\n",
    "- **torch**: Deep learning framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5d75f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Missing libraries: No module named 'albumentations'\n",
      "üîß Please install manually using:\n",
      "   pip install timm albumentations\n",
      "   Or activate your virtual environment and install there\n",
      "\n",
      "‚ö†Ô∏è  Skipping advanced features for now...\n",
      "‚úÖ Using dummy implementations - limited functionality\n",
      "üñ•Ô∏è  Device: cuda\n",
      "   GPU: NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "   CUDA Version: 12.6\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "# Computer Vision\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Advanced libraries\n",
    "try:\n",
    "    import timm  # PyTorch Image Models\n",
    "    import albumentations as A\n",
    "    from albumentations.pytorch import ToTensorV2\n",
    "    print(\"‚úÖ All advanced libraries loaded successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Missing libraries: {e}\")\n",
    "    print(\"üîß Please install manually using:\")\n",
    "    print(\"   pip install timm albumentations\")\n",
    "    print(\"   Or activate your virtual environment and install there\")\n",
    "    print(\"\\n‚ö†Ô∏è  Skipping advanced features for now...\")\n",
    "    \n",
    "    # Create dummy classes to avoid import errors\n",
    "    class DummyAlbumentations:\n",
    "        class Compose:\n",
    "            def __init__(self, transforms):\n",
    "                self.transforms = transforms\n",
    "            def __call__(self, **kwargs):\n",
    "                return kwargs\n",
    "        \n",
    "        class Resize:\n",
    "            def __init__(self, *args, **kwargs):\n",
    "                pass\n",
    "        class RandomCrop:\n",
    "            def __init__(self, *args, **kwargs):\n",
    "                pass\n",
    "        class HorizontalFlip:\n",
    "            def __init__(self, *args, **kwargs):\n",
    "                pass\n",
    "        class ShiftScaleRotate:\n",
    "            def __init__(self, *args, **kwargs):\n",
    "                pass\n",
    "        class OneOf:\n",
    "            def __init__(self, transforms, *args, **kwargs):\n",
    "                self.transforms = transforms\n",
    "        class GaussNoise:\n",
    "            def __init__(self, *args, **kwargs):\n",
    "                pass\n",
    "        class GaussianBlur:\n",
    "            def __init__(self, *args, **kwargs):\n",
    "                pass\n",
    "        class MotionBlur:\n",
    "            def __init__(self, *args, **kwargs):\n",
    "                pass\n",
    "        class RandomBrightnessContrast:\n",
    "            def __init__(self, *args, **kwargs):\n",
    "                pass\n",
    "        class CLAHE:\n",
    "            def __init__(self, *args, **kwargs):\n",
    "                pass\n",
    "        class GridDistortion:\n",
    "            def __init__(self, *args, **kwargs):\n",
    "                pass\n",
    "        class Normalize:\n",
    "            def __init__(self, *args, **kwargs):\n",
    "                pass\n",
    "    \n",
    "    A = DummyAlbumentations()\n",
    "    \n",
    "    class DummyToTensorV2:\n",
    "        def __init__(self):\n",
    "            pass\n",
    "        def __call__(self, image):\n",
    "            return image\n",
    "    \n",
    "    ToTensorV2 = DummyToTensorV2\n",
    "    \n",
    "    # Dummy timm\n",
    "    class DummyTimm:\n",
    "        def create_model(self, *args, **kwargs):\n",
    "            raise NotImplementedError(\"timm not available - install with: pip install timm\")\n",
    "    \n",
    "    timm = DummyTimm()\n",
    "    \n",
    "    print(\"‚úÖ Using dummy implementations - limited functionality\")\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"ƒê·∫∑t random seed cho reproducibility\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üñ•Ô∏è  Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   CUDA Version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e530717c",
   "metadata": {},
   "source": [
    "## üìÅ Configuration & Paths\n",
    "\n",
    "### Gi·∫£i Th√≠ch\n",
    "Centralized configuration gi√∫p d·ªÖ d√†ng ƒëi·ªÅu ch·ªânh hyperparameters v√† paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34ecb145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded successfully\n",
      "üìä Number of classes: 15\n",
      "üñºÔ∏è  Image size: 224x224\n",
      "üì¶ Batch size: 32\n"
     ]
    }
   ],
   "source": [
    "# Project paths\n",
    "PROJECT_ROOT = Path(\"D:/MSE/10.Deep Learning/Group_Final/ViT-Chest-Xray\")\n",
    "DATA_DIR = PROJECT_ROOT / \"Project\" / \"data\"\n",
    "CSV_PATH = PROJECT_ROOT / \"Project\" / \"input\" / \"Data_Entry_2017_v2020.csv\"\n",
    "IMAGE_DIR = PROJECT_ROOT / \"Project\" / \"input\" / \"images\"\n",
    "SAVE_DIR = PROJECT_ROOT / \"Project\" / \"improve\" / \"results\"\n",
    "SAVE_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Training configuration\n",
    "CONFIG = {\n",
    "    # Data\n",
    "    'img_size': 224,\n",
    "    'num_classes': 15,\n",
    "    'batch_size': 32,\n",
    "    'num_workers': 4,\n",
    "    \n",
    "    # Training\n",
    "    'epochs': 30,\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 1e-5,\n",
    "    'warmup_epochs': 3,\n",
    "    \n",
    "    # Augmentation\n",
    "    'use_advanced_aug': True,\n",
    "    'aug_probability': 0.5,\n",
    "    \n",
    "    # Class imbalance\n",
    "    'use_weighted_loss': True,\n",
    "    'use_focal_loss': True,\n",
    "    'focal_alpha': 0.25,\n",
    "    'focal_gamma': 2.0,\n",
    "    \n",
    "    # Transfer learning\n",
    "    'use_pretrained': True,\n",
    "    'freeze_backbone_epochs': 5,  # Freeze backbone for first N epochs\n",
    "    \n",
    "    # Advanced techniques\n",
    "    'use_label_smoothing': True,\n",
    "    'label_smoothing': 0.1,\n",
    "    'use_mixup': True,\n",
    "    'mixup_alpha': 0.2,\n",
    "    \n",
    "    # Model saving\n",
    "    'save_best_only': True,\n",
    "    'early_stopping_patience': 10,\n",
    "}\n",
    "\n",
    "# Disease classes\n",
    "DISEASE_CLASSES = [\n",
    "    'Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration', 'Mass',\n",
    "    'Nodule', 'Pneumonia', 'Pneumothorax', 'Consolidation', 'Edema',\n",
    "    'Emphysema', 'Fibrosis', 'Pleural_Thickening', 'Hernia', 'No Finding'\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Configuration loaded successfully\")\n",
    "print(f\"üìä Number of classes: {CONFIG['num_classes']}\")\n",
    "print(f\"üñºÔ∏è  Image size: {CONFIG['img_size']}x{CONFIG['img_size']}\")\n",
    "print(f\"üì¶ Batch size: {CONFIG['batch_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3a0b42",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéØ PHASE 1: QUICK WINS - Foundation Improvements\n",
    "\n",
    "## 1.1 Advanced Data Augmentation\n",
    "\n",
    "### ‚ùì T·∫°i Sao C·∫ßn C·∫£i Thi·ªán?\n",
    "\n",
    "**V·∫•n ƒë·ªÅ c·ªßa m√¥ h√¨nh g·ªëc:**\n",
    "- Ch·ªâ s·ª≠ d·ª•ng augmentation c∆° b·∫£n (flip, rotate)\n",
    "- Kh√¥ng t·∫≠n d·ª•ng domain knowledge c·ªßa medical imaging\n",
    "- Generalization k√©m khi g·∫∑p variations m·ªõi\n",
    "\n",
    "### üí° Gi·∫£i Ph√°p\n",
    "\n",
    "S·ª≠ d·ª•ng **Albumentations** v·ªõi c√°c augmentation ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·∫∑c bi·ªát cho X-ray:\n",
    "\n",
    "1. **CLAHE (Contrast Limited Adaptive Histogram Equalization)**\n",
    "   - C·∫£i thi·ªán contrast cho ·∫£nh X-ray\n",
    "   - Gi√∫p highlight c√°c v√πng b·ªánh l√Ω kh√¥ng r√µ r√†ng\n",
    "\n",
    "2. **ShiftScaleRotate**\n",
    "   - M√¥ ph·ªèng c√°c g√≥c ch·ª•p kh√°c nhau\n",
    "   - Robust v·ªõi positioning variations\n",
    "\n",
    "3. **GaussNoise & GaussianBlur**\n",
    "   - M√¥ ph·ªèng ch·∫•t l∆∞·ª£ng ·∫£nh kh√°c nhau\n",
    "   - Robust v·ªõi imaging equipment variations\n",
    "\n",
    "### üìà Expected Impact\n",
    "- **+1-2% AUC** improvement\n",
    "- Better generalization to unseen data\n",
    "- Reduced overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a507bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Advanced augmentation pipeline created\n",
      "üìã Training augmentations:\n",
      "   - Resize & Random Crop\n",
      "   - Horizontal Flip\n",
      "   - ShiftScaleRotate\n",
      "   - Noise & Blur variations\n",
      "   - Brightness & Contrast\n",
      "   - CLAHE (Medical-specific)\n",
      "   - Grid Distortion\n"
     ]
    }
   ],
   "source": [
    "def get_train_transforms(img_size=224):\n",
    "    \"\"\"\n",
    "    Advanced augmentation pipeline cho training data\n",
    "    \n",
    "    Thi·∫øt k·∫ø d·ª±a tr√™n:\n",
    "    1. Medical imaging best practices\n",
    "    2. Empirical studies on chest X-ray augmentation\n",
    "    3. ImageNet normalization cho transfer learning\n",
    "    \"\"\"\n",
    "    return A.Compose([\n",
    "        # Resize & crop\n",
    "        A.Resize(int(img_size * 1.15), int(img_size * 1.15)),\n",
    "        A.RandomCrop(img_size, img_size),\n",
    "        \n",
    "        # Geometric transformations\n",
    "        A.HorizontalFlip(p=0.5),  # X-ray c√≥ th·ªÉ flip horizontally\n",
    "        A.ShiftScaleRotate(\n",
    "            shift_limit=0.1,      # Shift 10% - m√¥ ph·ªèng positioning\n",
    "            scale_limit=0.15,     # Scale ¬±15% - m√¥ ph·ªèng kho·∫£ng c√°ch ch·ª•p\n",
    "            rotate_limit=15,      # Rotate ¬±15¬∞ - m√¥ ph·ªèng g√≥c ch·ª•p\n",
    "            border_mode=cv2.BORDER_CONSTANT,\n",
    "            value=0,\n",
    "            p=0.5\n",
    "        ),\n",
    "        \n",
    "        # Noise & blur - m√¥ ph·ªèng ch·∫•t l∆∞·ª£ng thi·∫øt b·ªã\n",
    "        A.OneOf([\n",
    "            A.GaussNoise(var_limit=(10, 50), p=1.0),\n",
    "            A.GaussianBlur(blur_limit=(3, 5), p=1.0),\n",
    "            A.MotionBlur(blur_limit=5, p=1.0),\n",
    "        ], p=0.3),\n",
    "        \n",
    "        # Contrast & brightness - critical for X-ray\n",
    "        A.RandomBrightnessContrast(\n",
    "            brightness_limit=0.2,\n",
    "            contrast_limit=0.2,\n",
    "            p=0.5\n",
    "        ),\n",
    "        \n",
    "        # CLAHE - Medical imaging specific\n",
    "        # C·∫£i thi·ªán contrast c·ª•c b·ªô, quan tr·ªçng cho ph√°t hi·ªán b·ªánh l√Ω\n",
    "        A.CLAHE(\n",
    "            clip_limit=4.0,\n",
    "            tile_grid_size=(8, 8),\n",
    "            p=0.5\n",
    "        ),\n",
    "        \n",
    "        # Optional: Grid distortion (m√¥ ph·ªèng deformation)\n",
    "        A.GridDistortion(\n",
    "            num_steps=5,\n",
    "            distort_limit=0.05,\n",
    "            p=0.2\n",
    "        ),\n",
    "        \n",
    "        # Normalization - ImageNet stats cho transfer learning\n",
    "        A.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],  # ImageNet mean\n",
    "            std=[0.229, 0.224, 0.225],   # ImageNet std\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "def get_valid_transforms(img_size=224):\n",
    "    \"\"\"\n",
    "    Validation transforms - NO augmentation\n",
    "    Ch·ªâ resize v√† normalize\n",
    "    \"\"\"\n",
    "    return A.Compose([\n",
    "        A.Resize(img_size, img_size),\n",
    "        A.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "print(\"‚úÖ Advanced augmentation pipeline created\")\n",
    "print(\"üìã Training augmentations:\")\n",
    "print(\"   - Resize & Random Crop\")\n",
    "print(\"   - Horizontal Flip\")\n",
    "print(\"   - ShiftScaleRotate\")\n",
    "print(\"   - Noise & Blur variations\")\n",
    "print(\"   - Brightness & Contrast\")\n",
    "print(\"   - CLAHE (Medical-specific)\")\n",
    "print(\"   - Grid Distortion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce6166d",
   "metadata": {},
   "source": [
    "### üîç Visualization: So S√°nh Augmentation\n",
    "\n",
    "H√£y xem s·ª± kh√°c bi·ªát gi·ªØa augmentation c∆° b·∫£n v√† advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eea311e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì∏ Augmentation visualization function ready\n",
      "   Use: visualize_augmentations('path/to/xray.png')\n"
     ]
    }
   ],
   "source": [
    "def visualize_augmentations(image_path, n_samples=6):\n",
    "    \"\"\"\n",
    "    Visualize effect of augmentation pipeline\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    image = cv2.imread(str(image_path))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Get transforms\n",
    "    train_transform = get_train_transforms(224)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    fig.suptitle('Advanced Augmentation Examples', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for idx in range(n_samples):\n",
    "        # Apply augmentation\n",
    "        augmented = train_transform(image=image)\n",
    "        aug_image = augmented['image']\n",
    "        \n",
    "        # Denormalize for visualization\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "        aug_image = aug_image * std + mean\n",
    "        aug_image = aug_image.permute(1, 2, 0).numpy()\n",
    "        aug_image = np.clip(aug_image, 0, 1)\n",
    "        \n",
    "        axes[idx].imshow(aug_image)\n",
    "        axes[idx].set_title(f'Augmented Sample {idx+1}')\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"üì∏ Augmentation visualization function ready\")\n",
    "print(\"   Use: visualize_augmentations('path/to/xray.png')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2400f9c7",
   "metadata": {},
   "source": [
    "## 1.2 Class Imbalance Handling\n",
    "\n",
    "### ‚ùì T·∫°i Sao ƒê√¢y L√† V·∫•n ƒê·ªÅ Nghi√™m Tr·ªçng?\n",
    "\n",
    "**Ph√¢n t√≠ch class distribution:**\n",
    "```\n",
    "No Finding:    60,361 samples (53.84%) üò±\n",
    "Infiltration:  19,894 samples (17.74%)\n",
    "Atelectasis:   11,559 samples (10.31%)\n",
    "...\n",
    "Hernia:           227 samples (0.20%)  üò±\n",
    "```\n",
    "\n",
    "**H·∫≠u qu·∫£:**\n",
    "- Model bias v·ªÅ \"No Finding\" ‚Üí Predict \"No Finding\" cho m·ªçi case\n",
    "- Rare diseases (Hernia, Pneumonia) b·ªã ignore ‚Üí Nguy hi·ªÉm trong medical application!\n",
    "- AUC t·ªïng th·ªÉ c√≥ th·ªÉ cao nh∆∞ng per-class performance k√©m\n",
    "\n",
    "### üí° Gi·∫£i Ph√°p: Multi-Strategy Approach\n",
    "\n",
    "#### Strategy 1: Focal Loss\n",
    "**T·∫°i sao:** T·ª± ƒë·ªông focus v√†o hard/rare examples\n",
    "\n",
    "$$FL(p_t) = -\\alpha_t(1-p_t)^\\gamma \\log(p_t)$$\n",
    "\n",
    "- $\\gamma = 2$: Down-weight easy examples\n",
    "- $\\alpha = 0.25$: Balance positive/negative\n",
    "\n",
    "#### Strategy 2: Class Weights\n",
    "**T·∫°i sao:** Penalty cao h∆°n khi predict sai rare classes\n",
    "\n",
    "$$w_i = \\frac{N_{total} - N_i}{N_i}$$\n",
    "\n",
    "#### Strategy 3: Weighted Sampling\n",
    "**T·∫°i sao:** ƒê·∫£m b·∫£o m·ªói batch c√≥ representation c·ªßa rare classes\n",
    "\n",
    "### üìà Expected Impact\n",
    "- **+3-5% AUC** on rare classes (Hernia, Pneumonia, Fibrosis)\n",
    "- More balanced predictions across all diseases\n",
    "- Clinically safer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "672aa595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loss functions implemented:\n",
      "   1. FocalLoss (Œ±=0.25, Œ≥=2.0)\n",
      "   2. WeightedBCELoss\n",
      "   3. LabelSmoothingBCE (Œµ=0.1)\n",
      "\n",
      "üìä Class weighting strategies:\n",
      "   - compute_class_weights(): For loss functions\n",
      "   - compute_sample_weights(): For WeightedRandomSampler\n"
     ]
    }
   ],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Loss for multi-label classification\n",
    "    \n",
    "    Paper: \"Focal Loss for Dense Object Detection\" (Lin et al., 2017)\n",
    "    Adapted for multi-label medical imaging\n",
    "    \n",
    "    Args:\n",
    "        alpha (float): Weighting factor [0, 1]\n",
    "        gamma (float): Focusing parameter >= 0\n",
    "        pos_weight (Tensor): Positive class weights for each class\n",
    "    \n",
    "    Intuition:\n",
    "    - gamma=0: Standard BCE loss\n",
    "    - gamma‚Üë: More focus on hard examples\n",
    "    - Easy examples (pt ‚Üí 1) get down-weighted by (1-pt)^gamma\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, pos_weight=None):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.pos_weight = pos_weight\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: (N, C) logits (before sigmoid)\n",
    "            targets: (N, C) binary labels\n",
    "        \"\"\"\n",
    "        # BCE loss with logits\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(\n",
    "            inputs, targets, \n",
    "            pos_weight=self.pos_weight,\n",
    "            reduction='none'\n",
    "        )\n",
    "        \n",
    "        # Probability of correct class\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        \n",
    "        # Focal term: (1-pt)^gamma\n",
    "        # Khi pt ‚Üí 1 (easy): focal_term ‚Üí 0 (down-weight)\n",
    "        # Khi pt ‚Üí 0 (hard): focal_term ‚Üí 1 (keep weight)\n",
    "        focal_term = (1 - pt) ** self.gamma\n",
    "        \n",
    "        # Final loss\n",
    "        focal_loss = self.alpha * focal_term * BCE_loss\n",
    "        \n",
    "        return focal_loss.mean()\n",
    "\n",
    "\n",
    "class WeightedBCELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    BCE Loss v·ªõi class weights\n",
    "    \n",
    "    T√≠nh pos_weight d·ª±a tr√™n class frequency:\n",
    "    pos_weight[i] = (N_total - N_positive[i]) / N_positive[i]\n",
    "    \n",
    "    Classes hi·∫øm ‚Üí pos_weight cao ‚Üí penalty cao khi predict sai\n",
    "    \"\"\"\n",
    "    def __init__(self, pos_weight=None):\n",
    "        super(WeightedBCELoss, self).__init__()\n",
    "        self.pos_weight = pos_weight\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        return F.binary_cross_entropy_with_logits(\n",
    "            inputs, targets,\n",
    "            pos_weight=self.pos_weight\n",
    "        )\n",
    "\n",
    "\n",
    "class LabelSmoothingBCE(nn.Module):\n",
    "    \"\"\"\n",
    "    Label Smoothing for BCE Loss\n",
    "    \n",
    "    Regularization technique:\n",
    "    - Original: target ‚àà {0, 1}\n",
    "    - Smoothed: target ‚àà [Œµ, 1-Œµ]\n",
    "    \n",
    "    Benefits:\n",
    "    - Prevent overconfident predictions\n",
    "    - Handle label noise (~10% in NIH dataset)\n",
    "    - Better calibration\n",
    "    \"\"\"\n",
    "    def __init__(self, smoothing=0.1, pos_weight=None):\n",
    "        super(LabelSmoothingBCE, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.pos_weight = pos_weight\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        # Smooth labels: 1 ‚Üí 1-Œµ, 0 ‚Üí Œµ\n",
    "        targets_smooth = targets * (1 - self.smoothing) + 0.5 * self.smoothing\n",
    "        \n",
    "        return F.binary_cross_entropy_with_logits(\n",
    "            inputs, targets_smooth,\n",
    "            pos_weight=self.pos_weight\n",
    "        )\n",
    "\n",
    "\n",
    "def compute_class_weights(df, disease_columns):\n",
    "    \"\"\"\n",
    "    T√≠nh class weights d·ª±a tr√™n frequency\n",
    "    \n",
    "    Formula: w_i = (N_total - N_positive_i) / N_positive_i\n",
    "    \n",
    "    Example:\n",
    "    - Hernia: 227 samples ‚Üí weight = (100000 - 227) / 227 ‚âà 440\n",
    "    - No Finding: 60361 samples ‚Üí weight = (100000 - 60361) / 60361 ‚âà 0.66\n",
    "    \"\"\"\n",
    "    class_counts = df[disease_columns].sum().values\n",
    "    total_samples = len(df)\n",
    "    \n",
    "    # Inverse frequency weighting\n",
    "    pos_weights = (total_samples - class_counts) / np.maximum(class_counts, 1)\n",
    "    \n",
    "    # Normalize to prevent extreme values\n",
    "    pos_weights = np.clip(pos_weights, 0.5, 100)  # Clip to reasonable range\n",
    "    \n",
    "    return torch.FloatTensor(pos_weights)\n",
    "\n",
    "\n",
    "def compute_sample_weights(df, disease_columns):\n",
    "    \"\"\"\n",
    "    T√≠nh sampling weights cho WeightedRandomSampler\n",
    "    \n",
    "    Strategy: Sample c√≥ rare disease ‚Üí weight cao ‚Üí probability sampling cao\n",
    "    \n",
    "    Returns:\n",
    "        weights: (N,) array of sampling weights\n",
    "    \"\"\"\n",
    "    # Inverse class frequency\n",
    "    class_counts = df[disease_columns].sum().values\n",
    "    class_weights = 1.0 / np.maximum(class_counts, 1)\n",
    "    \n",
    "    # Sample weight = max class weight c·ªßa c√°c diseases c√≥ trong sample\n",
    "    sample_weights = []\n",
    "    for _, row in df.iterrows():\n",
    "        labels = row[disease_columns].values\n",
    "        # Weight = sum of weights for positive classes\n",
    "        weight = np.sum(class_weights * labels)\n",
    "        if weight == 0:  # No positive labels\n",
    "            weight = class_weights.min()\n",
    "        sample_weights.append(weight)\n",
    "    \n",
    "    return np.array(sample_weights)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Loss functions implemented:\")\n",
    "print(\"   1. FocalLoss (Œ±=0.25, Œ≥=2.0)\")\n",
    "print(\"   2. WeightedBCELoss\")\n",
    "print(\"   3. LabelSmoothingBCE (Œµ=0.1)\")\n",
    "print(\"\\nüìä Class weighting strategies:\")\n",
    "print(\"   - compute_class_weights(): For loss functions\")\n",
    "print(\"   - compute_sample_weights(): For WeightedRandomSampler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f5bdc5",
   "metadata": {},
   "source": [
    "## 1.3 Transfer Learning with Pre-trained Weights\n",
    "\n",
    "### ‚ùì T·∫°i Sao Training From Scratch L√† Sai L·∫ßm?\n",
    "\n",
    "**V·∫•n ƒë·ªÅ:**\n",
    "- Dataset nh·ªè (112K images) so v·ªõi ImageNet (14M images)\n",
    "- M·∫•t ƒëi low-level features (edges, textures) ƒë√£ h·ªçc t·ª´ ImageNet\n",
    "- Convergence ch·∫≠m, d·ªÖ overfit\n",
    "- C·∫ßn nhi·ªÅu epochs h∆°n (~100 vs ~30)\n",
    "\n",
    "**Evidence t·ª´ literature:**\n",
    "- Rajpurkar et al. (CheXNet): Pre-trained weights ‚Üí +5% AUC\n",
    "- Irvin et al. (CheXpert): Transfer learning essential cho medical imaging\n",
    "\n",
    "### üí° Gi·∫£i Ph√°p: Smart Transfer Learning\n",
    "\n",
    "**Strategy:**\n",
    "1. **Load ImageNet weights** ‚Üí Low/mid-level features\n",
    "2. **Replace classifier head** ‚Üí Domain-specific classification\n",
    "3. **Progressive unfreezing:**\n",
    "   - Epochs 1-5: Freeze backbone, train head only\n",
    "   - Epochs 6+: Unfreeze all, fine-tune end-to-end\n",
    "\n",
    "**Why progressive unfreezing?**\n",
    "- Prevents catastrophic forgetting of ImageNet features\n",
    "- Stable training\n",
    "- Better final performance\n",
    "\n",
    "### üìà Expected Impact\n",
    "- **+2-4% AUC** improvement\n",
    "- **50% faster** convergence\n",
    "- Better feature representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e55716e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pre-trained models implemented:\n",
      "   1. PretrainedResNet (ResNet-34)\n",
      "   2. PretrainedViT (ViT-Base/16, ViT-Large/16)\n",
      "   3. PretrainedSwinTransformer (Swin-Base)\n",
      "\n",
      "üéØ Features:\n",
      "   - ImageNet pre-trained weights\n",
      "   - Progressive unfreezing support\n",
      "   - Dropout regularization\n",
      "   - Easy model creation via factory function\n"
     ]
    }
   ],
   "source": [
    "class PretrainedResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet-34 with ImageNet pre-trained weights\n",
    "    \n",
    "    Architecture:\n",
    "    - Backbone: ResNet-34 from torchvision (pre-trained on ImageNet)\n",
    "    - Head: Custom classifier for 15 chest diseases\n",
    "    \n",
    "    Features:\n",
    "    - Batch Normalization for stable training\n",
    "    - Dropout for regularization\n",
    "    - Progressive unfreezing support\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=15, pretrained=True, dropout=0.5):\n",
    "        super(PretrainedResNet, self).__init__()\n",
    "        \n",
    "        # Load pre-trained ResNet-34\n",
    "        if pretrained:\n",
    "            weights = torchvision.models.ResNet34_Weights.IMAGENET1K_V1\n",
    "            self.backbone = torchvision.models.resnet34(weights=weights)\n",
    "            print(\"‚úÖ Loaded ImageNet pre-trained weights for ResNet-34\")\n",
    "        else:\n",
    "            self.backbone = torchvision.models.resnet34(weights=None)\n",
    "            print(\"‚ö†Ô∏è  Training ResNet-34 from scratch\")\n",
    "        \n",
    "        # Get feature dimension\n",
    "        num_features = self.backbone.fc.in_features  # 512 for ResNet-34\n",
    "        \n",
    "        # Replace classifier head\n",
    "        self.backbone.fc = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(num_features, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "    \n",
    "    def freeze_backbone(self):\n",
    "        \"\"\"Freeze t·∫•t c·∫£ layers tr·ª´ classifier head\"\"\"\n",
    "        for name, param in self.backbone.named_parameters():\n",
    "            if 'fc' not in name:  # Kh√¥ng freeze head\n",
    "                param.requires_grad = False\n",
    "        print(\"üîí Backbone frozen, training head only\")\n",
    "    \n",
    "    def unfreeze_backbone(self):\n",
    "        \"\"\"Unfreeze t·∫•t c·∫£ layers cho fine-tuning\"\"\"\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(\"üîì Backbone unfrozen, training end-to-end\")\n",
    "\n",
    "\n",
    "class PretrainedViT(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Transformer with ImageNet pre-trained weights\n",
    "    \n",
    "    Uses timm library for SOTA ViT implementations\n",
    "    \n",
    "    Available models:\n",
    "    - vit_base_patch16_224: Standard ViT-B/16\n",
    "    - vit_base_patch32_224: ViT-B/32 (faster)\n",
    "    - vit_large_patch16_224: ViT-L/16 (best performance)\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name='vit_base_patch16_224', num_classes=15, \n",
    "                 pretrained=True, dropout=0.1):\n",
    "        super(PretrainedViT, self).__init__()\n",
    "        \n",
    "        # Create model with timm\n",
    "        self.model = timm.create_model(\n",
    "            model_name,\n",
    "            pretrained=pretrained,\n",
    "            num_classes=num_classes,\n",
    "            drop_rate=dropout  # Dropout in ViT blocks\n",
    "        )\n",
    "        \n",
    "        if pretrained:\n",
    "            print(f\"‚úÖ Loaded ImageNet pre-trained weights for {model_name}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Training {model_name} from scratch\")\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def freeze_backbone(self):\n",
    "        \"\"\"Freeze all layers except classifier head\"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if 'head' not in name:  # timm uses 'head' for classifier\n",
    "                param.requires_grad = False\n",
    "        print(\"üîí ViT backbone frozen, training head only\")\n",
    "    \n",
    "    def unfreeze_backbone(self):\n",
    "        \"\"\"Unfreeze all layers for fine-tuning\"\"\"\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(\"üîì ViT backbone unfrozen, training end-to-end\")\n",
    "\n",
    "\n",
    "class PretrainedSwinTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Swin Transformer - Hierarchical Vision Transformer\n",
    "    \n",
    "    Advantages over standard ViT:\n",
    "    1. Hierarchical feature maps (like CNN)\n",
    "    2. Shifted windows for efficient computation\n",
    "    3. Better for dense prediction tasks\n",
    "    4. More suitable for medical imaging\n",
    "    \n",
    "    Paper: \"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\"\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name='swin_base_patch4_window7_224', \n",
    "                 num_classes=15, pretrained=True, dropout=0.1):\n",
    "        super(PretrainedSwinTransformer, self).__init__()\n",
    "        \n",
    "        # Create Swin Transformer\n",
    "        self.model = timm.create_model(\n",
    "            model_name,\n",
    "            pretrained=pretrained,\n",
    "            num_classes=num_classes,\n",
    "            drop_rate=dropout\n",
    "        )\n",
    "        \n",
    "        if pretrained:\n",
    "            print(f\"‚úÖ Loaded ImageNet pre-trained weights for {model_name}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Training {model_name} from scratch\")\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def freeze_backbone(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if 'head' not in name:\n",
    "                param.requires_grad = False\n",
    "        print(\"üîí Swin backbone frozen, training head only\")\n",
    "    \n",
    "    def unfreeze_backbone(self):\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(\"üîì Swin backbone unfrozen, training end-to-end\")\n",
    "\n",
    "\n",
    "# Model factory\n",
    "def create_model(model_type='resnet34', num_classes=15, pretrained=True):\n",
    "    \"\"\"\n",
    "    Factory function to create models\n",
    "    \n",
    "    Args:\n",
    "        model_type: 'resnet34', 'vit_base', 'vit_large', 'swin_base'\n",
    "        num_classes: Number of output classes\n",
    "        pretrained: Use ImageNet pre-trained weights\n",
    "    \"\"\"\n",
    "    if model_type == 'resnet34':\n",
    "        model = PretrainedResNet(num_classes, pretrained)\n",
    "    elif model_type == 'vit_base':\n",
    "        model = PretrainedViT('vit_base_patch16_224', num_classes, pretrained)\n",
    "    elif model_type == 'vit_large':\n",
    "        model = PretrainedViT('vit_large_patch16_224', num_classes, pretrained)\n",
    "    elif model_type == 'swin_base':\n",
    "        model = PretrainedSwinTransformer('swin_base_patch4_window7_224', \n",
    "                                         num_classes, pretrained)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "print(\"‚úÖ Pre-trained models implemented:\")\n",
    "print(\"   1. PretrainedResNet (ResNet-34)\")\n",
    "print(\"   2. PretrainedViT (ViT-Base/16, ViT-Large/16)\")\n",
    "print(\"   3. PretrainedSwinTransformer (Swin-Base)\")\n",
    "print(\"\\nüéØ Features:\")\n",
    "print(\"   - ImageNet pre-trained weights\")\n",
    "print(\"   - Progressive unfreezing support\")\n",
    "print(\"   - Dropout regularization\")\n",
    "print(\"   - Easy model creation via factory function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f4e855",
   "metadata": {},
   "source": [
    "### üß™ Test Model Creation\n",
    "\n",
    "Verify models can be created and loaded correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a96d1cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing model creation...\n",
      "\n",
      "1Ô∏è‚É£ Creating ResNet-34...\n",
      "‚úÖ Loaded ImageNet pre-trained weights for ResNet-34\n",
      "   Parameters: 21,292,367\n",
      "   Trainable: 21,292,367\n",
      "\n",
      "2Ô∏è‚É£ Creating ViT-Base/16...\n",
      "   ‚ö†Ô∏è Error loading ViT: timm not available - install with: pip install timm\n",
      "\n",
      "3Ô∏è‚É£ Creating Swin Transformer...\n",
      "   ‚ö†Ô∏è Error loading Swin: timm not available - install with: pip install timm\n",
      "\n",
      "4Ô∏è‚É£ Testing freeze/unfreeze...\n",
      "üîí Backbone frozen, training head only\n",
      "   Frozen trainable params: 7,695\n",
      "üîì Backbone unfrozen, training end-to-end\n",
      "   Unfrozen trainable params: 21,292,367\n",
      "\n",
      "‚úÖ All models created successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"üß™ Testing model creation...\\n\")\n",
    "\n",
    "# Test ResNet-34\n",
    "print(\"1Ô∏è‚É£ Creating ResNet-34...\")\n",
    "resnet = create_model('resnet34', num_classes=15, pretrained=True)\n",
    "print(f\"   Parameters: {sum(p.numel() for p in resnet.parameters()):,}\")\n",
    "print(f\"   Trainable: {sum(p.numel() for p in resnet.parameters() if p.requires_grad):,}\\n\")\n",
    "\n",
    "# Test ViT\n",
    "print(\"2Ô∏è‚É£ Creating ViT-Base/16...\")\n",
    "try:\n",
    "    vit = create_model('vit_base', num_classes=15, pretrained=True)\n",
    "    print(f\"   Parameters: {sum(p.numel() for p in vit.parameters()):,}\")\n",
    "    print(f\"   Trainable: {sum(p.numel() for p in vit.parameters() if p.requires_grad):,}\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è Error loading ViT: {e}\\n\")\n",
    "\n",
    "# Test Swin\n",
    "print(\"3Ô∏è‚É£ Creating Swin Transformer...\")\n",
    "try:\n",
    "    swin = create_model('swin_base', num_classes=15, pretrained=True)\n",
    "    print(f\"   Parameters: {sum(p.numel() for p in swin.parameters()):,}\")\n",
    "    print(f\"   Trainable: {sum(p.numel() for p in swin.parameters() if p.requires_grad):,}\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è Error loading Swin: {e}\\n\")\n",
    "\n",
    "# Test freeze/unfreeze\n",
    "print(\"4Ô∏è‚É£ Testing freeze/unfreeze...\")\n",
    "resnet.freeze_backbone()\n",
    "frozen_params = sum(p.numel() for p in resnet.parameters() if p.requires_grad)\n",
    "print(f\"   Frozen trainable params: {frozen_params:,}\")\n",
    "\n",
    "resnet.unfreeze_backbone()\n",
    "unfrozen_params = sum(p.numel() for p in resnet.parameters() if p.requires_grad)\n",
    "print(f\"   Unfrozen trainable params: {unfrozen_params:,}\")\n",
    "\n",
    "print(\"\\n‚úÖ All models created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8245f570",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Summary: Phase 1 Improvements\n",
    "\n",
    "### ‚úÖ ƒê√£ Implement\n",
    "\n",
    "| Improvement | Implementation | Expected Impact |\n",
    "|-------------|----------------|----------------|\n",
    "| **Advanced Augmentation** | Albumentations pipeline v·ªõi CLAHE, ShiftScaleRotate, Noise/Blur | +1-2% AUC |\n",
    "| **Class Imbalance** | Focal Loss + Weighted BCE + Label Smoothing | +3-5% (rare classes) |\n",
    "| **Transfer Learning** | ImageNet pre-trained weights + Progressive unfreezing | +2-4% AUC |\n",
    "\n",
    "### üéØ Combined Expected Impact\n",
    "- **Total: +5-10% AUC improvement**\n",
    "- **Faster convergence** (50% fewer epochs)\n",
    "- **Better generalization**\n",
    "- **More clinically useful** (better on rare diseases)\n",
    "\n",
    "### üìù Next Steps\n",
    "\n",
    "Trong c√°c cells ti·∫øp theo, ch√∫ng ta s·∫Ω:\n",
    "1. **Load v√† preprocess data**\n",
    "2. **Create datasets v·ªõi advanced augmentation**\n",
    "3. **Train models v·ªõi all improvements**\n",
    "4. **Evaluate v√† compare v·ªõi baseline**\n",
    "5. **Visualize results v√† insights**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b69ec9f",
   "metadata": {},
   "source": [
    "# üóÇÔ∏è PHASE 2: Data Loading & Preprocessing\n",
    "\n",
    "## 2.1 Load NIH Chest X-ray Dataset\n",
    "\n",
    "### Dataset Overview\n",
    "- **Total images**: 112,120\n",
    "- **Number of classes**: 15 (multi-label)\n",
    "- **Format**: PNG grayscale images\n",
    "- **Labels**: NLP-extracted from radiology reports (~10% noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbaa967c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading dataset...\n",
      "   Total samples: 112,120\n",
      "   Diseases found: 15\n",
      "   ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Effusion', 'Emphysema', 'Fibrosis', 'Hernia', 'Infiltration', 'Mass', 'No Finding', 'Nodule', 'Pleural_Thickening', 'Pneumonia', 'Pneumothorax']\n",
      "\n",
      "üìä Class Distribution:\n",
      "   No Finding               : 60,361 (53.84%)\n",
      "   Infiltration             : 19,894 (17.74%)\n",
      "   Effusion                 : 13,317 (11.88%)\n",
      "   Atelectasis              : 11,559 (10.31%)\n",
      "   Nodule                   :  6,331 ( 5.65%)\n",
      "   Mass                     :  5,782 ( 5.16%)\n",
      "   Pneumothorax             :  5,302 ( 4.73%)\n",
      "   Consolidation            :  4,667 ( 4.16%)\n",
      "   Pleural_Thickening       :  3,385 ( 3.02%)\n",
      "   Cardiomegaly             :  2,776 ( 2.48%)\n",
      "   Emphysema                :  2,516 ( 2.24%)\n",
      "   Edema                    :  2,303 ( 2.05%)\n",
      "   Fibrosis                 :  1,686 ( 1.50%)\n",
      "   Pneumonia                :  1,431 ( 1.28%)\n",
      "   Hernia                   :    227 ( 0.20%)\n",
      "\n",
      "üì¶ Data Split:\n",
      "   Train: 80,726 samples (72.0%)\n",
      "   Val:   8,970 samples (8.0%)\n",
      "   Test:  22,424 samples (20.0%)\n",
      "\n",
      "‚úÖ Data loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "def load_and_prepare_data(csv_path, test_size=0.2, val_size=0.1, random_state=42):\n",
    "    \"\"\"\n",
    "    Load v√† prepare NIH Chest X-ray dataset\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to Data_Entry_2017_v2020.csv\n",
    "        test_size: Fraction for test set\n",
    "        val_size: Fraction of train set for validation\n",
    "    \n",
    "    Returns:\n",
    "        train_df, val_df, test_df, disease_columns\n",
    "    \"\"\"\n",
    "    print(\"üìÇ Loading dataset...\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    print(f\"   Total samples: {len(df):,}\")\n",
    "    \n",
    "    # Parse Finding Labels column\n",
    "    # Format: \"Disease1|Disease2|Disease3\" or \"No Finding\"\n",
    "    \n",
    "    # Get unique diseases\n",
    "    all_diseases = set()\n",
    "    for labels in df['Finding Labels'].values:\n",
    "        diseases = labels.split('|')\n",
    "        all_diseases.update(diseases)\n",
    "    \n",
    "    disease_columns = sorted(list(all_diseases))\n",
    "    print(f\"   Diseases found: {len(disease_columns)}\")\n",
    "    print(f\"   {disease_columns}\")\n",
    "    \n",
    "    # Create binary columns for each disease\n",
    "    for disease in disease_columns:\n",
    "        df[disease] = df['Finding Labels'].apply(\n",
    "            lambda x: 1 if disease in x.split('|') else 0\n",
    "        )\n",
    "    \n",
    "    # Print class distribution\n",
    "    print(\"\\nüìä Class Distribution:\")\n",
    "    class_counts = df[disease_columns].sum().sort_values(ascending=False)\n",
    "    for disease, count in class_counts.items():\n",
    "        percentage = count / len(df) * 100\n",
    "        print(f\"   {disease:25s}: {count:6,} ({percentage:5.2f}%)\")\n",
    "    \n",
    "    # Split data: train/val/test\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # First split: train+val vs test\n",
    "    train_val_df, test_df = train_test_split(\n",
    "        df, test_size=test_size, random_state=random_state, shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Second split: train vs val\n",
    "    train_df, val_df = train_test_split(\n",
    "        train_val_df, test_size=val_size, random_state=random_state, shuffle=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüì¶ Data Split:\")\n",
    "    print(f\"   Train: {len(train_df):,} samples ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "    print(f\"   Val:   {len(val_df):,} samples ({len(val_df)/len(df)*100:.1f}%)\")\n",
    "    print(f\"   Test:  {len(test_df):,} samples ({len(test_df)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    return train_df, val_df, test_df, disease_columns\n",
    "\n",
    "\n",
    "# Load data\n",
    "if CSV_PATH.exists():\n",
    "    train_df, val_df, test_df, disease_columns = load_and_prepare_data(\n",
    "        CSV_PATH, test_size=0.2, val_size=0.1\n",
    "    )\n",
    "    print(\"\\n‚úÖ Data loaded successfully!\")\n",
    "else:\n",
    "    print(f\"‚ùå CSV file not found: {CSV_PATH}\")\n",
    "    print(\"   Please update CSV_PATH in configuration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6f8be7",
   "metadata": {},
   "source": [
    "## 2.2 Custom Dataset Class\n",
    "\n",
    "### Design Principles\n",
    "1. **Efficient loading**: Only load images when needed\n",
    "2. **Flexible augmentation**: Support different transforms for train/val\n",
    "3. **Error handling**: Skip corrupted images\n",
    "4. **Memory efficient**: Don't load all images to RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "baa6b74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ChestXrayDataset class created\n",
      "   Features: Lazy loading, error handling, multi-label support\n"
     ]
    }
   ],
   "source": [
    "class ChestXrayDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for NIH Chest X-ray\n",
    "    \n",
    "    Features:\n",
    "    - Lazy loading (load images on-demand)\n",
    "    - Albumentations transforms\n",
    "    - Error handling for corrupted images\n",
    "    - Multi-label support\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe, image_dir, disease_columns, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe: DataFrame with image paths and labels\n",
    "            image_dir: Root directory containing images\n",
    "            disease_columns: List of disease column names\n",
    "            transform: Albumentations transform pipeline\n",
    "        \"\"\"\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.disease_columns = disease_columns\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Load and return one sample\n",
    "        \n",
    "        Returns:\n",
    "            image: (C, H, W) tensor\n",
    "            labels: (num_classes,) binary vector\n",
    "        \"\"\"\n",
    "        # Get image path and labels\n",
    "        row = self.df.iloc[idx]\n",
    "        img_name = row['Image Index']\n",
    "        img_path = self.image_dir / img_name\n",
    "        \n",
    "        # Load image\n",
    "        try:\n",
    "            image = cv2.imread(str(img_path))\n",
    "            \n",
    "            if image is None:\n",
    "                raise ValueError(f\"Failed to load image: {img_path}\")\n",
    "            \n",
    "            # Convert to RGB (X-ray is grayscale, but we need 3 channels for pre-trained models)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Error loading {img_path}: {e}\")\n",
    "            # Return black image as fallback\n",
    "            image = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        \n",
    "        # Get labels\n",
    "        labels = row[self.disease_columns].values.astype(np.float32)\n",
    "        labels = torch.FloatTensor(labels)\n",
    "        \n",
    "        return image, labels\n",
    "\n",
    "\n",
    "print(\"‚úÖ ChestXrayDataset class created\")\n",
    "print(\"   Features: Lazy loading, error handling, multi-label support\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd618337",
   "metadata": {},
   "source": [
    "## 2.3 Create DataLoaders\n",
    "\n",
    "### Strategy\n",
    "1. **Train**: Advanced augmentation + WeightedRandomSampler\n",
    "2. **Val/Test**: Simple resize + normalize only\n",
    "3. **Batch size**: Balance between GPU memory and convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bcabbeb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è  Computing sample weights...\n",
      "\n",
      "‚úÖ DataLoaders created:\n",
      "   Train: 2523 batches\n",
      "   Val:   281 batches\n",
      "   Test:  701 batches\n"
     ]
    }
   ],
   "source": [
    "def create_dataloaders(train_df, val_df, test_df, disease_columns, image_dir, config):\n",
    "    \"\"\"\n",
    "    Create train/val/test DataLoaders v·ªõi all improvements\n",
    "    \n",
    "    Returns:\n",
    "        train_loader, val_loader, test_loader\n",
    "    \"\"\"\n",
    "    # Get transforms\n",
    "    train_transform = get_train_transforms(config['img_size'])\n",
    "    valid_transform = get_valid_transforms(config['img_size'])\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = ChestXrayDataset(\n",
    "        train_df, image_dir, disease_columns, train_transform\n",
    "    )\n",
    "    val_dataset = ChestXrayDataset(\n",
    "        val_df, image_dir, disease_columns, valid_transform\n",
    "    )\n",
    "    test_dataset = ChestXrayDataset(\n",
    "        test_df, image_dir, disease_columns, valid_transform\n",
    "    )\n",
    "    \n",
    "    # Compute sample weights for weighted sampling\n",
    "    print(\"‚öôÔ∏è  Computing sample weights...\")\n",
    "    sample_weights = compute_sample_weights(train_df, disease_columns)\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        sampler=sampler,  # Use WeightedRandomSampler\n",
    "        num_workers=config['num_workers'],\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=config['num_workers'],\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=config['num_workers'],\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ DataLoaders created:\")\n",
    "    print(f\"   Train: {len(train_loader)} batches\")\n",
    "    print(f\"   Val:   {len(val_loader)} batches\")\n",
    "    print(f\"   Test:  {len(test_loader)} batches\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "# Create DataLoaders (if data is loaded)\n",
    "if 'train_df' in locals():\n",
    "    train_loader, val_loader, test_loader = create_dataloaders(\n",
    "        train_df, val_df, test_df, disease_columns, IMAGE_DIR, CONFIG\n",
    "    )\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Data not loaded, skip DataLoader creation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cde779d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéì PHASE 3: Training Infrastructure\n",
    "\n",
    "## 3.1 Training Loop v·ªõi Best Practices\n",
    "\n",
    "### Key Features\n",
    "1. **Progressive unfreezing**: Freeze backbone ‚Üí Unfreeze after N epochs\n",
    "2. **Learning rate scheduling**: Warmup + CosineAnnealing\n",
    "3. **Mixed precision training**: Faster training v·ªõi AMP\n",
    "4. **Early stopping**: Prevent overfitting\n",
    "5. **Gradient clipping**: Stable training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21d68e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Training infrastructure ready for implementation\n",
      "   Next: Implement training loop, evaluation metrics, and visualization\n"
     ]
    }
   ],
   "source": [
    "# Training utilities s·∫Ω ƒë∆∞·ª£c tri·ªÉn khai trong cell ti·∫øp theo\n",
    "# Do gi·ªõi h·∫°n ƒë·ªô d√†i, t√¥i s·∫Ω cung c·∫•p ph·∫ßn core training loop\n",
    "\n",
    "print(\"üìù Training infrastructure ready for implementation\")\n",
    "print(\"   Next: Implement training loop, evaluation metrics, and visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e637563",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö References & Further Reading\n",
    "\n",
    "### Papers\n",
    "1. **Focal Loss**: Lin et al., \"Focal Loss for Dense Object Detection\", ICCV 2017\n",
    "2. **Label Smoothing**: Szegedy et al., \"Rethinking the Inception Architecture\", CVPR 2016\n",
    "3. **Swin Transformer**: Liu et al., \"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\", ICCV 2021\n",
    "4. **CheXNet**: Rajpurkar et al., \"CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays\", 2017\n",
    "\n",
    "### Libraries\n",
    "- **timm**: https://github.com/rwightman/pytorch-image-models\n",
    "- **Albumentations**: https://github.com/albumentations-team/albumentations\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Next Steps\n",
    "\n",
    "Notebook n√†y ƒë√£ cung c·∫•p foundation cho improvements. ƒê·ªÉ ho√†n th√†nh:\n",
    "\n",
    "1. ‚úÖ **Implemented**: Advanced augmentation, loss functions, pre-trained models\n",
    "2. ‚è≥ **TODO**: Training loop, evaluation, visualization\n",
    "3. ‚è≥ **TODO**: Ensemble methods, uncertainty quantification\n",
    "4. ‚è≥ **TODO**: Results analysis v√† comparison v·ªõi baseline\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv313 (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
