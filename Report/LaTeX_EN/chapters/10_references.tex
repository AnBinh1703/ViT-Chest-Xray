% ============================================================
% CHAPTER 10: REFERENCES
% Bibliography and Citations
% ============================================================
\chapter{References}

\section{Primary Paper}

\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=Main Paper]
\textbf{[1]} Ananya Jain, Aviral Bhardwaj, Kaushik Murali, Isha Surani.\\
\textit{``A Comparative Study of CNN, ResNet, and Vision Transformers for Multi-Classification of Chest Diseases''}\\
arXiv:2406.00237, 2024.\\
\url{https://arxiv.org/abs/2406.00237}
\end{tcolorbox}

\section{Foundational Papers}

\subsection{Convolutional Neural Networks}

\begin{enumerate}[label={[\arabic*]}]
\setcounter{enumi}{1}

\item \textbf{LeCun, Y., Bottou, L., Bengio, Y., \& Haffner, P.}\\
\textit{``Gradient-based learning applied to document recognition''}\\
Proceedings of the IEEE, 86(11), 2278-2324, 1998.\\
\textit{Original CNN paper for image recognition.}

\item \textbf{Krizhevsky, A., Sutskever, I., \& Hinton, G. E.}\\
\textit{``ImageNet classification with deep convolutional neural networks''}\\
NeurIPS 2012.\\
\textit{AlexNet - Deep learning revolution in computer vision.}

\item \textbf{Simonyan, K., \& Zisserman, A.}\\
\textit{``Very deep convolutional networks for large-scale image recognition''}\\
arXiv:1409.1556, 2014.\\
\textit{VGGNet - Importance of network depth.}

\end{enumerate}

\subsection{Residual Networks}

\begin{enumerate}[label={[\arabic*]}]
\setcounter{enumi}{4}

\item \textbf{He, K., Zhang, X., Ren, S., \& Sun, J.}\\
\textit{``Deep residual learning for image recognition''}\\
CVPR 2016.\\
\textit{ResNet - Skip connections enabling very deep networks.}

\item \textbf{He, K., Zhang, X., Ren, S., \& Sun, J.}\\
\textit{``Identity mappings in deep residual networks''}\\
ECCV 2016.\\
\textit{Pre-activation ResNet - Improved residual block design.}

\end{enumerate}

\subsection{Transformers and Vision Transformers}

\begin{enumerate}[label={[\arabic*]}]
\setcounter{enumi}{6}

\item \textbf{Vaswani, A., Shazeer, N., Parmar, N., et al.}\\
\textit{``Attention is all you need''}\\
NeurIPS 2017.\\
\textit{Original Transformer architecture for NLP.}

\item \textbf{Dosovitskiy, A., Beyer, L., Kolesnikov, A., et al.}\\
\textit{``An image is worth 16Ã—16 words: Transformers for image recognition at scale''}\\
ICLR 2021.\\
\textit{Vision Transformer (ViT) - Pure Transformer for vision.}

\item \textbf{Touvron, H., Cord, M., Douze, M., et al.}\\
\textit{``Training data-efficient image transformers \& distillation through attention''}\\
ICML 2021.\\
\textit{DeiT - Data-efficient ViT training.}

\item \textbf{Liu, Z., Lin, Y., Cao, Y., et al.}\\
\textit{``Swin Transformer: Hierarchical vision transformer using shifted windows''}\\
ICCV 2021.\\
\textit{Swin Transformer - Hierarchical ViT with local attention.}

\end{enumerate}

\section{Medical Imaging}

\subsection{Chest X-ray Analysis}

\begin{enumerate}[label={[\arabic*]}]
\setcounter{enumi}{10}

\item \textbf{Wang, X., Peng, Y., Lu, L., et al.}\\
\textit{``ChestX-ray8: Hospital-scale chest X-ray database and benchmarks''}\\
CVPR 2017.\\
\textit{NIH Chest X-ray dataset (original release).}

\item \textbf{Rajpurkar, P., Irvin, J., Zhu, K., et al.}\\
\textit{``CheXNet: Radiologist-level pneumonia detection on chest X-rays with deep learning''}\\
arXiv:1711.05225, 2017.\\
\textit{DenseNet for chest X-ray, achieved radiologist-level performance.}

\item \textbf{Irvin, J., Rajpurkar, P., Ko, M., et al.}\\
\textit{``CheXpert: A large chest radiograph dataset with uncertainty labels and expert comparison''}\\
AAAI 2019.\\
\textit{CheXpert dataset with uncertainty handling.}

\item \textbf{Johnson, A. E., Pollard, T. J., Greenbaum, N. R., et al.}\\
\textit{``MIMIC-CXR-JPG, a large publicly available database of labeled chest radiographs''}\\
arXiv:1901.07042, 2019.\\
\textit{MIMIC-CXR dataset for chest X-ray research.}

\end{enumerate}

\subsection{Deep Learning in Medical Imaging}

\begin{enumerate}[label={[\arabic*]}]
\setcounter{enumi}{14}

\item \textbf{Litjens, G., Kooi, T., Bejnordi, B. E., et al.}\\
\textit{``A survey on deep learning in medical image analysis''}\\
Medical Image Analysis, 42, 60-88, 2017.\\
\textit{Comprehensive survey of deep learning for medical imaging.}

\item \textbf{Esteva, A., Kuprel, B., Novoa, R. A., et al.}\\
\textit{``Dermatologist-level classification of skin cancer with deep neural networks''}\\
Nature, 542(7639), 115-118, 2017.\\
\textit{CNN achieving dermatologist-level skin cancer detection.}

\end{enumerate}

\section{Deep Learning Techniques}

\subsection{Optimization and Training}

\begin{enumerate}[label={[\arabic*]}]
\setcounter{enumi}{16}

\item \textbf{Kingma, D. P., \& Ba, J.}\\
\textit{``Adam: A method for stochastic optimization''}\\
ICLR 2015.\\
\textit{Adam optimizer - Adaptive learning rates.}

\item \textbf{Ioffe, S., \& Szegedy, C.}\\
\textit{``Batch normalization: Accelerating deep network training''}\\
ICML 2015.\\
\textit{Batch normalization for stable training.}

\item \textbf{Srivastava, N., Hinton, G., Krizhevsky, A., et al.}\\
\textit{``Dropout: A simple way to prevent neural networks from overfitting''}\\
JMLR, 15(1), 1929-1958, 2014.\\
\textit{Dropout regularization technique.}

\end{enumerate}

\subsection{Attention Mechanisms}

\begin{enumerate}[label={[\arabic*]}]
\setcounter{enumi}{19}

\item \textbf{Bahdanau, D., Cho, K., \& Bengio, Y.}\\
\textit{``Neural machine translation by jointly learning to align and translate''}\\
ICLR 2015.\\
\textit{Attention mechanism for sequence-to-sequence models.}

\item \textbf{Hu, J., Shen, L., \& Sun, G.}\\
\textit{``Squeeze-and-excitation networks''}\\
CVPR 2018.\\
\textit{Channel attention for CNNs.}

\end{enumerate}

\section{Frameworks and Tools}

\begin{enumerate}[label={[\arabic*]}]
\setcounter{enumi}{21}

\item \textbf{Paszke, A., Gross, S., Massa, F., et al.}\\
\textit{``PyTorch: An imperative style, high-performance deep learning library''}\\
NeurIPS 2019.\\
\textit{PyTorch deep learning framework.}

\item \textbf{Abadi, M., Barham, P., Chen, J., et al.}\\
\textit{``TensorFlow: A system for large-scale machine learning''}\\
OSDI 2016.\\
\textit{TensorFlow deep learning framework.}

\end{enumerate}

\section{Online Resources}

\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,title=Useful Links]
\begin{itemize}
    \item \textbf{NIH Chest X-ray Dataset:}\\
    \url{https://nihcc.app.box.com/v/ChestXray-NIHCC}
    
    \item \textbf{PyTorch Documentation:}\\
    \url{https://pytorch.org/docs/stable/index.html}
    
    \item \textbf{Papers with Code - Chest X-ray:}\\
    \url{https://paperswithcode.com/task/chest-x-ray-classification}
    
    \item \textbf{Vision Transformer Implementation (timm):}\\
    \url{https://github.com/huggingface/pytorch-image-models}
    
    \item \textbf{Original Paper GitHub (TensorFlow):}\\
    \url{https://github.com/ViT-ChestXray/ViT-Chest-Xray}
\end{itemize}
\end{tcolorbox}

\section{Citation Format}

\begin{tcolorbox}[colback=yellow!5!white,colframe=yellow!75!black,title=How to Cite This Work]
\textbf{BibTeX:}
\begin{lstlisting}[style=plain,basicstyle=\ttfamily\scriptsize]
@article{jain2024comparative,
  title={A Comparative Study of CNN, ResNet, and Vision 
         Transformers for Multi-Classification of Chest Diseases},
  author={Jain, Ananya and Bhardwaj, Aviral and 
          Murali, Kaushik and Surani, Isha},
  journal={arXiv preprint arXiv:2406.00237},
  year={2024}
}
\end{lstlisting}

\textbf{APA:}\\
Jain, A., Bhardwaj, A., Murali, K., \& Surani, I. (2024). A Comparative Study of CNN, ResNet, and Vision Transformers for Multi-Classification of Chest Diseases. \textit{arXiv preprint arXiv:2406.00237}.
\end{tcolorbox}
