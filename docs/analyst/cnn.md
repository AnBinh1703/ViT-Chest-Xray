# PhÃ¢n TÃ­ch Chi Tiáº¿t: CNN Model cho Chest X-ray Classification

## ğŸ“‹ Tá»•ng Quan

File `cnn.ipynb` triá»ƒn khai má»™t mÃ´ hÃ¬nh **Convolutional Neural Network (CNN)** Ä‘Æ¡n giáº£n Ä‘á»ƒ phÃ¢n loáº¡i bá»‡nh lÃ½ tá»« hÃ¬nh áº£nh X-quang ngá»±c. ÄÃ¢y lÃ  bÃ i toÃ¡n **multi-label classification** vá»›i 15 lá»›p bá»‡nh lÃ½ khÃ¡c nhau.

---

## ğŸ”§ Cell 1: Import Libraries

```python
import glob, os, random, math, warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import cv2
import tensorflow as tf
import torch
from sklearn.metrics import roc_curve, auc
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MultiLabelBinarizer
from tensorflow.keras import layers, models
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2
warnings.filterwarnings('ignore')
```

### PhÃ¢n tÃ­ch chi tiáº¿t:

| ThÆ° viá»‡n | Má»¥c Ä‘Ã­ch |
|----------|----------|
| `glob, os` | Xá»­ lÃ½ file system, tÃ¬m kiáº¿m file |
| `numpy, pandas` | Xá»­ lÃ½ dá»¯ liá»‡u sá»‘ vÃ  báº£ng |
| `matplotlib, cv2` | Hiá»ƒn thá»‹ vÃ  xá»­ lÃ½ áº£nh |
| `tensorflow, torch` | Framework deep learning (TF lÃ  chÃ­nh, torch cho device check) |
| `sklearn` | Metrics Ä‘Ã¡nh giÃ¡ vÃ  chia dá»¯ liá»‡u |
| `ImageDataGenerator` | Data augmentation cho training |
| `Callbacks` | Äiá»u khiá»ƒn quÃ¡ trÃ¬nh training |

### âš ï¸ Nháº­n xÃ©t:
- **Import InceptionResNetV2 nhÆ°ng khÃ´ng sá»­ dá»¥ng** â†’ code thá»«a
- **Import cáº£ TensorFlow vÃ  PyTorch** â†’ khÃ´ng cáº§n thiáº¿t, chá»‰ dÃ¹ng TF
- `MultiLabelBinarizer` Ä‘Æ°á»£c import nhÆ°ng khÃ´ng sá»­ dá»¥ng

---

## ğŸ”§ Cell 2: Load Data

```python
%run data.ipynb
```

### PhÃ¢n tÃ­ch:
- Sá»­ dá»¥ng magic command `%run` Ä‘á»ƒ cháº¡y notebook `data.ipynb`
- Táº£i cÃ¡c biáº¿n tá»« data.ipynb: `train_generator`, `validation_generator`, `parser`
- **Æ¯u Ä‘iá»ƒm**: TÃ¡i sá»­ dá»¥ng code, modular
- **NhÆ°á»£c Ä‘iá»ƒm**: KhÃ³ debug, phá»¥ thuá»™c vÃ o file khÃ¡c

---

## ğŸ”§ Cell 3: Device Configuration

```python
gpus = tf.config.list_physical_devices('GPU')
if torch.cuda.is_available():
    device = torch.device('cuda')
else:
    if gpus:
        device = '/GPU:0' 
    else:
        device = '/CPU:0' 
print("Using device:", device)
```

### PhÃ¢n tÃ­ch logic:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   torch.cuda.available?    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   YES    â”‚       NO         â”‚
â”‚ cuda     â”‚  TF GPU exists?  â”‚
â”‚          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚          â”‚  YES   â”‚   NO    â”‚
â”‚          â”‚ /GPU:0 â”‚ /CPU:0  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### âš ï¸ Váº¥n Ä‘á»:
1. **Inconsistent device types**: `torch.device` vs TensorFlow string format
2. **Biáº¿n `device` khÃ´ng Ä‘Æ°á»£c sá»­ dá»¥ng** trong model training
3. TensorFlow tá»± Ä‘á»™ng sá»­ dá»¥ng GPU náº¿u cÃ³, khÃ´ng cáº§n explicit placement

### ğŸ’¡ Gá»£i Ã½ cáº£i thiá»‡n:
```python
# Chá»‰ cáº§n cho TensorFlow
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    print(f"Using GPU: {gpus[0].name}")
else:
    print("Using CPU")
```

---

## ğŸ”§ Cell 4: Hyperparameters

```python
batch_size = 32
learning_rate = 1e-4
weight_decay = 1e-6
num_epochs = 10
num_classes = 15
```

### PhÃ¢n tÃ­ch chi tiáº¿t:

| Parameter | GiÃ¡ trá»‹ | ÄÃ¡nh giÃ¡ |
|-----------|---------|----------|
| `batch_size` | 32 | âœ… PhÃ¹ há»£p cho háº§u háº¿t GPU |
| `learning_rate` | 1e-4 | âœ… Tá»‘t cho Adam optimizer |
| `weight_decay` | 1e-6 | âš ï¸ KhÃ¡ nhá», cÃ³ thá»ƒ tÄƒng lÃªn 1e-4 |
| `num_epochs` | 10 | âš ï¸ CÃ³ thá»ƒ Ã­t cho medical imaging |
| `num_classes` | 15 | âœ… ÄÃºng vá»›i dataset NIH Chest X-ray |

### 15 Classes trong Dataset:
1. Cardiomegaly
2. Emphysema
3. Effusion
4. Hernia
5. Nodule
6. Pneumothorax
7. Atelectasis
8. Pleural_Thickening
9. Mass
10. Edema
11. Consolidation
12. Infiltration
13. Fibrosis
14. Pneumonia
15. No Finding

---

## ğŸ”§ Cell 5: CNN Architecture

```python
def create_cnn_classifier():
    model = Sequential([
        Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),
        MaxPooling2D((2, 2)),
        Conv2D(64, (3, 3), activation='relu', name="last_conv_layer"),
        MaxPooling2D((2, 2)),
        Flatten(),
        Dense(512, activation='relu'),
        Dense(num_classes, activation='sigmoid')
    ])
    return model
```

### Kiáº¿n trÃºc máº¡ng:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INPUT LAYER                         â”‚
â”‚                   (224, 224, 3)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Conv2D(32, 3Ã—3, ReLU)                     â”‚
â”‚         Output: (222, 222, 32)                         â”‚
â”‚         Params: (3Ã—3Ã—3+1)Ã—32 = 896                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              MaxPooling2D(2Ã—2)                         â”‚
â”‚         Output: (111, 111, 32)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Conv2D(64, 3Ã—3, ReLU)                     â”‚
â”‚         Output: (109, 109, 64)                         â”‚
â”‚         Params: (3Ã—3Ã—32+1)Ã—64 = 18,496                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              MaxPooling2D(2Ã—2)                         â”‚
â”‚         Output: (54, 54, 64)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Flatten()                            â”‚
â”‚         Output: (186,624)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Dense(512, ReLU)                          â”‚
â”‚         Params: 186,624Ã—512+512 = 95,552,000          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Dense(15, Sigmoid)                        â”‚
â”‚         Params: 512Ã—15+15 = 7,695                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                    OUTPUT (15)
```

### Tá»•ng sá»‘ Parameters:
- **Total**: ~95.6 million parameters
- **Váº¥n Ä‘á» lá»›n**: 99%+ params náº±m á»Ÿ Dense layer Ä‘áº§u tiÃªn

### âš ï¸ NhÆ°á»£c Ä‘iá»ƒm nghiÃªm trá»ng:

1. **QuÃ¡ Ä‘Æ¡n giáº£n**: Chá»‰ 2 conv layers khÃ´ng Ä‘á»§ Ä‘á»ƒ extract features phá»©c táº¡p tá»« medical images

2. **Bottleneck táº¡i Flatten**: Flatten tá»« (54,54,64) táº¡o vector khá»•ng lá»“ 186,624 chiá»u

3. **Thiáº¿u cÃ¡c ká»¹ thuáº­t regularization**:
   - KhÃ´ng cÃ³ Dropout
   - KhÃ´ng cÃ³ BatchNormalization
   - Dá»… overfitting

4. **KhÃ´ng cÃ³ Padding**: Sá»­ dá»¥ng default `padding='valid'` lÃ m giáº£m kÃ­ch thÆ°á»›c feature map

5. **Activation cuá»‘i lÃ  Sigmoid**: âœ… ÄÃºng cho multi-label classification

### ğŸ’¡ Gá»£i Ã½ cáº£i thiá»‡n:

```python
def create_improved_cnn():
    model = Sequential([
        Conv2D(32, (3, 3), padding='same', input_shape=(224, 224, 3)),
        BatchNormalization(),
        Activation('relu'),
        MaxPooling2D((2, 2)),
        
        Conv2D(64, (3, 3), padding='same'),
        BatchNormalization(),
        Activation('relu'),
        MaxPooling2D((2, 2)),
        
        Conv2D(128, (3, 3), padding='same'),
        BatchNormalization(),
        Activation('relu'),
        MaxPooling2D((2, 2)),
        
        GlobalAveragePooling2D(),  # Thay vÃ¬ Flatten
        
        Dense(256, activation='relu'),
        Dropout(0.5),
        Dense(num_classes, activation='sigmoid')
    ])
    return model
```

---

## ğŸ”§ Cell 6: Training Function

```python
def run_experiment(model):
    optimizer = keras.optimizers.AdamW(
        learning_rate=learning_rate, weight_decay=weight_decay
    )
    model.compile(
        optimizer=optimizer,
        loss='binary_crossentropy',
        metrics=[
            keras.metrics.BinaryAccuracy(name="accuracy"),
            keras.metrics.AUC(name="auc"),
        ]
    )
    history = model.fit(
        train_generator,
        epochs=num_epochs,
        validation_data=validation_generator,
        callbacks=[ModelCheckpoint(...)]
    )
    return history
```

### PhÃ¢n tÃ­ch chi tiáº¿t:

#### Optimizer: AdamW
- **Adam with Weight Decay**: Káº¿t há»£p Adam vá»›i L2 regularization
- **Learning rate**: 1e-4 (phÃ¹ há»£p)
- **Weight decay**: 1e-6 (khÃ¡ nhá»)

#### Loss Function: Binary Cross-Entropy
$$\mathcal{L} = -\frac{1}{N}\sum_{i=1}^{N}\sum_{j=1}^{C}[y_{ij}\log(\hat{y}_{ij}) + (1-y_{ij})\log(1-\hat{y}_{ij})]$$

- âœ… **ÄÃºng cho multi-label classification**
- Má»—i class Ä‘Æ°á»£c xá»­ lÃ½ Ä‘á»™c láº­p

#### Metrics:
| Metric | MÃ´ táº£ |
|--------|-------|
| `BinaryAccuracy` | % predictions Ä‘Ãºng cho má»—i label |
| `AUC` | Area Under ROC Curve |

#### Callback: ModelCheckpoint
- LÆ°u model khi `val_loss` giáº£m
- âœ… Best practice

### âš ï¸ Thiáº¿u sÃ³t:
1. **KhÃ´ng cÃ³ EarlyStopping** â†’ cÃ³ thá»ƒ train quÃ¡ lÃ¢u
2. **KhÃ´ng cÃ³ ReduceLROnPlateau** â†’ khÃ´ng Ä‘iá»u chá»‰nh learning rate
3. **KhÃ´ng cÃ³ TensorBoard** â†’ khÃ³ visualize training

### ğŸ’¡ Gá»£i Ã½ callbacks:
```python
callbacks = [
    EarlyStopping(patience=5, restore_best_weights=True),
    ReduceLROnPlateau(factor=0.5, patience=3),
    ModelCheckpoint('best_model.keras', save_best_only=True),
    TensorBoard(log_dir='./logs')
]
```

---

## ğŸ”§ Cell 7: Execute Training

```python
cnn_classifier = create_cnn_classifier()
history_cnn = run_experiment(cnn_classifier)
```

### Flow:
1. Táº¡o model CNN
2. Compile vá»›i optimizer, loss, metrics
3. Fit trÃªn training data
4. Tráº£ vá» history Ä‘á»ƒ visualize

---

## ğŸ”§ Cell 8: Visualization

```python
def plot_combined_history(history):
    plt.figure(figsize=(12, 5))
    
    # Plot 1: Loss
    plt.subplot(1, 2, 1)  
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    
    # Plot 2: Accuracy
    plt.subplot(1, 2, 2) 
    plt.plot(history.history['accuracy'], label='Train Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
```

### Output dá»± kiáº¿n:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    LOSS CURVES       â”‚  ACCURACY CURVES     â”‚
â”‚                      â”‚                      â”‚
â”‚  â•²                   â”‚              ___     â”‚
â”‚   â•²___train          â”‚         ___/  val    â”‚
â”‚      â•²___            â”‚    ___/              â”‚
â”‚          â•²val        â”‚  /  train            â”‚
â”‚                      â”‚                      â”‚
â”‚  Epochs â†’            â”‚  Epochs â†’            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Äiá»u cáº§n quan sÃ¡t:
1. **Overfitting**: Train loss giáº£m, val loss tÄƒng
2. **Underfitting**: Cáº£ hai loss cao
3. **Good fit**: Cáº£ hai giáº£m vÃ  há»™i tá»¥

---

## ğŸ“Š ÄÃ¡nh GiÃ¡ Tá»•ng Thá»ƒ

### âœ… Äiá»ƒm máº¡nh:
1. Code structure rÃµ rÃ ng, dá»… Ä‘á»c
2. Sá»­ dá»¥ng Ä‘Ãºng loss function cho multi-label
3. CÃ³ model checkpointing
4. Visualization training curves

### âŒ Äiá»ƒm yáº¿u:

| Váº¥n Ä‘á» | Má»©c Ä‘á»™ | Giáº£i phÃ¡p |
|--------|--------|-----------|
| Model quÃ¡ Ä‘Æ¡n giáº£n | ğŸ”´ Critical | ThÃªm layers, dÃ¹ng pretrained |
| Thiáº¿u regularization | ğŸŸ  High | ThÃªm Dropout, BatchNorm |
| Bottleneck á»Ÿ Flatten | ğŸŸ  High | DÃ¹ng GlobalAveragePooling |
| Thiáº¿u callbacks | ğŸŸ¡ Medium | ThÃªm EarlyStopping, ReduceLR |
| Import thá»«a | ğŸŸ¢ Low | Cleanup imports |

### ğŸ’¡ Khuyáº¿n nghá»‹:

1. **Sá»­ dá»¥ng Transfer Learning**:
```python
base_model = tf.keras.applications.DenseNet121(
    include_top=False,
    weights='imagenet',
    input_shape=(224, 224, 3)
)
```

2. **ThÃªm Class Weights** cho imbalanced data:
```python
class_weights = compute_class_weight('balanced', ...)
```

3. **Metrics phÃ¹ há»£p hÆ¡n cho Medical Imaging**:
   - Sensitivity/Specificity per class
   - ROC-AUC per class
   - F1-score

---

## ğŸ“ˆ Expected Performance

Vá»›i architecture hiá»‡n táº¡i, dá»± kiáº¿n:
- **AUC**: 0.55-0.65 (khÃ´ng tá»‘t)
- **Binary Accuracy**: 0.85-0.90 (misleading do class imbalance)

Vá»›i improvements:
- **AUC**: 0.75-0.85
- Cáº§n pretrained model Ä‘á»ƒ Ä‘áº¡t SOTA (~0.85-0.90)

---

## ğŸ“š References

1. NIH Chest X-ray Dataset: https://nihcc.app.box.com/v/ChestXray-NIHCC
2. CheXNet Paper: Rajpurkar et al., 2017
3. TensorFlow Documentation: https://www.tensorflow.org/api_docs
