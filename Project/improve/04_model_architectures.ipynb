{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9e1b8ef",
   "metadata": {},
   "source": [
    "## 1.3 Transfer Learning with Pre-trained Weights\n",
    "\n",
    "### ‚ùì T·∫°i Sao Training From Scratch L√† Sai L·∫ßm?\n",
    "\n",
    "**V·∫•n ƒë·ªÅ:**\n",
    "- Dataset nh·ªè (112K images) so v·ªõi ImageNet (14M images)\n",
    "- M·∫•t ƒëi low-level features (edges, textures) ƒë√£ h·ªçc t·ª´ ImageNet\n",
    "- Convergence ch·∫≠m, d·ªÖ overfit\n",
    "- C·∫ßn nhi·ªÅu epochs h∆°n (~100 vs ~30)\n",
    "\n",
    "**Evidence t·ª´ literature:**\n",
    "- Rajpurkar et al. (CheXNet): Pre-trained weights ‚Üí +5% AUC\n",
    "- Irvin et al. (CheXpert): Transfer learning essential cho medical imaging\n",
    "\n",
    "### üí° Gi·∫£i Ph√°p: Smart Transfer Learning\n",
    "\n",
    "**Strategy:**\n",
    "1. **Load ImageNet weights** ‚Üí Low/mid-level features\n",
    "2. **Replace classifier head** ‚Üí Domain-specific classification\n",
    "3. **Progressive unfreezing:**\n",
    "   - Epochs 1-5: Freeze backbone, train head only\n",
    "   - Epochs 6+: Unfreeze all, fine-tune end-to-end\n",
    "\n",
    "**Why progressive unfreezing?**\n",
    "- Prevents catastrophic forgetting of ImageNet features\n",
    "- Stable training\n",
    "- Better final performance\n",
    "\n",
    "### üìà Expected Impact\n",
    "- **+2-4% AUC** improvement\n",
    "- **50% faster** convergence\n",
    "- Better feature representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f5fd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet-34 with ImageNet pre-trained weights\n",
    "    \n",
    "    Architecture:\n",
    "    - Backbone: ResNet-34 from torchvision (pre-trained on ImageNet)\n",
    "    - Head: Custom classifier for 15 chest diseases\n",
    "    \n",
    "    Features:\n",
    "    - Batch Normalization for stable training\n",
    "    - Dropout for regularization\n",
    "    - Progressive unfreezing support\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=15, pretrained=True, dropout=0.5):\n",
    "        super(PretrainedResNet, self).__init__()\n",
    "        \n",
    "        # Load pre-trained ResNet-34\n",
    "        if pretrained:\n",
    "            weights = torchvision.models.ResNet34_Weights.IMAGENET1K_V1\n",
    "            self.backbone = torchvision.models.resnet34(weights=weights)\n",
    "            print(\"‚úÖ Loaded ImageNet pre-trained weights for ResNet-34\")\n",
    "        else:\n",
    "            self.backbone = torchvision.models.resnet34(weights=None)\n",
    "            print(\"‚ö†Ô∏è  Training ResNet-34 from scratch\")\n",
    "        \n",
    "        # Get feature dimension\n",
    "        num_features = self.backbone.fc.in_features  # 512 for ResNet-34\n",
    "        \n",
    "        # Replace classifier head\n",
    "        self.backbone.fc = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(num_features, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "    \n",
    "    def freeze_backbone(self):\n",
    "        \"\"\"Freeze t·∫•t c·∫£ layers tr·ª´ classifier head\"\"\"\n",
    "        for name, param in self.backbone.named_parameters():\n",
    "            if 'fc' not in name:  # Kh√¥ng freeze head\n",
    "                param.requires_grad = False\n",
    "        print(\"üîí Backbone frozen, training head only\")\n",
    "    \n",
    "    def unfreeze_backbone(self):\n",
    "        \"\"\"Unfreeze t·∫•t c·∫£ layers cho fine-tuning\"\"\"\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(\"üîì Backbone unfrozen, training end-to-end\")\n",
    "\n",
    "\n",
    "class PretrainedViT(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Transformer with ImageNet pre-trained weights\n",
    "    \n",
    "    Uses timm library for SOTA ViT implementations\n",
    "    \n",
    "    Available models:\n",
    "    - vit_base_patch16_224: Standard ViT-B/16\n",
    "    - vit_base_patch32_224: ViT-B/32 (faster)\n",
    "    - vit_large_patch16_224: ViT-L/16 (best performance)\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name='vit_base_patch16_224', num_classes=15, \n",
    "                 pretrained=True, dropout=0.1):\n",
    "        super(PretrainedViT, self).__init__()\n",
    "        \n",
    "        # Create model with timm\n",
    "        self.model = timm.create_model(\n",
    "            model_name,\n",
    "            pretrained=pretrained,\n",
    "            num_classes=num_classes,\n",
    "            drop_rate=dropout  # Dropout in ViT blocks\n",
    "        )\n",
    "        \n",
    "        if pretrained:\n",
    "            print(f\"‚úÖ Loaded ImageNet pre-trained weights for {model_name}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Training {model_name} from scratch\")\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def freeze_backbone(self):\n",
    "        \"\"\"Freeze all layers except classifier head\"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if 'head' not in name:  # timm uses 'head' for classifier\n",
    "                param.requires_grad = False\n",
    "        print(\"üîí ViT backbone frozen, training head only\")\n",
    "    \n",
    "    def unfreeze_backbone(self):\n",
    "        \"\"\"Unfreeze all layers for fine-tuning\"\"\"\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(\"üîì ViT backbone unfrozen, training end-to-end\")\n",
    "\n",
    "\n",
    "class PretrainedSwinTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Swin Transformer - Hierarchical Vision Transformer\n",
    "    \n",
    "    Advantages over standard ViT:\n",
    "    1. Hierarchical feature maps (like CNN)\n",
    "    2. Shifted windows for efficient computation\n",
    "    3. Better for dense prediction tasks\n",
    "    4. More suitable for medical imaging\n",
    "    \n",
    "    Paper: \"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\"\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name='swin_base_patch4_window7_224', \n",
    "                 num_classes=15, pretrained=True, dropout=0.1):\n",
    "        super(PretrainedSwinTransformer, self).__init__()\n",
    "        \n",
    "        # Create Swin Transformer\n",
    "        self.model = timm.create_model(\n",
    "            model_name,\n",
    "            pretrained=pretrained,\n",
    "            num_classes=num_classes,\n",
    "            drop_rate=dropout\n",
    "        )\n",
    "        \n",
    "        if pretrained:\n",
    "            print(f\"‚úÖ Loaded ImageNet pre-trained weights for {model_name}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Training {model_name} from scratch\")\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def freeze_backbone(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if 'head' not in name:\n",
    "                param.requires_grad = False\n",
    "        print(\"üîí Swin backbone frozen, training head only\")\n",
    "    \n",
    "    def unfreeze_backbone(self):\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(\"üîì Swin backbone unfrozen, training end-to-end\")\n",
    "\n",
    "\n",
    "# Model factory\n",
    "def create_model(model_type='resnet34', num_classes=15, pretrained=True):\n",
    "    \"\"\"\n",
    "    Factory function to create models\n",
    "    \n",
    "    Args:\n",
    "        model_type: 'resnet34', 'vit_base', 'vit_large', 'swin_base'\n",
    "        num_classes: Number of output classes\n",
    "        pretrained: Use ImageNet pre-trained weights\n",
    "    \"\"\"\n",
    "    if model_type == 'resnet34':\n",
    "        model = PretrainedResNet(num_classes, pretrained)\n",
    "    elif model_type == 'vit_base':\n",
    "        model = PretrainedViT('vit_base_patch16_224', num_classes, pretrained)\n",
    "    elif model_type == 'vit_large':\n",
    "        model = PretrainedViT('vit_large_patch16_224', num_classes, pretrained)\n",
    "    elif model_type == 'swin_base':\n",
    "        model = PretrainedSwinTransformer('swin_base_patch4_window7_224', \n",
    "                                         num_classes, pretrained)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "print(\"‚úÖ Pre-trained models implemented:\")\n",
    "print(\"   1. PretrainedResNet (ResNet-34)\")\n",
    "print(\"   2. PretrainedViT (ViT-Base/16, ViT-Large/16)\")\n",
    "print(\"   3. PretrainedSwinTransformer (Swin-Base)\")\n",
    "print(\"\\nüéØ Features:\")\n",
    "print(\"   - ImageNet pre-trained weights\")\n",
    "print(\"   - Progressive unfreezing support\")\n",
    "print(\"   - Dropout regularization\")\n",
    "print(\"   - Easy model creation via factory function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6658b4b8",
   "metadata": {},
   "source": [
    "### üß™ Test Model Creation\n",
    "\n",
    "Verify models can be created and loaded correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273e72e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üß™ Testing model creation...\\n\")\n",
    "\n",
    "# Test ResNet-34\n",
    "print(\"1Ô∏è‚É£ Creating ResNet-34...\")\n",
    "resnet = create_model('resnet34', num_classes=15, pretrained=True)\n",
    "print(f\"   Parameters: {sum(p.numel() for p in resnet.parameters()):,}\")\n",
    "print(f\"   Trainable: {sum(p.numel() for p in resnet.parameters() if p.requires_grad):,}\\n\")\n",
    "\n",
    "# Test ViT\n",
    "print(\"2Ô∏è‚É£ Creating ViT-Base/16...\")\n",
    "try:\n",
    "    vit = create_model('vit_base', num_classes=15, pretrained=True)\n",
    "    print(f\"   Parameters: {sum(p.numel() for p in vit.parameters()):,}\")\n",
    "    print(f\"   Trainable: {sum(p.numel() for p in vit.parameters() if p.requires_grad):,}\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è Error loading ViT: {e}\\n\")\n",
    "\n",
    "# Test Swin\n",
    "print(\"3Ô∏è‚É£ Creating Swin Transformer...\")\n",
    "try:\n",
    "    swin = create_model('swin_base', num_classes=15, pretrained=True)\n",
    "    print(f\"   Parameters: {sum(p.numel() for p in swin.parameters()):,}\")\n",
    "    print(f\"   Trainable: {sum(p.numel() for p in swin.parameters() if p.requires_grad):,}\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è Error loading Swin: {e}\\n\")\n",
    "\n",
    "# Test freeze/unfreeze\n",
    "print(\"4Ô∏è‚É£ Testing freeze/unfreeze...\")\n",
    "resnet.freeze_backbone()\n",
    "frozen_params = sum(p.numel() for p in resnet.parameters() if p.requires_grad)\n",
    "print(f\"   Frozen trainable params: {frozen_params:,}\")\n",
    "\n",
    "resnet.unfreeze_backbone()\n",
    "unfrozen_params = sum(p.numel() for p in resnet.parameters() if p.requires_grad)\n",
    "print(f\"   Unfrozen trainable params: {unfrozen_params:,}\")\n",
    "\n",
    "print(\"\\n‚úÖ All models created successfully!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
