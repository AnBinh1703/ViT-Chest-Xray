% ============================================================
% CHAPTER 10: REFERENCES
% ============================================================

\begin{thebibliography}{99}

% ===== Original Paper =====
\bibitem{original_paper}
Ananya Jain, Aviral Bhardwaj, Kaushik Murali, and Isha Surani.
\textit{A Comparative Study of CNN, ResNet, and Vision Transformers for Multi-Classification of Chest Diseases}.
arXiv:2406.00237, 2024.
\url{https://arxiv.org/abs/2406.00237}

% ===== Foundational Deep Learning Papers =====
\bibitem{alexnet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.
\textit{ImageNet Classification with Deep Convolutional Neural Networks}.
Advances in Neural Information Processing Systems (NeurIPS), 2012.

\bibitem{vgg}
Karen Simonyan and Andrew Zisserman.
\textit{Very Deep Convolutional Networks for Large-Scale Image Recognition}.
International Conference on Learning Representations (ICLR), 2015.

\bibitem{resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\textit{Deep Residual Learning for Image Recognition}.
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.

% ===== Transformer Papers =====
\bibitem{attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin.
\textit{Attention Is All You Need}.
Advances in Neural Information Processing Systems (NeurIPS), 2017.

\bibitem{vit}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\textit{An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}.
International Conference on Learning Representations (ICLR), 2021.

\bibitem{bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\textit{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}.
Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), 2019.

% ===== Medical Imaging Papers =====
\bibitem{chestxray14}
Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald M. Summers.
\textit{ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases}.
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.

\bibitem{chexnet}
Pranav Rajpurkar, Jeremy Irvin, Kaylie Zhu, Brandon Yang, Hershel Mehta, Tony Duan, Daisy Ding, Aarti Bagul, Curtis Langlotz, Katie Shpanskaya, Matthew P. Lungren, and Andrew Y. Ng.
\textit{CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning}.
arXiv:1711.05225, 2017.

\bibitem{chexpert}
Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al.
\textit{CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison}.
AAAI Conference on Artificial Intelligence, 2019.

% ===== Optimization and Training =====
\bibitem{adam}
Diederik P. Kingma and Jimmy Ba.
\textit{Adam: A Method for Stochastic Optimization}.
International Conference on Learning Representations (ICLR), 2015.

\bibitem{batch_norm}
Sergey Ioffe and Christian Szegedy.
\textit{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}.
International Conference on Machine Learning (ICML), 2015.

\bibitem{dropout}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
\textit{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}.
Journal of Machine Learning Research (JMLR), 2014.

% ===== Vision Transformer Variants =====
\bibitem{deit}
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou.
\textit{Training data-efficient image transformers \& distillation through attention}.
International Conference on Machine Learning (ICML), 2021.

\bibitem{swin}
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.
\textit{Swin Transformer: Hierarchical Vision Transformer using Shifted Windows}.
IEEE/CVF International Conference on Computer Vision (ICCV), 2021.

% ===== Evaluation Metrics =====
\bibitem{auc}
James A. Hanley and Barbara J. McNeil.
\textit{The Meaning and Use of the Area under a Receiver Operating Characteristic (ROC) Curve}.
Radiology, 1982.

% ===== Frameworks =====
\bibitem{pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al.
\textit{PyTorch: An Imperative Style, High-Performance Deep Learning Library}.
Advances in Neural Information Processing Systems (NeurIPS), 2019.

\bibitem{tensorflow}
Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al.
\textit{TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems}.
Software available from tensorflow.org, 2015.

\end{thebibliography}
