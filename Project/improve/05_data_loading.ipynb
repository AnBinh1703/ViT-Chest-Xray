{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "731daec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "PyTorch version: 2.10.0+cu126\n",
      "CSV path: ..\\input\\Data_Entry_2017_v2020.csv\n",
      "Image dir: ..\\input\\images\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from utils.evaluation import get_train_transforms, get_valid_transforms, compute_sample_weights\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'img_size': 224,\n",
    "    'batch_size': 32,\n",
    "    'num_workers': 4,\n",
    "    'learning_rate': 1e-3,\n",
    "    'weight_decay': 1e-4,\n",
    "    'epochs': 50\n",
    "}\n",
    "\n",
    "# Paths\n",
    "PROJECT_ROOT = Path(\"../\")\n",
    "CSV_PATH = PROJECT_ROOT / \"input\" / \"Data_Entry_2017_v2020.csv\"\n",
    "IMAGE_DIR = PROJECT_ROOT / \"input\" / \"images\"\n",
    "\n",
    "# Setup device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(f'CSV path: {CSV_PATH}')\n",
    "print(f'Image dir: {IMAGE_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6008655",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Summary: Phase 1 Improvements\n",
    "\n",
    "### ‚úÖ ƒê√£ Implement\n",
    "\n",
    "| Improvement | Implementation | Expected Impact |\n",
    "|-------------|----------------|----------------|\n",
    "| **Advanced Augmentation** | Albumentations pipeline v·ªõi CLAHE, ShiftScaleRotate, Noise/Blur | +1-2% AUC |\n",
    "| **Class Imbalance** | Focal Loss + Weighted BCE + Label Smoothing | +3-5% (rare classes) |\n",
    "| **Transfer Learning** | ImageNet pre-trained weights + Progressive unfreezing | +2-4% AUC |\n",
    "\n",
    "### üéØ Combined Expected Impact\n",
    "- **Total: +5-10% AUC improvement**\n",
    "- **Faster convergence** (50% fewer epochs)\n",
    "- **Better generalization**\n",
    "- **More clinically useful** (better on rare diseases)\n",
    "\n",
    "### üìù Next Steps\n",
    "\n",
    "Trong c√°c cells ti·∫øp theo, ch√∫ng ta s·∫Ω:\n",
    "1. **Load v√† preprocess data**\n",
    "2. **Create datasets v·ªõi advanced augmentation**\n",
    "3. **Train models v·ªõi all improvements**\n",
    "4. **Evaluate v√† compare v·ªõi baseline**\n",
    "5. **Visualize results v√† insights**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f40dd9",
   "metadata": {},
   "source": [
    "# üóÇÔ∏è PHASE 2: Data Loading & Preprocessing\n",
    "\n",
    "## 2.1 Load NIH Chest X-ray Dataset\n",
    "\n",
    "### Dataset Overview\n",
    "- **Total images**: 112,120\n",
    "- **Number of classes**: 15 (multi-label)\n",
    "- **Format**: PNG grayscale images\n",
    "- **Labels**: NLP-extracted from radiology reports (~10% noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f2cbc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading dataset...\n",
      "   Total samples: 112,120\n",
      "   Diseases found: 15\n",
      "   ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Effusion', 'Emphysema', 'Fibrosis', 'Hernia', 'Infiltration', 'Mass', 'No Finding', 'Nodule', 'Pleural_Thickening', 'Pneumonia', 'Pneumothorax']\n",
      "\n",
      "üìä Class Distribution:\n",
      "   No Finding               : 60,361 (53.84%)\n",
      "   Infiltration             : 19,894 (17.74%)\n",
      "   Effusion                 : 13,317 (11.88%)\n",
      "   Atelectasis              : 11,559 (10.31%)\n",
      "   Nodule                   :  6,331 ( 5.65%)\n",
      "   Mass                     :  5,782 ( 5.16%)\n",
      "   Pneumothorax             :  5,302 ( 4.73%)\n",
      "   Consolidation            :  4,667 ( 4.16%)\n",
      "   Pleural_Thickening       :  3,385 ( 3.02%)\n",
      "   Cardiomegaly             :  2,776 ( 2.48%)\n",
      "   Emphysema                :  2,516 ( 2.24%)\n",
      "   Edema                    :  2,303 ( 2.05%)\n",
      "   Fibrosis                 :  1,686 ( 1.50%)\n",
      "   Pneumonia                :  1,431 ( 1.28%)\n",
      "   Hernia                   :    227 ( 0.20%)\n",
      "\n",
      "üì¶ Data Split:\n",
      "   Train: 80,726 samples (72.0%)\n",
      "   Val:   8,970 samples (8.0%)\n",
      "   Test:  22,424 samples (20.0%)\n",
      "\n",
      "‚úÖ Data loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "def load_and_prepare_data(csv_path, test_size=0.2, val_size=0.1, random_state=42):\n",
    "    \"\"\"\n",
    "    Load v√† prepare NIH Chest X-ray dataset\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to Data_Entry_2017_v2020.csv\n",
    "        test_size: Fraction for test set\n",
    "        val_size: Fraction of train set for validation\n",
    "    \n",
    "    Returns:\n",
    "        train_df, val_df, test_df, disease_columns\n",
    "    \"\"\"\n",
    "    print(\"üìÇ Loading dataset...\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    print(f\"   Total samples: {len(df):,}\")\n",
    "    \n",
    "    # Parse Finding Labels column\n",
    "    # Format: \"Disease1|Disease2|Disease3\" or \"No Finding\"\n",
    "    \n",
    "    # Get unique diseases\n",
    "    all_diseases = set()\n",
    "    for labels in df['Finding Labels'].values:\n",
    "        diseases = labels.split('|')\n",
    "        all_diseases.update(diseases)\n",
    "    \n",
    "    disease_columns = sorted(list(all_diseases))\n",
    "    print(f\"   Diseases found: {len(disease_columns)}\")\n",
    "    print(f\"   {disease_columns}\")\n",
    "    \n",
    "    # Create binary columns for each disease\n",
    "    for disease in disease_columns:\n",
    "        df[disease] = df['Finding Labels'].apply(\n",
    "            lambda x: 1 if disease in x.split('|') else 0\n",
    "        )\n",
    "    \n",
    "    # Print class distribution\n",
    "    print(\"\\nüìä Class Distribution:\")\n",
    "    class_counts = df[disease_columns].sum().sort_values(ascending=False)\n",
    "    for disease, count in class_counts.items():\n",
    "        percentage = count / len(df) * 100\n",
    "        print(f\"   {disease:25s}: {count:6,} ({percentage:5.2f}%)\")\n",
    "    \n",
    "    # Split data: train/val/test\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # First split: train+val vs test\n",
    "    train_val_df, test_df = train_test_split(\n",
    "        df, test_size=test_size, random_state=random_state, shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Second split: train vs val\n",
    "    train_df, val_df = train_test_split(\n",
    "        train_val_df, test_size=val_size, random_state=random_state, shuffle=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüì¶ Data Split:\")\n",
    "    print(f\"   Train: {len(train_df):,} samples ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "    print(f\"   Val:   {len(val_df):,} samples ({len(val_df)/len(df)*100:.1f}%)\")\n",
    "    print(f\"   Test:  {len(test_df):,} samples ({len(test_df)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    return train_df, val_df, test_df, disease_columns\n",
    "\n",
    "\n",
    "# Load data\n",
    "if CSV_PATH.exists():\n",
    "    train_df, val_df, test_df, disease_columns = load_and_prepare_data(\n",
    "        CSV_PATH, test_size=0.2, val_size=0.1\n",
    "    )\n",
    "    print(\"\\n‚úÖ Data loaded successfully!\")\n",
    "else:\n",
    "    print(f\"‚ùå CSV file not found: {CSV_PATH}\")\n",
    "    print(\"   Please update CSV_PATH in configuration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f14e71c",
   "metadata": {},
   "source": [
    "## 2.2 Custom Dataset Class\n",
    "\n",
    "### Design Principles\n",
    "1. **Efficient loading**: Only load images when needed\n",
    "2. **Flexible augmentation**: Support different transforms for train/val\n",
    "3. **Error handling**: Skip corrupted images\n",
    "4. **Memory efficient**: Don't load all images to RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "480789e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ChestXrayDataset class created\n",
      "   Features: Lazy loading, error handling, multi-label support\n"
     ]
    }
   ],
   "source": [
    "class ChestXrayDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for NIH Chest X-ray\n",
    "    \n",
    "    Features:\n",
    "    - Lazy loading (load images on-demand)\n",
    "    - Albumentations transforms\n",
    "    - Error handling for corrupted images\n",
    "    - Multi-label support\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe, image_dir, disease_columns, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe: DataFrame with image paths and labels\n",
    "            image_dir: Root directory containing images\n",
    "            disease_columns: List of disease column names\n",
    "            transform: Albumentations transform pipeline\n",
    "        \"\"\"\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.disease_columns = disease_columns\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Load and return one sample\n",
    "        \n",
    "        Returns:\n",
    "            image: (C, H, W) tensor\n",
    "            labels: (num_classes,) binary vector\n",
    "        \"\"\"\n",
    "        # Get image path and labels\n",
    "        row = self.df.iloc[idx]\n",
    "        img_name = row['Image Index']\n",
    "        img_path = self.image_dir / img_name\n",
    "        \n",
    "        # Load image\n",
    "        try:\n",
    "            image = cv2.imread(str(img_path))\n",
    "            \n",
    "            if image is None:\n",
    "                raise ValueError(f\"Failed to load image: {img_path}\")\n",
    "            \n",
    "            # Convert to RGB (X-ray is grayscale, but we need 3 channels for pre-trained models)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Error loading {img_path}: {e}\")\n",
    "            # Return black image as fallback\n",
    "            image = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        \n",
    "        # Get labels\n",
    "        labels = row[self.disease_columns].values.astype(np.float32)\n",
    "        labels = torch.FloatTensor(labels)\n",
    "        \n",
    "        return image, labels\n",
    "\n",
    "\n",
    "print(\"‚úÖ ChestXrayDataset class created\")\n",
    "print(\"   Features: Lazy loading, error handling, multi-label support\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207bcede",
   "metadata": {},
   "source": [
    "## 2.3 Create DataLoaders\n",
    "\n",
    "### Strategy\n",
    "1. **Train**: Advanced augmentation + WeightedRandomSampler\n",
    "2. **Val/Test**: Simple resize + normalize only\n",
    "3. **Batch size**: Balance between GPU memory and convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3b5293b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è  Computing sample weights...\n",
      "\n",
      "‚úÖ DataLoaders created:\n",
      "   Train: 2523 batches\n",
      "   Val:   281 batches\n",
      "   Test:  701 batches\n"
     ]
    }
   ],
   "source": [
    "def create_dataloaders(train_df, val_df, test_df, disease_columns, image_dir, config):\n",
    "    \"\"\"\n",
    "    Create train/val/test DataLoaders v·ªõi all improvements\n",
    "    \n",
    "    Returns:\n",
    "        train_loader, val_loader, test_loader\n",
    "    \"\"\"\n",
    "    # Get transforms\n",
    "    train_transform = get_train_transforms(config['img_size'])\n",
    "    valid_transform = get_valid_transforms(config['img_size'])\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = ChestXrayDataset(\n",
    "        train_df, image_dir, disease_columns, train_transform\n",
    "    )\n",
    "    val_dataset = ChestXrayDataset(\n",
    "        val_df, image_dir, disease_columns, valid_transform\n",
    "    )\n",
    "    test_dataset = ChestXrayDataset(\n",
    "        test_df, image_dir, disease_columns, valid_transform\n",
    "    )\n",
    "    \n",
    "    # Compute sample weights for weighted sampling\n",
    "    print(\"‚öôÔ∏è  Computing sample weights...\")\n",
    "    sample_weights = compute_sample_weights(train_df, disease_columns)\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        sampler=sampler,  # Use WeightedRandomSampler\n",
    "        num_workers=config['num_workers'],\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=config['num_workers'],\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=config['num_workers'],\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ DataLoaders created:\")\n",
    "    print(f\"   Train: {len(train_loader)} batches\")\n",
    "    print(f\"   Val:   {len(val_loader)} batches\")\n",
    "    print(f\"   Test:  {len(test_loader)} batches\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "# Create DataLoaders (if data is loaded)\n",
    "if 'train_df' in locals():\n",
    "    train_loader, val_loader, test_loader = create_dataloaders(\n",
    "        train_df, val_df, test_df, disease_columns, IMAGE_DIR, CONFIG\n",
    "    )\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Data not loaded, skip DataLoader creation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv313 (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
